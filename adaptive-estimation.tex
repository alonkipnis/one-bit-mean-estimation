\section{Adaptive Estimation \label{sec:sequential}}

The first main result of this paper (Theorem~\ref{thm:adpative_lower_bound})
gives that the asymptotic variance of any adaptive estimator must be at
least $\eta(0)\sigma^2$, which is precisely the efficiency of the median of
the sample $X_1,\ldots,X_n$. Conveniently, the stochastic (sub)gradient
estimator for the median---which minimizes $\E[|X - \theta|]$---is a
sequence of signs (single bits), so that we can exhibit an asymptotically
optimal adaptive estimation scheme.

%Finally, in Theorem~\ref{thm:opt_one_step}, we provide an adaptive estimation scheme that is one-step optimal in the sense that at each step $i$, the chosen message $B_i$  minimizes the MSE given $X_i$ and the previous $i-1$ messages. %Numerical simulations for the case of $f(x)$ the normal density function shows that the ARE of the estimator described by this scheme is $\eta(0)\sigma^2$. 

We begin with our first theorem, whose proof we provide in
Appendix~\ref{proof:thm:adpative_lower_bound}.
\begin{thm}[Fundamental limits]\label{thm:adpative_lower_bound}
  Let Assumption~\ref{assump:failure_rate} hold.
  Let ${\theta}_n$ be any estimator of $\theta$ in the adaptive setting of
  Figure~\ref{fig:setup}(ii). Assume that the prior
  density $\pi(\cdot)$ on $\theta$ converges to zero
  at the endpoints of the interval $\Theta$ and
  define the prior Fisher information
  $I_0 \defeq \E_\pi[(\pi'(\theta) / \pi(\theta))^2]$.
  Then
  \begin{equation*}
    \E\left[ (\theta-{\theta}_n)^2 \right] \geq   \frac{1}{ 4f^2(0) n + I_0}.
  \end{equation*}
\end{thm}
%Theorem~\ref{thm:adpative_lower_bound} can be extended to other loss functions using more general versions of the Van Trees inequality~\cite{efroimovich1980information}; see also \cite{DBLP:journals/corr/abs-1902-08582}.

We now turn to asymptotically optimal estimators, first
showing how a simple stochastic gradient scheme is asymptotically
optimal (in the fully adaptive setting), after which we show that
a one-round adaptive scheme can also achieve this optimal efficiency.

\subsection{Asymptotically optimal estimator}

The starting point for our first estimator is to note that the median of a
distribution minimizes $\E[|X - \theta|]$ over $\theta \in \R$, and
moreover, we have the familiar result (cf.~\cite{VanDerVaart98}, Ch.~21)
that given a sample $X_1,\ldots,X_n \simiid P$, if $\theta = \mbox{med}(P)$ and
$P$ has continuous density $f(\cdot - \theta)$ near $\theta$, then
\begin{equation*}
  \sqrt{n}(\mbox{med}(X_1^n) - \theta)
  \cd \normal\left(0, \frac{1}{4 f(0)^2}\right),
\end{equation*}
which is precisely the variance lower bound in
Theorem~\ref{thm:adpative_lower_bound}.  Thus, it is natural to consider a
stochastic gradient procedure for minimizing $\E[|X - \theta|]$. To that end,
let $\left\{ \gamma_n \right\}_{n\in \mathbb N}$ be a strictly positive
sequence of stepsizes,
and define the sequence
\begin{equation}
  \label{eq:sgd_alg}
  \theta_n = \theta_{n-1} + \gamma_n B_n, \quad n = 1,2,\ldots,
\end{equation}
where 
\begin{equation*}
  B_n = \sgn (X_n - \theta_{n-1}).
\end{equation*}
We make one of two assumptions on the stepsizes $\gamma_n$, which
are relatively standard: we always have $\gamma_n$ non-increasing, and
\begin{subequations}
  For some $0 < \lambda \le 1$,
  \begin{align}
    \label{eqn:lazy-gamma}
    \frac{\gamma_n - \gamma_{n+1}}{\gamma_n^2}
    \to 0, & ~~~
    \sum_n \frac{\gamma_n^\frac{1 + \lambda}{2}}{\sqrt{n}} < \infty
    ~~ \mbox{or} \\
    \gamma_n = o(n^{-2/3}),
    & ~~~
    \sum_n \gamma_n = \infty.
    \label{eqn:stringent-gamma}
  \end{align}
\end{subequations}

Then we can adapt the results of Polyak and Juditsky~\cite{polyak1992acceleration}
on the asymptotic normality of averaged stochastic gradient estimators
to establish the following theorem.
\begin{thm}
  \label{thm:sgd}
  Define the average $\bar{\theta}_n \defeq \frac{1}{n}
  \sum_{i = 1}^n \theta_i$. Assume
  that in a neighborhood
  of $\theta = \mbox{med}(P)$,
  the distribution $P$ has a Lipschitz continuous density $f$.
  Then
  \begin{enumerate}[(i)]
  \item \label{item:normal-sgd}
    Assume that $\left\{ \gamma_n \right\}_{n\in \mathbb N}$ satisfies
    condition~\eqref{eqn:lazy-gamma}.
    Then
    \begin{equation*}
      \sqrt{n}\left( \bar{\theta}_n - \theta\right)
      \cd \normal\left(0,\frac{1}{4 f(0)^2}\right).
    \end{equation*}
    %
    %
  \item \label{item:sgd-regular}
    %(or that $f(x-\theta)$ is differentiable in quadratic mean).  
    Let $\{P_\theta\}_{\theta \in \R}$ be the family of distributions
    with density $f(\cdot - \theta)$, where $f$ has median 0.
    Let $h_n \to h \in \R$, and define the distributions
    $P_n = P_{\theta + h_n/\sqrt{n}}^n$. Then
    \begin{equation*}
      \sqrt{n}\left(\bar{\theta}_n - \theta - h_n / \sqrt{n}\right)
      \mathop{\cd}_{P_n}
      \normal\left(0, \frac{1}{4 f(0)^2}\right),
    \end{equation*}
    and for any bounded, symmetric, and quasi-convex function $L$,
    \begin{align} 
      & \sup_{c < \infty} \limsup_{n \to \infty}
      \sup_{\tau\,:\,|\theta-\tau| \leq \frac{c}{\sqrt{n} }}
      \E_\tau \left[ L\left( \sqrt{n}(\bar{\theta}_{n} - \tau) \right) \right] \nonumber 
      \\
      & \qquad \qquad \qquad \qquad = \mathbb E \left[L(Z/ 2 f(0)) \right],
        \label{eq:attaining_LAM}
    \end{align}
    where $Z \sim \normal(0,1)$. 
    %
    %
  \item \label{item:sgd-ms-convergence} Assume the stepsizes $\gamma_n$
    satisfy both conditions~\eqref{eqn:lazy-gamma}
    and~\eqref{eqn:stringent-gamma}. Let
    $\pi$ be a distribution on $\R$ \newtext{with a finite second moment}. Then
    \begin{align}
      \int \E\Big[( \bar{\theta}_n - \theta )^2\Big] \pi(d\theta)
      = \frac{1}{4 n f(0)^2} + o(n^{-1}).
      \label{eq:adaptive_3}
    \end{align}
  \end{enumerate}
\end{thm}

\noindent
We provide the proofs of items (i)-(iii) in Appendices~\ref{sec:proof-normal-sgd},
\ref{sec:proof-sgd-regular}, \ref{sec:proof-sgd-ms-convergence},
respectively.

As an immediate corollary to Theorem~\ref{thm:sgd}, we obtain the following
asymptotic optimality results of the averaged stochastic gradient
sequence. Specifically, the average of the stochastic gradient
iterates~\eqref{eq:sgd_alg}
is locally asymptotically minimax, and they achieve the lower
bound of Theorem~\ref{thm:adpative_lower_bound}.

\begin{corollary}
  Let the conditions of Theorem~\ref{thm:adpative_lower_bound} hold
  and $\theta_n$ be defined by the iteration~\eqref{eq:sgd_alg}.
  Let $\{P_\theta\}_{\theta \in \R}$ be the family of distributions
  with densities $f(\cdot - \theta)$.
  \begin{enumerate}[(i)]
  \item Define the shorthand $P_n = P_{\theta + h_n/\sqrt{n}}^n$.
    If the stepsizes satisfy condition~\eqref{eqn:lazy-gamma}, then
    \begin{equation*}
      \sqrt{n}(\bar{\theta}_n - \theta - h_n / \sqrt{n})
      \mathop{\cd}_{P_n} \normal\left(0, \frac{1}{\eta(0)}\right).
    \end{equation*}
  \item If in addition the stepsizes satisfy
    condition~\eqref{eqn:stringent-gamma}, then they
    achieve the lower bound of Theorem~\ref{thm:adpative_lower_bound} for any
    prior $\pi$ on $\R$.
  \end{enumerate}
\end{corollary}

  
\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm,auto]
  \node at (0,0) (source) {$X_1$} ;
  \node[int1, right of = source, node distance = 1.2cm] (enc1) {$\enc$};  
\draw[->,line width = 2pt] (source) -- (enc1); 

% \node[below of = source, node distance = 1cm] (source2) {$X_2$};
%\node[int1, right of = source2, node distance = 1.2cm] (enc2) {Enc};  
%\draw[->,line width = 2pt] (source2) -- (enc2); 

\node[below of = source, node distance = 1.7cm] (source3) {$X_{n_1}$};
\node[int1, right of = source3, node distance = 1.2cm] (enc3) {$\enc$};  

\draw[->,line width = 2pt] (source3) -- (enc3); 

\node[below of = source, node distance = 0.5cm] {$\vdots$};

\node[int1, right of = enc3, node distance = 2.1cm ] (est) {$\est_1$};

\draw[->,line width = 2pt] (enc1) -| node[above, xshift = -1cm] (mes1) {$B_1$} (est);   

%\draw[->,line width = 2pt] (enc2) -| node[above, xshift = -1cm] (mes2) {$B_2$} (est);   

\draw[->,line width = 2pt] (enc3) -- node[above, xshift = 0cm]  {$B_{n_1}$} (est);   

\node[below right = 0.75 and 1.5 of source3] (sourceB) {$X_{n_1 +1}$} ;
\node[int1, right of = sourceB, node distance = 1.7cm] (enc1B) {$\enc$};  
\draw[->,line width = 2pt] (sourceB) -- (enc1B); 

\node[below of = sourceB, node distance = 1.7cm] (source3B) {$X_n$};
\node[int1, right of = source3B, node distance = 1.7cm] (enc3B) {$\enc$};  
\draw[->,line width = 2pt] (source3B) -- (enc3B); 
\node[below of = sourceB, node distance = 0.4cm] {$\vdots$};

\node[int1, right of = enc3B, node distance = 2.1cm ] (estB) {$\est_2$};

\draw[->,line width = 2pt] (enc1B) -| node[above, xshift = -0.5cm] {$B_{n_1+1}$} (estB);
\draw[->,line width = 2pt] (enc3B) -- node[above] {$B_n$} (estB);

\draw[->,line width = 1pt] (est.east) node[above, xshift  =0.5cm] {${\theta}_{n_1}$} -| (enc1B.north);

%\draw[->,line width = 1pt] (enc1B) -- (enc3B);

\draw[->,line width = 0.5pt] (est.east) -| +(1.3,-0.5) -- +(1.3,-2.5) -| (enc3B.north);

%\draw[->,line width = 1pt] (enc1B)  -- node[right] {${\theta}_{n_1}$} (enc3B);

\draw[->,line width = 0.5pt] (estB) -- +(0.8,0) node[right] {${\theta}_n$};
\node[below of = enc1B, node distance = 0.5cm] {$\vdots$};

\end{tikzpicture}
\end{center}
\caption{Distributed encoding with one round of threshold adaptation. The estimation obtained from the first $n_1$ bits in a distributed manner is utilized in obtaining another $n-n_1$ bits in a distributed manner. 
\label{fig:one_round}
}
\end{figure}

\subsection{Maximal Efficiency using One Round of Threshold Adaptation}


In the encoding and estimating procedure \eqref{eq:sgd_alg}, each one-bit
message $B_n$ depends on its private sample as well as the current gradient
descent estimate $\theta_{n-1}$. In this sense, each encoder in this
algorithm interacts with previous one by using the current estimate.  This
amount of adaptivity is unnecessary: as we now consider, a similar encoding
yields an asymptotically normal estimator attaining the lower variance bound
$1/\eta(0)$, provided we allow \emph{one} adaptive update to the threshold
value $\theta_0$ based on previously observed bits.
%
In this procedure we separate the sample into the disjoint sets
$X_1,\ldots,X_{n_1}$ and $X_{n_1+1},\ldots,X_n$ for some $n_1 < n$.  We
first use the estimator \eqref{eq:estimator_naive} to obtain an estimate
${\theta}_{n_1}$ based on $B_1,\ldots,B_{n_1}$, and then use
${\theta}_{n_1}$ as the new threshold value to obtain messages $B_{n_1+1},
\ldots, B_n$. Figure~\ref{fig:one_round} illustrates a diagram of this
procedure.

More formally, we consider the following estimation scheme. Given
$n_1 \in \{1, \ldots, n\}$,
set the individual bits
\begin{equation*}
  B_i =
  \begin{cases}
    \indic{X_i \le \theta_0} & i = 1,\ldots,n_1, \\
    \indic{X_i \le T_n}& i={n_1+1,\ldots,n},
  \end{cases}
\end{equation*}
where
\begin{equation*}
  T_n \defeq \theta_0 - F^{-1}\left(
  \frac{1}{n_1} \sum_{i=1}^{n_1} B_i 
  \right)
  ~~ \mbox{and} ~~
  \theta_n \defeq
  T_n - F^{-1} \left(\frac{1}{n - n_1}
  \sum_{i = n_1}^n B_i \right).
\end{equation*} 
The intuition here is that the estimator $\theta_n$ is a one-step
correction (cf.~\cite[Thm.~6.4.3]{LehmannCa98}) of the initial estimator
$T_n$, which approximately estimates
$\theta_0 - F^{-1}(\theta_0 - \theta) = \theta$. We then have the
following convergence result.
\begin{thm}
  Assume that $X_i = Z_i + \theta$, where $Z_i$ are i.i.d.\
  with density $f$ and CDF $F$ and $\mbox{med}(Z_i) = 0$. Assume that
  $f$ is continuous at 0, and that as $n \to \infty$,
  $n_1(n) \rightarrow \infty$ and $n_1 / n \to
  0$. Then
  \begin{align*}
    \sqrt{n} \left( {\theta}_n - \theta  \right)
    \cd  \normal\left( 0, \frac{1}{4 f(0)^2}\right).
  \end{align*}
\end{thm}
\noindent
That is, under Assumption~\ref{assump:failure_rate}, the method is
asymptotically optimal.
%
\begin{proof}
  We abuse notation and instead of assuming we receive $n$ observations, assume
  we receive the $n + n_1$ observations $X_{-n_1}, \ldots, X_{-1}$ and
  $X_1, \ldots, X_n$, defining $T_n = \theta_0 - F^{-1}(\frac{1}{n_1}
  \sum_{i = -n_1}^{-1} B_i)$ and
  $B_i = \indic{X_i \le T_n}$ for $i \ge 1$.
  Letting $X_i = Z_i + \theta$ for $Z_i$ i.i.d.\ with fixed density
  $f = F'$, we have
  $\ex{B_i} \cas F(\theta_0 - \theta)$, so that
  $\frac{1}{n_1} \sum_{i = -n_1}^{-1} B_i \cas F(\theta - \theta_0)$ and
  by the continuous
  mapping theorem we have $T_n \cas \theta$ as $n_1 \to \infty$.

  Now let
  $E_n = \E[B_i \mid T_n] = P(X_i \le T_n)$,
  so that $\var(B_i \mid T_n) = E_n(1 - E_n)$. Define also the random
  variable
  \begin{equation*}
    Y_n \defeq \sqrt{n}
    \frac{1}{\sqrt{E_n (1 - E_n)}}
    \bigg[\frac{1}{n}\sum_{i = 1}^n B_i - E_n\bigg],
  \end{equation*}
  and let $F_n(\cdot \mid T_n)$ be its cumulative distribution function.
  Then because
  \begin{equation*}
    \E\left[|B_i - E_n|^3 \mid T_n\right] \le E_n(1 - E_n)
    ~~ \mbox{we have} ~~
    \E\left[\frac{|B_i - E_n|^3}{(E_n(1 - E_n))^{3/2}} \mid T_n\right]
    \le \frac{1}{\sqrt{E_n(1 - E_n)}}.
  \end{equation*}
 The Berry-Esseen theorem implies that there exists a constant
  $C \le 1$ such that
  \begin{equation*}
    \sup_t \left|F_n(t \mid T_n) - \Phi(t) \right|
    \le \frac{C}{\sqrt{E_n (1 - E_n)} \sqrt{n}} \wedge 2,
  \end{equation*}
  where $\Phi$ is the standard Gaussian CDF.
  As $E_n (1 - E_n) \cas \frac{1}{4}$ by definition of the median,
  we have
  that (with probability 1)
  \begin{equation*}
    \sup_t \left|F_n(t \mid T_n) - \Phi(t)\right| \le \frac{C}{\sqrt{n}}
    ~~ \mbox{eventually}.
  \end{equation*}
  By dominated convergence and Jensen's inequality we thus obtain
  \begin{equation*}
    \sup_t \left|\mathbb{P}(Y_n \le t) - \Phi(t)\right|
    \le \E\left[
      \sup_t \left|F_n(t \mid T_n) - \Phi(t)\right| \right]
    \to 0,
  \end{equation*}
  which gives that $Y_n \cd \normal(0, 1)$.

  Now,
  Slutsky's lemmas imply
  \begin{equation}
    \sqrt{n}
    \cdot \frac{2}{n} \sum_{i = 1}^n (B_i - E_n)
    =
    (1 + o_P(1)) \frac{1}{\sqrt{n E_n(1 - E_n)}}
    \sum_{i = 1}^n \left(B_i - E_n\right)
    \cd \normal(0, 1).
    \label{eqn:apply-slutsky}
  \end{equation}
  For shorthand we write $P_n B = \frac{1}{n} \sum_{i = 1}^n B_i$,
  and using that $E_n = \E[B_i \mid T_n] = F(T_n - \theta)$, we may thus
  use the delta method to write
  \begin{align*}
    \sqrt{n}(\theta_n - \theta)
    & = \sqrt{n}\left(T_n - F^{-1}\left(P_n B\right)
    - \theta \right) \\
    & = \sqrt{n} \left(T_n - F^{-1}\left(F(T_n - \theta)
    + P_n B - F(T_n - \theta)\right) - \theta \right) \\
    & = \sqrt{n} \left(T_n - (T_n - \theta )
    + (F^{-1})'(T_n - \theta + o_P(1))\cdot \left(
    P_n B - E_n\right) - \theta \right) \\
    & = \sqrt{n} (F^{-1})'(0) (P_n B - E_n)
    + o_P(1)
    \cd \normal\left(0, \frac{1}{4 f(0)^2}\right),
  \end{align*}
  where we have used the limiting distribution~\eqref{eqn:apply-slutsky}. 
\end{proof}
%

Figure \ref{fig:adaptive_error} illustrates the empirical risks 
of the estimator \eqref{eq:sgd_alg} and an estimator obtained using one round of threshold adaptation under a series of Monte Carlo simulations when $f(x)$ is the standard normal desnity.


\begin{figure}
  %%%%% COMMENTED TIKZ PICTURE %%%%%
\begin{center}
\begin{tikzpicture}[scale = 0.6]
\begin{axis}[
width=10cm, height=6cm,
xmin = 200, xmax=800, 
restrict y to domain = 0:3,
ymin = 0,
ymax = 3.4,
samples=10, 
xlabel= $n$,
ylabel = {$n\cdot\mathbb E \left[\left(\theta - {\theta}_n \right)^2 \right]$},
%xtick={-3,-2,-1,0,1,2,3},
%xticklabels={-3,-2,-1,0,1,2,3},
ytick={0,1,1.57},
yticklabels={0,1,$\pi/2$},
line width=1.0pt,
mark size=1.5pt,
ymajorgrids,
xmajorgrids,
legend style= {at={(1,1)},anchor=north east,draw=black,fill=white,align=left}
]

\addplot[color = blue, solid, smooth] plot table [x = itr, y = SGD, col sep=comma] {./SimRes/sim_res_nMonte5000.csv};
\addlegendentry{asymptotically optimal};

%\addplot[color = red, solid, smooth] plot table [x = itr, y = Bayes, col sep=comma] {./SimRes/sim_res_nMonte5000.csv};\addlegendentry{one step optimal};

\addplot[color = red, solid, smooth] plot table [x = itr, y = split, col sep=comma] {./SimRes/sim_res_nMonte5000.csv};
\addlegendentry{one adaptation};

\end{axis}
\end{tikzpicture}
\caption{Normalized empirical risk versus number of samples $n$ for $10,000$ Monte Carlo trials with $f(x)$ the standard normal density. In each trial, $\theta$ is chosen uniformly over the interval $(-1.64,1.64)$. The one round threshold adaptation strategy uses $n_1 = \lfloor \sqrt{n} \rfloor$ samples before adapting the threshold.
\label{fig:adaptive_error}  }
\end{center}
\end{figure}
