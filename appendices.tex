% -*- Mode: latex -*- %

\section{}

\subsection{Fast convergence of uniform estimators under bit constraints}
\label{sec:uniform-weirdos}

Here we consider the uniform distribution as our location family,
demonstrating that in the adaptive setting~\eqref{item:adaptive} or even the
one-step adaptive setting~(\ref{item:one-step-adaptive}'), constrained
estimators can attain rates faster than the $1 / \sqrt{n}$ rates regular
estimands allow. Indeed, define $c(x) = -\log 2$ for $x \in [-1, 1]$ and
$c(x) = -\infty$ for $x \not \in [-1, 1]$. Then $f(x) = e^{-c(x)}$ is
log-concave and symmetric, and we may consider the location family with
densities $f(x - \theta)$. For notational simplicity, we assume we have
a sample of size $2n$. We provide a proof sketch that
there is a one-step adaptive estimator $\theta_n$ such that
\begin{equation}
  \label{eqn:uniforms-are-easy}
  \sup_{|\theta| \le \log n}
  P_\theta\left(|\theta_n - \theta| \ge \frac{8 \log n}{n^{3/4}}\right)
  \le \frac{2}{n^2}.
\end{equation}
for all large $n$,
and so (by the Borel-Cantelli lemmas), for any $\theta \in \R$ we have
$P_\theta(|\theta_n - \theta| \le \sqrt{2 \log n} / n^{3/4} ~
\mbox{eventually}) = 1$. This is of course faster than the $1/\sqrt{n}$
rates we prove throughout.

To prove inequality~\eqref{eqn:uniforms-are-easy}, we proceed in two steps,
both quite similar.
First, we define an initial estimator $\theta\init_n$.
Let $\epsilon > 0$, which we will determine presently, though we will
take $n \epsilon \to \infty$ as $n \to \infty$, so that we may assume
w.l.o.g.\ that $\theta \in [-n\epsilon/2, n \epsilon/2]$. Take the interval
$[-n\epsilon, n\epsilon]$, and construct
$m$ thresholds at intervals of size $2 n \epsilon / m$; let
the $j$th such threshold be
\begin{equation*}
  t_j \defeq -n \epsilon + \frac{2 n (j-1) \epsilon}{m}
\end{equation*}
Then we ``assign'' observations to each pair of thresholds, so that
threshold $j$ corresponds to observations $I_j \defeq \{\frac{n (j - 1)}{m}
+ 1, \ldots, \frac{n j}{m}\}$, of which there are $n/m$.  For each index $i
\in I_j$, we set
\begin{equation*}
  B_i = \begin{cases}
    1 & \mbox{if~} X_i \ge t_j \\
    0 & \mbox{otherwise}.
  \end{cases}
\end{equation*}
Then we simply set $\theta_n\init$ to be the maximal threshold for which
$B_i = 0$ for all observations $X_i$ corresponding to that threshold.

Let us now consider the probability that $\theta_n\init$ is substantially
wrong. For notational simplicity, let $U_i = (1 + X_i - \theta) / 2$, so that
the $U_i$ are uniform on $[0, 1]$.
First, note that we always have $\theta_n\init \ge \theta
- \frac{2 n \epsilon}{m}$, because no observations will be below
the appropriate threshold. Let $j\opt$ be the smallest index $j$
for which $\theta \le t_{j\opt}$, and consider
the index sets $I_{j\opt}$, $I_{j\opt + 1}$, and so on.
The event $\theta\init_n \ge t_{j\opt} + \frac{2 n \epsilon}{m}$ may
occur only if for each of the $n/m$ observations in the set
$I_{j\opt + 1}$, we have $U_i \ge \frac{n \epsilon}{m}$. Thus,
\begin{equation*}
  P_\theta\left(\theta_n\init \ge t_{j\opt} + \frac{2 n \epsilon}{m}
  \right)
  \le \left(1 - \frac{n \epsilon}{m}\right)^\frac{n}{m}
  \le \exp\left(-\frac{n^2\epsilon}{m^2}\right).
\end{equation*}
Setting the number of bins $m = \sqrt{n}$,
the resolution $\epsilon = 2 \log n / n$, we obtain
$P_\theta(\theta_n\init \ge t_{j\opt} + 4 \log n / \sqrt{n})
\le n^{-2}$. Thus we have
\begin{equation}
  \label{eqn:quality-of-initial-estimate}
  \sup_{|\theta| \le \log n} P_\theta\left(|\theta_n\init - \theta| \ge
  \frac{8 \log n}{\sqrt{n}}\right) \le \frac{1}{n^2}.
\end{equation}

The second stage estimator follows roughly the same strategy, except that
the resolution of the bins is tighter. In particular, let us assume that
$|\theta_n\init - \theta| \le \frac{8 \log n}{\sqrt{n}}$, which happens
eventually by inequality~\eqref{eqn:quality-of-initial-estimate}.  (We will
assume this tacitly for the remainder of the argument.)  Consider the
interval $\Theta_n \defeq \theta_n\init + [-\frac{16 \log n}{\sqrt{n}},
  \frac{16 \log n}{\sqrt{n}}]$ centered at $\theta_n\init$; we know that the
interval includes $[\theta - \frac{8 \log n}{\sqrt{n}}, \theta + \frac{8
    \log n}{\sqrt{n}}]$. Without loss of generality we assume $\theta_n\init
= 0$.  Following precisely the same discretization strategy as that for
$\theta_n\init$, we divide $\Theta_n$ into $m$ equal intervals, with
thresholds $t_j = -\frac{16 \log n}{\sqrt{n}} + \frac{32 (j - 1) \log n}{m
  \sqrt{n}}$; let $\epsilon_n = \frac{32 \log n}{m \sqrt{n}}$ be the width of
these intervals.  Then following exactly the same reasoning as above, we
assign indices $I_j = \{\frac{n(j - 1)}{m} + 1, \ldots, \frac{n j}{m}\}$ and
for $i \in I_j$, set $B_i = 1$ if $X_i \ge t_j$. We define $\theta_n$ to be
the maximal threshold $t_j$ for which $B_i = 0$ for all observations $X_i
\in I_j$. Then following precisely the reasoning above, we have (on the
event that $|\theta_n\init - \theta| \le \frac{8 \log n}{\sqrt{n}}$)
\begin{equation*}
  P_\theta(|\theta_n - \theta|
  \ge 2 \epsilon_n)
  \le (1 - \epsilon_n)^\frac{n}{m}
  \le \exp\left(-\frac{n \epsilon_n}{m}\right)
  = \exp\left(-\frac{32 \sqrt{n} \log n}{m^2}\right).
\end{equation*}
Set $m = 4 n^{1/4}$ to obtain the claimed
result~\eqref{eqn:uniforms-are-easy}.



\subsection{Proof of Proposition~\ref{prop:CEO}
\label{app:proof:CEO}}

%
Denote by $D^\star$ the optimal MSE in the Gaussian CEO with $L$ observers and under a total sum-rate $r = r_1 + \ldots +r_L$. An expression for $D^\star$ as a function of $r$ is give as \cite[Eq. 10]{chen2004upper}:
\begin{equation} \label{eq:ceo_optimal_sumrate}
r = \frac{1}{2} \log^+ \left[ \frac{\sigma_\theta^2}{D^\star} \left( \frac{D^\star L}{ D^\star L - \sigma^2 + D^\star \sigma^2 / \sigma_\theta^2 }\right)^L  \right].
\end{equation}
For the special case where $r = n$ and $L=n$, we have
\begin{equation} \label{eq:ceo_optimal_sumrate2}
n = \frac{1}{2} \log_2 \left[ \frac{\sigma_\theta^2}{D^\star} \left(\frac{ D^\star n }{D^\star n - \sigma^2 + D^\star \sigma^2/\sigma_\theta^2 }  \right)^n  \right].
\end{equation}
Consider the distributed encoding setting (iii) in the case where $f(x) = \Ncal(0,\sigma^2)$ and the prior on $\Theta$ is $\pi = \Ncal(0,\sigma_\theta^2)$. The Gaussian CEO problem of \cite{viswanathan1997quadratic} with a unit bitrate $r_1=\ldots = r_n =1$ at each terminal and blocklength $k=1$ reduces to our distributed setting (iii). Since $D^\star$ satisfying \eqref{eq:ceo_optimal_sumrate2} describes the MSE in the CEO setting under an optimal allocation of the sum-rate $r = n$ among $n$ encoders, it provides a lower bound to the minimal MSE in estimating $\theta$ in the distributed setting. By considering the limit $n\rightarrow \infty$ in \eqref{eq:ceo_optimal_sumrate2}, we see that 
\[
D^\star = \frac{ 4\sigma^2 }{3n + 4 \sigma^2 / \sigma_\theta^2 } + o(n^{-1}) =  \frac{4\sigma^2}{3n} + o(n^{-1}). 
\]
This implies Proposition~\ref{prop:CEO}. 
%We note that although the lower bound \eqref{eq:ceo_bound} was derived assuming the optimal allocation of $n$ bits per observation among the encoders, this bound cannot be tightened by considering the MSE in the CEO setting while enforcing the condition $r_1=\ldots = r_n = 1$. Indeed, an upper bound for the CEO MSE under the condition $r_1=\ldots = r_n = 1$ follows from \cite{KipnisRini2019}, and leads to
%\[
%D^\star \leq  \left( \frac{1}{\sigma_\theta^2} +  \frac{3n}{4\sigma^2 + \sigma_\theta^2} \right)^{-1}   =
%\frac{4 \sigma^2}{3n} +  \frac{\sigma_\theta^2}{3n} + O(n^{-2}),
%\]
%which is equivalent to \eqref{eq:ceo_bound} when $\sigma_\theta$ is small. 

\subsection{Proof of Theorem~\ref{thm:adpative_lower_bound}
\label{proof:thm:adpative_lower_bound}
}
Consider the following two Lemmas:
\begin{lem} \label{lem:bound_intervals}
Let $f(x)$ be log-concave, symmetric, and differentiable density function such that Assumption~1 holds. For any $x_1 \geq \ldots \geq x_n \in \R$,
\begin{equation}
\frac{ \left| \sum_{k=1}^n (-1)^{k+1} f(x_k) \right|^2 }
{\left( \sum_{k=1}^n (-1)^{k+1} F(x_k) \right) \left(1- \sum_{k=1}^n (-1)^{k+1} F(x_k) \right) } 
\leq  4f^2(0). 
\label{eq:lem_bound_intervals}
\end{equation}
\end{lem}
\begin{lem} \label{lem:fisher_bound}
Let $X$ be a random variable with a symmetric, log-concave, and continuously differentiable density function $f(x)$ such that Assumption~1 holds. For a Borel measurable $A$ set, 
\[
B(X) = \begin{cases} 1,& X \in A, \\
-1, & X \notin A.
\end{cases}
\]
Then the Fisher information of $B$ with respect to $\theta$ is bounded from above by $\eta(0)$.
\end{lem}

Lemma~\ref{lem:bound_intervals} is obtained as the special case $\delta = 0$ of lemma \ref{lem:bound_intervals_delta}. 
We now prove Lemma~\ref{lem:fisher_bound}.
\subsubsection*{Proof of Lemma~\ref{lem:fisher_bound}}
When $f(x)$ is the normal density function, this lemma follows from \cite[Thm. 3]{Barnes2018}. The proof below is based on a different techique than in \cite{Barnes2018}, and is valid for any log-concave symmetric density satisfying Assumption~\ref{assump:failure_rate}. \\

The Fisher information of $B$ with respect to $\theta$ is given by
\begin{align}
I_\theta & =  \ex{ \left( \frac{d}{d\theta} \log P\left( B | \theta \right) \right)^2 |\theta } \nonumber \\
& = \frac{ \left(\frac{d}{d\theta} P(B=1|\theta) \right)^2}{P(B=1| \theta)} + \frac{ \left(\frac{d}{d\theta} P(B=-1|\theta) \right)^2} {P(B=-1| \theta)} \nonumber \\
& =  \frac{ \left( \frac{d}{d\theta} \int_A f \left( x-\theta\right)dx \right)^2} { P(B=1| \theta) } + \frac{ \left( \frac{d}{d\theta}\int_A f \left( x-\theta \right)dx \right)^2} { P(B=-1| \theta) } \nonumber \\ 
& \overset{(a)}{=} \frac{ \left( - \int_A f' \left( x-\theta \right)dx \right)^2} { P(B=1| \theta) } + \frac{ \left(- \int_A f' \left( x-\theta \right)dx \right)^2} { P(B=-1| \theta) } \nonumber \\ 
& = \frac{\left( \int_A f'\left( x-\theta \right) dx \right)^2 }{  P(B=1 | \theta) \left(1-P(B=1|\theta) \right)  }, \nonumber \\
& = \frac{\left( \int_A f'\left( x-\theta \right) dx \right) \left( \int_A f'\left( x-\theta \right) dx \right)}{ \left( \int_A f \left( x-\theta \right) dx \right)  \left(1- \int_A f \left( x-\theta \right) dx \right) }, \label{eq:lem_fisher_bound_proof1}
\end{align}
where differentiation under the integral sign in $(a)$ is possible since $f(x)$ is differentiable with continuous derivative $f'(x)$. Regularity of the Lebesgue measure implies that for any $\epsilon>0$, there exists a finite number $k$ of disjoint open intervals $I_1,\ldots I_k$ such that 
\[
\int_{A\setminus \cup_{j=1}^k I_j }  dx < \epsilon,
\]
which implies that for any $\epsilon' > 0$, the set $A$ in \eqref{eq:lem_fisher_bound_proof1} can be replaced by a finite union of disjoint intervals without increasing $I_\theta$ by more than $\epsilon'$. It is therefore enough to proceed in the proof assuming that $A$ is of the form
\[
A = \cup_{j=1}^k (a_j,b_j),
\]
with $\infty \leq a_1 \leq \ldots a_k$, $b_1 \leq b_k \leq \infty$ and $a_j \leq b_j$ for $j=1,\ldots,k$. Under this assumption we have
\begin{align*}
\mathbb P(B_n=1| \theta) & = \sum_{j=1}^k \mathbb P\left(X_n \in (a_j,b_j) \right)  \\
& = \sum_{j=1}^k \left( F \left(b_j-\theta\right) -  F \left(a_j-\theta\right)  \right),
\end{align*}
so \eqref{eq:lem_fisher_bound_proof1} can be rewritten as
\begin{align}
& =   \frac { \left( \sum_{j=1}^{k} f \left(a_j-\theta \right) - f \left( b_j-\theta \right)  \right)^2 } 
{ \left( \sum_{j=1}^k F \left( b_j-\theta \right) - F \left( a_j-\theta \right)  \right) }  \nonumber \\
& \times \frac {1} 
{1- \left( \sum_{j=1}^k F \left(  b_j-\theta \right) - F \left( a_j-\theta \right)  \right) } 
\label{eq:lemma_J}
\end{align}
It follows from Lemma~\ref{lem:bound_intervals} that for any $\theta \in \R$ and any choice of the intervals endpoints, \eqref{eq:lemma_J} is smaller than 
\[
\max_{t \in \{a_j,b_j, j=1,\ldots,k\} } 4f^2(t) \leq 4 f^2(0), 
\]
where the last transition is due to Assumption~1. 
\QEDA \\


We now finish the proof of Theorem~\ref{thm:adpative_lower_bound}. In order to bound from above the Fisher information of any set of $n$ one-bit messages with respect to $\theta$, we first note that without loss of generality, each message $B_i$ can is of the form
\begin{equation}
\label{eq:general_messages}
B_i = \begin{cases}
X_i \in A_i & 1, \\
X_i \notin A_i & -1,
\end{cases} 
\end{equation}
where $A_i \subset \R$ is a Borel measurable set. 
%Indeed, any measurable function $(X_i) \in \{-1,1\}$ can be written in the form \eqref{eq:general_messages} with $A_i = B^{-1}(1)$.
Consider the conditional distribution $P({B^n|\theta})$ of $B^n$ given $\theta$. We have 
\begin{align}
P\left( B^n | \theta \right) & =  \prod_{i=1}^n P\left(B_i | \theta, B^{i-1} \right), \label{eq:adpt_lower_bound_proof:1}
\end{align}
where $P\left(B_i =1 | \theta, B^{i-1}  \right) = \mathbb P\left( X_i \in A_i\right)$, so that the Fisher information of $B^n$ with respect to $\theta$ is given by 
\begin{align}
I_\theta(B^n) = \sum_{i=1}^n I_\theta (B_i|B^{i-1}),
\label{eq:fisher_information}
\end{align}
where $I_\theta (B_i|B^{i-1})$ is the Fisher information of the distribution of $B_i$ given $B^{i-1}$. From Lemma~\ref{lem:fisher_bound} it follows that $I_\theta (B_i|B^{i-1}) \leq 4f^2(0)$. The Van Trees inequality \cite{van2004detection, gill1995applications} now implies 
\begin{align*}
\ex{ \left( \theta_n - \theta \right)^2} &  \geq \frac{1}{ \ex{ I_\theta(B^n)} + I_0} \\
& = \frac{1}{ \sum_{i=1}^n I_\theta (B_i | B^{i-1} ) + I_0} \\
& \geq \frac{1}{ 4f^2(0) n + I_0}.
\end{align*}
\QEDA


\subsection{Isoperimetric Lemma
\label{sec:bound_intervals_delta} }
The following lemma is used in the proof Theorems~\ref{thm:adpative_lower_bound}, \ref{thm:LAN1}, and \ref{thm:non_existence}. 

\begin{lem} \label{lem:bound_intervals_delta}
Let $f(x)$ be log-concave, symmetric, and differentiable density function. Let $\delta\geq 0$. Assume that the origin $x=0$ uniquely maximizes the function
\[
\eta_\delta(x) \triangleq  \eta^{1+\delta}(x)/f^\delta(x) = \frac{  \left( f(x) \right)^{2+\delta}}{\left(F(x)(1-F(x))\right)^{1+\delta}}
\]
and that $\eta_\delta(x)$ is non-increasing in $|x|$. For any $x_1 \ge \ldots \ge x_n \in \R^n$,
\begin{equation}
\frac{ \left| \sum_{i=1}^n (-1)^{i+1} f(x_i) \right|^{2+\delta} }
{\left| \sum_{i=1}^n (-1)^{i+1} F(x_i) \right|^{1+\delta} \left|1- \sum_{k=1}^n (-1)^{i+1} F(x_i) \right|^{1+\delta} } 
\leq  \max_{i} \eta_\delta(x_i).
\label{eq:lem_bound_intervals_delta}
\end{equation}
\end{lem}
In particular, 
\[
\frac{ \left| \sum_{i=1}^n (-1)^{i+1} f(x_i) \right|^{2+\delta} }
{\left| \sum_{i=1}^n (-1)^{i+1} F(x_i) \right|^{1+\delta} \left|1- \sum_{i=1}^n (-1)^{i+1} F(x_i) \right|^{1+\delta} } 
\leq \eta_\delta(0) = 4^{1+\delta} f^{2+\delta}(0).
\]

\subsubsection*{
Proof of Lemma~\ref{lem:bound_intervals_delta}}
Denote 
\[
\delta_n(x_1,\ldots,x_n) \triangleq \sum_{i=1}^{n} s_i f(x_i),
\]
\[
 \Delta_n(x_1,\ldots,x_n) \triangleq  \sum_{i=1}^n s_i F(x_i), 
\]
where $s_i \triangleq (-1)^{i+1}$. We use induction on $n \in \N$ to show that 
\begin{align}
\label{eq:lemm:interval_bounds:to_show}
\frac{ \left| \delta_n(x_1,\ldots,x_n) \right|^{2+\delta}} 
{\left|\Delta_n(x_1,\ldots,x_n)\left(1- \Delta_n(x_1,\ldots,x_n) \right) \right|^{1+\delta} } \leq \max_{i}\eta_{\delta}(x_i).
\end{align}
 Since 
\[
\eta_\delta(x) =  \frac{  \left|\delta_1(x) \right|^{2+\delta}}{\left|\Delta_1(x)
(1-\Delta_1(x)) \right|^{1+\delta}}, 
\]
The case $n=1$ is trivial.  
%follows from the assumption that $\eta_\delta(x)$ is non-increasing in $|x|$. 
Assume that \eqref{eq:lemm:interval_bounds:to_show} holds for all integers up to $n = N$ and for any $x_1 \ge \ldots \ge x_N$. Consider the case $n = N+1$. Let $i^*$ be the index such that $x_{i^*}$ has minimal absolute value among $x_1,\ldots,x_N$. The assumption on $\eta_\delta(x)$ implies that
\[
\eta_\delta(x_{i^*}) = \max_i \eta_\delta(x_i).
\]
Since the LHS of \eqref{eq:lem_bound_intervals_delta} is invariant to a sign flip of all $x_1,\ldots,x_{N+1}$, we may assume that $x_{i^*}$ is positive without loss of generality. 
%For simplicity, we also assume that $s_{i^*} = 1$ and note that the case $s_{i^*} = -1$ is obtained using identical arguments. 
Set $x^* = x_{i^*}$ and let $k=i^*-1$. In what follows, variables with subscript of non-positive index are ignored in summations and in lists of arguments to functions. Consider the function
\begin{align}
& g(y_1,\ldots,y_N) \triangleq g(y_1,\ldots,y_N|x^*,k) \\
&  \triangleq  \frac{\left| \delta_{N+1}(y_1,\ldots,y_k,x^*,y_{k+1}\ldots,y_N) )\right|^{2+\delta}}{
\left|\Delta_{N+1}(y_1,\ldots,y_k,x_{i^*},y_{k+1}\ldots,y_N)  (1 -\Delta_{N+1}(y_1,\ldots,y_k,x^*,y_{k+1}\ldots,y_N)  \right|^{1+\delta}
} \label{eq:g_def}.
\end{align}
The LHS of \eqref{eq:lemm:interval_bounds:to_show} is obtained by taking $y_i=x_{k_i}$ where $k_i$ is the $i$th element in $\{1,\ldots,N+1\}\setminus \{i^*\}$. It is therefore enough to prove that 
\[
\max_{(y_1,\ldots,y_N) \in A_N} g(y_1,\ldots,y_N) \leq \eta_{\delta}(x^*),
\]
where 
\[
A_N(x^*,k) \triangleq \left\{ (y_1,\ldots,y_N) \in \R^N\, : \, y_1 \ge y_k \ge x^* \ge -x^* \ge y_{k+1} \ldots \ge y_N
\right\}.
\]
%
Since $f(x)$ is log-concave, symmetric, and differentiable, we may write $f(x) = e^{c(x)}$ where $c(x)$ is concave, symmetric, and differentiable with derivative 
\[
c'(x) \triangleq \frac{f'(x)}{f(x)} 
\]
that is non-increasing. We first prove the lemma under the assumption that $c'(x)$ is an injection, or, equivalently, that $c(x)$ is strictly decreasing for all $x\in \R$. \par
%
The maximal value of $g(y_1,\ldots,y_N)$ is attained for the same $(y_1,\ldots,y_N) \in A_N(x^*,k)$ that maximizes
\begin{align*}
 \log(g)(y_1,\ldots, y_N) =  (2+\delta) \log \left( \delta_N  \right)  - (1+\delta) \log \left( \Delta_N  \right) - (1+\delta) \log \left(1 - \Delta_N \right),
\end{align*}
where in the last display and henceforth we suppress the arguments $y_1,\ldots,y_k,x^*,y_{k+1},\ldots, y_N$ of the functions $\delta_N$ and $\Delta_N$.
%
Within the interior of $A_N(x^*,k)$, all three expressions in \eqref{eq:g_def} within an absolute value are positive. It follows that partial derivative of $\log(g)(y_1,\ldots,y_N)$ with respect to $y_i$ within the interior of $A_N(x^*,k)$ is given by
\[
\frac{\partial \log(g)}{\partial y_i} = \frac{(2+\delta) s_i f'(x_i)}{\delta_N } -\frac{(1+\delta) s_i f(x_i)}{\Delta_N  } + \frac{(1+\delta)s_i f(y_i)}{1-\Delta_N }.
\]
We conclude that the gradient of $\log(g)$ vanishes if and only if
\begin{equation}
\label{eq:gradient_zero}
c'(y_i) = \frac{f'(y_i)}{f(y_i)} = \frac{1+\delta}{2+\delta} \frac{\delta_N}{2} \left( \frac{1}{\Delta_N} - \frac{1}{1-\Delta_N } \right),\quad i=1,\ldots,N.
\end{equation}
%
Since we assumed that $c'(x)$ is an injection, \eqref{eq:gradient_zero} is satisfied if and only if $y_1 = \ldots = y_N$. In this case, 
$g(y_1,\ldots,y_N) = \eta_\delta(x_{i^*})$  
if $N$ is even. If $N$ is odd and $y_1 = \ldots = y_N > x^*$, then 
\begin{align*}
& g(y_1,\ldots,y_N) = \frac{\left| f(y_1)-f(x_{i^*})  \right|^{2+\delta}} { 
\left|  F(y_1) - F(x_{i^*}) \right|^{1+ \delta} 
\left| 1 - (F(y_1) -F(x_{i^*})) \right|^{1+ \delta} } 
\end{align*} 
which is bounded from from above by $\eta_\delta(x_{i^*})$ by the induction hypothesis. The case where $N$ is odd and $-x^* \leq y_1 = \ldots = y_N$ is similar. 
%
We now consider the possibility that the maximum of $g(y_1,\ldots,y_N)$ is attained at the boundaries of $A_N(x^*,k)$. At boundary points for which $y_i = y_{i+1}$ for some $i$, the contribution of $y_i$ and $y_{i+1}$ to $g(y_1,\ldots,y_N)$ is zero and the induction assumption for $n=N-1$ implies that 
\[
g(y_1,\ldots,y_N) \leq \eta_{\delta}(x^*)
\]
The remaining boundary points of $A_N(x^*,k)$ are covered by the following cases:
\begin{itemize}
\item[(1)]  $y_N \to -\infty$. 
\item[(2)] $y_1 \to \infty$.
\item[(3)] $y_k = x_{i^*}$.
\item[(4)] $y_{k+1} = -x_{i^*}$. 
\end{itemize}
For case (1), 
\begin{align*}
g(y_1,\ldots,y_N) \to \frac{ \left| \sum_{i=1}^{k} s_i f(y_i) + s_{i^*} f(x_{i^*}) - \sum_{i=k+1}^{N-1} s_i f(y_i) \right|^{2+\delta}} 
{\left| \sum_{i=1}^{k} s_i F(y_i) + s_{i^*} F(x_{i^*}) - \sum_{i=k+1}^{N-1} s_i F(y_i) \right|^{1+\delta}\left|1- \left( \sum_{i=1}^{k} s_i F(y_i) + s_{i^*} F(x_{i^*}) - \sum_{i=k+1}^{N-1} s_i F(y_i) \right)  \right|^{1+\delta} },
\end{align*}
which is smaller than $\eta_\delta(x_{i^*})$ by the induction hypothesis. Similarly, under case (2),
\begin{align*}
& g(y_1,\ldots,y_N) \to 
\frac{ \left| \sum_{i=2}^{k} s_i f(y_i) + s_{i^*} f(x_{i^*}) - \sum_{i=k+1}^{N} s_i f(y_i) \right|^{2+\delta}} 
{\left| 1+ \sum_{i=2}^{k} s_i F(y_i) + s_{i^*} F(x_{i^*}) - \sum_{i=k+1}^{N} s_i F(y_i) \right|^{1+\delta}\left|-\left( \sum_{i=2}^{k} s_i F(y_i) + s_{i^*} F(x_{i^*}) - \sum_{i=k+1}^{N} s_i F(y_i) \right)  \right|^{1+\delta} },
 \\
& = \frac{ \left| -\sum_{i=2}^{k} s_i f(y_i) - s_{i^*} f(x_{i^*}) + \sum_{i=k+1}^{N} s_i f(y_i) \right|^{2+\delta}} 
{ \left|1 - \left(-\sum_{i=2}^{k} s_i F(y_i) - s_{i^*} F(x_{i^*}) + \sum_{i=k+1}^{N} s_i F(y_i) \right) \right|^{1+\delta} 
\left|-\sum_{i=2}^{k} s_i F(y_i) - s_{i^*} F(x_{i^*}) + \sum_{i=k+1}^{N} s_i F(y_i) \right|^{1+\delta} }
\end{align*}
which is smaller than $\eta_{\delta}(x_{i^*})$ by the induction hypothesis. Under case (3), the terms in $\delta_N$ and $\Delta_N$ corresponding to $y_k$ and $x_{i^*}$ cancel each other. As a result,  $g(y_1,\ldots,y_N)$ reduces to an expression with $n=N-1$ variables hence this case is handled by the induction hypothesis. 
%
Finally, under case (4), set 
\[
d \triangleq s_k F(-x^*) + s_{i^*} F(x^*) = s_{i^*}\left(1-2F(-x^*) \right), 
\]
\[
\sigma \triangleq \sum_{i=1}^{k-1} s_i f(y_i) - \sum_{i=k+1}^{N} s_i f(y_i). 
\]
and 
\[
\Sigma \triangleq \sum_{i=1}^{k-1} s_i F(y_i) - \sum_{i=k+1}^{N} s_i F(y_i). 
\]
We have
\begin{align*}
& g(y_1,\ldots,y_N) =\nonumber  \\ 
& =
\frac{ \left| \sum_{i=1}^{k-1} s_i f(y_i) - \sum_{i=k+1}^{N} s_i f(y_i) \right|^{2+\delta}} 
{\left| \sum_{i=1}^{k-1} s_i F(y_i) + d(x^*) - \sum_{i=k+1}^{N} s_i F(y_i) \right|^{1+\delta} \left|1- \sum_{i=1}^{k-1} s_i F(y_i) - d(x^*) + \sum_{i=k+1}^{N} s_i F(y_i)   \right|^{1+\delta} }, \\
& = \frac{ \left| \sigma \right|^{2+\delta}} 
{\left| \Sigma+d \right|^{1+\delta} \left|1- \Sigma - d \right|^{1+\delta} }  =  
\frac{ \left| \sigma \right|^{2+\delta}} 
{\left| \Sigma \right|^{1+\delta} \left|1- \Sigma \right|^{1+\delta} }  \left| \frac{\Sigma(1-\Sigma) } { \Sigma(1-\Sigma) + d(1-2\Sigma)-d^2} \right|^{1+\delta}. 
\end{align*}
By the induction hypothesis,
\[
\frac{ \left| \sigma \right|^{2+\delta}} 
{\left| \Sigma \right|^{1+\delta} \left|1- \Sigma \right|^{1+\delta} } \leq \eta_\delta(x^*), 
\]
hence it is left to show that 
\[
 \frac{\Sigma(1-\Sigma) } { \Sigma(1-\Sigma) + d(1-2\Sigma)-d^2} \leq 1.
\]
Whenever $d>0$, 
\begin{align*}
& \frac{ \Sigma(1-\Sigma) + d(1-2\Sigma)-d^2} {\Sigma(1-\Sigma)} \geq 1 \Leftrightarrow  1-2\Sigma \geq d, 
\end{align*}
while for $d<0$,
\begin{align*}
& \frac{ \Sigma(1-\Sigma) + d(1-2\Sigma)-d^2} {\Sigma(1-\Sigma)} \geq 1 \Leftrightarrow  1-2\Sigma \leq d. 
\end{align*}
Therefore, it is enough to show that $\Sigma \leq F(-x^*)$ if $s_{i^*}=1$ and 
$\Sigma \geq F(-x^*)$ if $s_{i^*}=-1$. 
%
Indeed, if $s_{i^*}=1$, then $s_{k+1}=-1$ and monotonicity of $F(x)$ implies that 
\[
\Sigma + d \leq F(y_1) - F(y_k) + F(x^*) - F(-x^*) + F(y_{k+2}) - F(y_N), 
\]
and hence
\[
\Sigma \leq 1-F(x^*) = F(-x^*). 
\]
Similarly, if $s_{i^*}=-1$ then 
\[
1 - \Sigma \leq 1 -  F(-x^*).
\]
This conclude the proof in the case where $c'(x)$ is an injection. 
\par  
In the case where $c'(x)$ is not necessarily strictly decreasing, we approximate $c(x)$ using another concave symmetric function whose derivative is always negative except, perhaps, at the origin. For $\alpha>0$ consider the function  $f_\alpha(x) = \kappa(\alpha) e^{\sgn(c(x))|c(x)|^{1+\alpha}}$, where $\kappa(\alpha)$ is chosen such that $f_\alpha(x)$ is a probability density function. Then $c_\alpha(x)$ is concave, symmetric, and differentiable with
\[
c_\alpha'(x) \triangleq \frac{f'_\alpha(x)}{f_\alpha(x)} = (1+\alpha)|c(x)|^{\alpha} c'(x). 
\]
Now $c_\alpha'(x)$ is non-increasing since it is the derivative of a concave function. Furthermore, since $c(x)$ is non-constant on any interval and $c'(x)$ is non-increasing, $c_\alpha'(x)$ is non-constant on any interval hence an injection. It follows from the first part of the proof that, for any $\alpha>0$,
\begin{align}
\label{eq:proof:lem:bound_intervals}
\frac{(\delta_{n,\alpha})^2}{\Delta_{n,\alpha}(1-\Delta_{n,\alpha})} \leq \max_i \eta_{\alpha}(x_i),
\end{align}
where 
\[
\delta_{n,\alpha} \triangleq  \sum_{k=1}^{n} (-1)^{k+1} f_{\alpha}(x_k),
\]
\[
\Delta_{n,\alpha} \triangleq \sum_{k=1}^n (-1)^{k+1} F_{\alpha}(x_k), 
\]
and 
\[
\eta_{\delta,\alpha}(x) \triangleq \frac{(f_\alpha(x))^{2+\delta}}{\left(F_{\alpha}(x)(1-F(x)) \right)^{1+\delta}}. 
\]
The proof is completed by noting that 
\begin{align*}
\lim_{\alpha \to 0} \frac{(\delta_{n,\alpha})^{2+\delta} }{ \left(\Delta_{n,\alpha}(1-\Delta_{n,\alpha})\right)^{1+\delta}}  = \frac{(\delta_{n})^{2+\delta }}{\left(\Delta_{n}(1-\Delta_{n}) \right)^{1+\delta}},  
\end{align*}
and, since the maximum is over a finite set,
\begin{align*}
\lim_{\alpha \to 0}  \max_i \eta_{\delta,\alpha}(x_i)  = \max_i\eta_\delta(x_i).
\end{align*}

%Our goal is to construct an approximation to $f(x)$ using a log-concave, symmetric, and differentiable function $\hat{f}(x)$ such that $\hat{f}'(x)/\hat{f}(x)$ is strictly decreasing. We achieve such an approximation by modifying $f(x)$ over intervals over which $c'(x)$ is a constant. Assume first $c'(x)$ is only a constant over an interval $(a_1,b_1)\subset \R$ containing $x_1>0$ and on the mirror image of this interval around $x=0$. Over this interval, $f(x)$ is necessarily of the form $f(x) = e^{-c_1|x| + d_1}$ for some $c_1\geq 0$ and $d_1$. % and For $\alpha>0$, consider
%\[
%F_1(x,\alpha) \triangleq  F(a_1) + \frac{1}{c_1} e^{-c_1 (|x|^{1+\alpha} +d_1}, \quad x \in (a_1,b_1),
%\]
%and 
%\[
%f_1(x,\alpha) \triangleq  F_1'(x,\alpha) =  -\sgn(x)(1+\alpha)|x|^\alpha e^{-c_1 |x|^{1+\alpha} + d_1}, \quad x \in (a_1,b_1).
%\]



\QEDA 

\subsection{Proof of Theorem~\ref{thm:sgd}
\label{proof:thm:sgd}
}
The estimation algorithm of \eqref{eq:sgd_alg} and \eqref{eq:sgd_est} is a special case of a more general class of estimation procedures given in \cite{polyak1992acceleration} and \cite{polyak1990new}. The proof of Theorem~\ref{thm:sgd} relies on various results from these works. 

\subsubsection*{Proof of (i)}
%Specifically, (i) in Theorem~\ref{thm:sgd} 
Consider the following simplified version of \cite[Thm. 4]{polyak1992acceleration}:
\begin{thm}{\cite[Thm. 4]{polyak1992acceleration}} \label{thm:polyak_juditsky}
Let 
\[
X_i = \theta + Z_i,\quad i=1,\ldots,n,
\]
where the $Z_i$s are i.i.d. with zero means and finite variances. Define
\begin{align}
\begin{split}
\theta_i & = \theta_{i-1} + \gamma_i \varphi(X_i - \theta_{i-1}), \\
\bar{\theta}_n & = \frac{1}{n} \sum_{i=0}^{n-1} \theta_i, 
\end{split}
\label{eq:Polyak_Juditsky_alg}
\end{align}
where in addition, assume the following: 
\begin{enumerate}
\item[(i)] There exits $K_1$ such that $\left| \varphi(x) \right| \leq K_1(1+|x|)$ for all $x\in \R$.
\item[(ii)] The sequence $\left\{ \gamma_i \right\}_{i=1}^\infty$ satisfies conditions \eqref{eq:conditions1}.
\item[(iii)] The function $\psi(x) \triangleq \ex{ \varphi(x+Z_1)}$ is differentiable at zero with $\psi'(0)>0$, and satisfies $\psi(0)=0$ and $x\psi(x) >0$ for all $x\neq 0$.
Moreover, assume that there exists $K_2$ and $0<\lambda \leq 1$ such that
\begin{equation}
\label{eq:Polyak_Juditsky_cond3}
\left| \psi(x) - \psi'(0)x \right|\leq K_2 |x|^{1+\lambda}.
\end{equation}
\item[(iv)] The function 
$\chi(x) \triangleq \ex{\varphi^2(x+Z_1)}$ is continuous at zero. 
\end{enumerate}
Then $\bar{\theta}_n \rightarrow \theta$ almost surely and $ \sqrt{n}({\theta}_n - \theta)$ converges in distribution to $\Ncal(0,V)$, where
\[
V = \frac{ \chi(0)} {\psi'^2(0)}. 
\]
\end{thm}

Using the notation above, we set $\varphi(x) = \sgn(x)$ and $Z_i = X_i - \theta$. We have that $\chi(x) = \ex{ \sgn^2(x+Z_1) }= 1$, so $\chi(0) = 1$. In addition,
\begin{align*}
\psi(x) & = \ex{ \sgn(x+ Z_1) }= \int_{-\infty}^\infty \sgn(x+z) f(z) dz \\
& = \int_{-x}^\infty f(z) dz -\int_{-\infty}^{-x} f(z) dz. 
\end{align*}
Using the symmetry of $f(x)$ around zero, it follows that $\psi'(x) = 2f(x)$ and thus $\psi'(0) = 2f(0)$. It is now easy to verify that the rest of the conditions in Theorem~\ref{thm:polyak_juditsky} are fulfilled for any $\lambda > 0$. Since 
\[
\frac{\chi(0)}{\psi'^2(0)} = \frac{1}{4 f^2(0)} = \frac{1}{\eta(0)},
\]
it follows from Theorem~\ref{thm:polyak_juditsky} that
\[
\sqrt{n}\left({\theta}_n-\theta \right) \overset{d}{\to} \Ncal \left(0, 1/ \eta(0)\right). 
\]

\subsubsection*{Proof of (ii)}
We first show that the estimator $\bar{\theta}_n$ is regular in the following sense: For $\theta \in \Theta$, $h\in \R$ and $n$ large enough such that $\theta+h/\sqrt{n} \in \Theta$, let $\Prob_{\theta,n}$ be a product probability measure on $\R^n$ with density $f(x-\theta - h/\sqrt{n})$ in each of its $n$ coordinates. Then 
\begin{align}
\label{eq:sgd_part2}
\sqrt{n}\left( \bar{\theta}_n - \theta\right) \overset{d}{\to} \Ncal\left( h,\frac{1}{\eta(0)}\right),
\end{align}
under $\Prob_{\theta,n}$. In order to show \eqref{eq:sgd_part2} we use the following refinement of Theorem~\ref{thm:polyak_juditsky}, proof of which is given in Subsection~\ref{proof:thm:normal_expansion} below.
%
\begin{thm} \label{thm:normal_expansion}
Set $\Delta_i = \theta_i - \theta$ and
$\bar{\Delta_i} = \frac{1}{n} \sum_{i=1}^n \Delta_i$. Assume that, in addition to Assumptions (i)-(iv) of Theorem~\ref{thm:polyak_juditsky}, there exists $K_1$ and $\lambda>0$ such that
\begin{equation}
\ex{\left| \varphi(Z_1) - \varphi(x+Z_1)  \right|} \leq K_1 |x|^{1+\lambda}
\label{eq:PJ_additional_cond}. 
\end{equation}
Then:
\begin{itemize}
\item[(i)] \begin{equation}
\sqrt{n} \bar{\Delta}_n = -\frac{1}{\sqrt{n}} \frac{1}{\psi'(0)} \sum_{i=1}^{n-1} \varphi(Z_i)+ o_{p.n}(1). \label{eq:normal_expansion_lem}
\end{equation}
where $o_{p,n}(1)$ converges in probability to $0$ as $n\to \infty$.
\item[(ii)] If $Z_1$ has continuously differentiable density $f(x)$ with finite Fisher information for location $\sigma_f^2$, then for any converging sequence $h_n \to h$,
\[
\sqrt{n} \left( \bar{\Delta}_n \right) \overset{d}{\to} \Ncal\left( \frac{-h}{\psi'(0)} \int_{\R} \varphi(x) f'(x) dx  , \frac{\chi(0)}{\psi'^2(0)} \right)
\]
under the local alternative $Z_1,\ldots,Z_n \sim \Prob^n_{h_n/\sqrt{n}}$ with density $\prod_{i=1}^n f(x_i-h_n/\sqrt{n})$.
\end{itemize}
\end{thm}
In our setting, we have
\begin{align*}
& %\varphi(Z_1) - \varphi(x+Z_1) = 
\sgn(Z_1) - \sgn(x + Z_1) \\
& = \begin{cases}
2 & Z_1 > 0,\, x+Z_1<0, \\
-2 & Z_1 <0, \, x+Z_1>0, \\
0 & \text{otherwise}.
\end{cases}
\end{align*}
It follows that
\begin{align*}
\ex{ \left| \varphi(Z_1) - \varphi(x+Z_1) \right| }  \leq \Prob\left( |Z_1| < x  \right) \leq f(0) |x|, 
\end{align*}
and hence condition \eqref{eq:PJ_additional_cond} is fulfilled. In addition, by anti symmetry of $f'(x)$ around $x=0$, 
\[
\int_{\R} \varphi(x) f'(x) dx = \int_{\R} \sgn(x) f'(x) dx = 2\int_0^\infty f'(x) dx = -2f(0) = -\psi'(0). 
\]
Theorem~\ref{thm:normal_expansion}, applied to the setting of Theorem~\ref{thm:sgd}, implies \eqref{eq:sgd_part2}. \par

Under the assumption that $f(x)$ is continuously differentiable with a finite Fisher information for location, the model $\{Z_n+ \theta\}_{n \in \N}$ is differentiable in quadratic mean \cite[Exm. 7.8]{van2000asymptotic} and hence local asymptotically normal (LAN) in the sense that
\[
\log \left(\frac{\Prob_{\theta,n}(X^n)}{\Prob_{\theta} (X^n) }\right) = h \eta^{-1/2}(0) Z - \frac{1}{2} h^2 \eta^{-1}(0)  + o_{p,n}(1),
\]
where $Z\sim \Ncal(0,1)$ and $o_{p,n}(1) \to 0$ as $n\to \infty$ under $\Prob_{\theta}$. The proof is concluded since any regular estimator in LAN model satisfies \eqref{eq:attaining_LAM} \cite{beran1995role}. 

\subsubsection*{Proof of (iii)}
%In order to prove part (ii) of Theorem~\ref{thm:sgd} 
Consider the following result from \cite{polyak1990new}:
\begin{thm}{\cite[Thm. 2]{polyak1990new}} \label{thm:polyak_new}
Let
\begin{align} \label{eq:polyak_new_measurements}
\begin{cases}
U_n = U_{n-1} - \gamma_n \varphi(Y_n), & Y_n = g'(U_{n-1})+Z_n \\
\bar{U}_n= \frac{1}{n} \sum_{i=1}^n U_n, & n=1,2,\ldots.
\end{cases}
\end{align}
Assume that the function $g(x)$ is twice differentiable with a strictly positive and uniformly bounded second derivative. In particular, $g(x)$ is convex with a unique minimizer $x^\star \in \R$. Moreover, assume that the noises $Z_n$ are uncorrelated and identically distributed with a distribution with a density for which the Fisher information exits. Let $\psi(x)$ and $\chi(x)$ be defined as in Theorem~\ref{thm:polyak_juditsky}-(iii) and satisfy the conditions there. Assume in addition that $\chi(0)>0$, condition \eqref{eq:Polyak_Juditsky_cond3} with $\lambda = 1$, 
and there exits $K_3$ such that 
\[
\ex{  | \varphi(x+Z_1) |^4 } \leq K_3(1+|x|^4). 
\]
Finally, assume that the sequence $\{\gamma_n \}$ satisfies conditions \eqref{eq:conditions1} and \eqref{eq:conditions2}. Then
\[
V_n \triangleq \ex{ \left(\bar{U}_n-x^\star \right)^2 } = n^{-1}\frac{\chi(0)} { (\psi'(0))^2 (g''(x^\star))^2 } + o(n^{-1}).
\]
\end{thm}

We now use Theorem~\ref{thm:polyak_new} with $g(x) = 0.5(x-\theta)^2$, $\varphi(x) = \sgn(x)$, $Z_n = \theta-X_n$. From \eqref{eq:polyak_new_measurements} we have
\begin{align*} 
U_n & = U_{n-1} + \gamma_n \sgn(\theta-U_{n-1} - Z_n )  \\
& = U_{n-1} + \gamma_n \sgn(X_n-U_{n-1} ),
\end{align*}
so the estimator $\bar{U}_n$ equals to the one defined by \eqref{eq:sgd_est} and \eqref{eq:sgd_alg}. Note that
\[
\ex{ | \varphi(x+Z_1) |^4 } = 1 \leq K_3(1+|x|^4)
\]
for any $K_3\geq 1$, the Fisher information of $Z_1$ is $\sigma^2$, $\chi(x) = 1 > 0$, and that 
the conditions in Theorem~\ref{thm:polyak_new} on $\psi(x)$ and $\chi(x)$ were verified to hold in the first part of the proof. In particular, $\psi'(0) = (2f(0))^{-2}$. Since $f(x)$ satisfies the conditions above with $x^\star = \theta$ and $g''(x) = 1$. Theorem~\ref{thm:polyak_new} implies that for any $\theta \in \R$, 
\[
V_n = \ex{ \left({\theta}_n-\theta \right)^2 } = \frac{1}{4n f^2(0)} + o(n^{-1}).
\]

\subsection{Proof of Theorem~\ref{thm:normal_expansion}
\label{proof:thm:normal_expansion}
}
\subsubsection*{Proof of (i)}
The proof of part (i) of Theorem~\ref{thm:normal_expansion} requires the following two additional results from \cite{polyak1992acceleration}:
\begin{lem}{\cite[Lem. 2]{polyak1992acceleration}}
\label{lem:Polyak_expansion}
Consider the process $\{\Delta_i^1 \}_{i=0}^\infty$ defined by
\[
\Delta^1_i = \Delta^1_{i-1} - \gamma_i (A \Delta_{i-1}+ \xi_i),\quad i=1,2\ldots.
\]
Assume that $A>0$ and condition (ii) of Theorem~\ref{thm:polyak_juditsky} holds. Then 
\begin{equation}
\label{eq:Polyak_expansion}
\sqrt{n} \bar{\Delta}_n^1 = \frac{1}{\sqrt{n}}\sum_{i=0}^{n-1} \Delta_i^1 = \frac{\alpha_n \Delta_0^1}{\sqrt{n} \gamma_0}  + \frac{1}{\sqrt{n} A} \sum_{i=1}^{n-1} \xi_i + \frac{1}{\sqrt{n}}\sum_{i=1}^{n-1} w_i^n \xi_i,
\end{equation}
where $\alpha_n$ and $w_i^n$ are real numbers such that $|\alpha_n| \leq K$ and $|w_i^n|\leq K$ for some $K< \infty$, and 
\[
\lim_{n\to \infty} \frac{1}{n} \sum_{i=1}^{n-1} |w_i^n| = 0. 
\]
\end{lem} 

\begin{lem} \label{lem:PJ_converging_sum}
Under the conditions of Theorem~\ref{thm:polyak_juditsky},
\[
\sum_{i=1}^\infty \frac{|\Delta_{i}|^{1+\lambda}}{\sqrt{i}} < \infty 
\]
almost surely. 
\end{lem}
Lemma~\ref{lem:PJ_converging_sum} follows from the proof of Theorem 2 in \cite{polyak1992acceleration}. 

We separate the proof of part (i) into two steps.
\subsubsection*{Step I}
The expansion \eqref{eq:normal_expansion_lem} holds for the process  $\{\bar{\Delta}^1_i\}_{i=1}^\infty$ defined as follows:
\begin{align} \label{eq:Polyak_expansion_lem1_alg}
& \Delta_i^1  = \Delta_{i-1}^1 - \gamma_i \psi'(0) \Delta_{i-1}^1 - \gamma_i \varphi(Z_i), \qquad
 \delta_0^1 = \Delta_0\\
& \bar{\Delta}^1_i = \frac{1}{n}\sum_{i=0}^{n-1} \Delta^1_i.
\end{align}

In order to prove this claim, use Lemma~\ref{lem:Polyak_expansion} with $A = \psi'(0)$ and $\xi_i = -\varphi(Z_i)$. The first expression on the RHS of \eqref{eq:Polyak_expansion} goes to zero in variance. In addition, 
\begin{align*}
& \ex{ \left( \frac{1}{\sqrt{n}} \sum_{i=1}^{n-1} w_i^n \xi_i \right)^2 }  = \frac{1}{n}  \sum_{i=1}^n (w_i^n)^2 \ex{ \xi_i^2} + \frac{1}{n}  \sum_{i\neq j}^n w_i^n w_j^n \ex{ \xi_i \xi_j} \\
& = \frac{1}{n}  \sum_{i=1}^n (w_i^n)^2 \ex{ \varphi(Z_i)^2} = \chi(0) \frac{1}{n}  \sum_{i=1}^n (w_i^n)^2 \to 0. 
\end{align*}
We obtain
\begin{equation}
\sqrt{n} \bar{\Delta}^1_n = -\frac{1}{\sqrt{n}} \frac{1}{\psi'(0)} \sum_{i=1}^{n-1} \varphi(Z_i)+ o_{p.n}(1), \label{eq:Polyak_expansion_aux_process}
\end{equation}

\subsubsection*{Step II} $\bar{\Delta}_n$ and $\bar{\Delta}^1_n$ are asymptotically equivalent. \\

From \eqref{eq:Polyak_Juditsky_alg} and \eqref{eq:Polyak_expansion_lem1_alg}, the difference $\delta_i = \Delta_i - \Delta_i^1$ satisfies the recursion
\[
\delta_i = \delta_{i-1} - \gamma_i \psi'(0) \delta_{i-1} + \gamma_i \left( \psi'(0) \Delta_{i-1}  + \varphi(Z_i) - \varphi(\Delta_{i-1} + Z_i) \right),
\]
where $\delta_0 = 0$. Use Lemma~\ref{lem:Polyak_expansion} with $\xi_i =  \psi'(0) \Delta_{i-1}  + \varphi(Z_i) - \varphi(\Delta_{i-1} + Z_i)$ to obtain
\begin{align}
& \sqrt{n}\bar{\delta}_n = \frac{1}{\sqrt{n}} \sum_{i=1}^{n-1} \left( \frac{1}{\psi'(0)}  + w_i^n \right)  \xi_i  \\
& = \frac{1}{\sqrt{n}} \sum_{i=1}^{n-1} \left( \frac{1}{\psi'(0)}  + w_i^n \right)  \left( \psi'(0) \Delta_{i-1}  - \psi(\Delta_{i-1}) \right) \label{eq:PJ_proof1} \\
& + 
\frac{1}{\sqrt{n}} \sum_{i=1}^{n-1} \left( \frac{1}{\psi'(0)}  + w_i^n \right)  \left( \psi(\Delta_{i-1})  + \varphi(Z_i) - \varphi(\Delta_{i-1}+Z_i)
\right) \label{eq:PJ_proof2}
\end{align}
For the term \eqref{eq:PJ_proof1} and using \eqref{eq:Polyak_Juditsky_cond3}, there exists $K_1$ and $K_2$ such that 
\begin{align*}
& \eqref{eq:PJ_proof1}
\leq K_1 \sum_{i=1}^\infty \frac{1}{\sqrt{i}} \left| \left( \psi'(0) \Delta_{i-1}  - \psi(\Delta_{i-1}) \right) \right| \\
& \leq K_2 \sum_{i=1}^\infty \frac{ |\Delta_i|^{1+\lambda}}{\sqrt{i}}. 
\end{align*}

Lemma~\ref{lem:PJ_converging_sum} shows that 
\begin{align}
\sum_{i=1}^\infty \frac{|\Delta_i|^{1+\lambda}}{\sqrt{i}}  < \infty \label{eq:PJ_converging_sum},
\end{align}
hence the Kronecker lemma implies
\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n-1} \left( \frac{1}{\psi'(0)}  + w_i^n \right)  \left( \psi'(0) \Delta_{i-1}  - \psi(\Delta_{i-1}) \right) \to 0.
\]
For the term \eqref{eq:PJ_proof2}, set 
\[
\epsilon_i \triangleq \psi(\Delta_{i-1}) + \varphi(Z_i) - \varphi(\Delta_{i-1}+Z_i).
\]
For $a>0$ and $n \in \N$, define the event
\[
A_{n,a} \triangleq \left\{ \sum_{i=1}^{n-1} \frac{|\epsilon_i|}{\sqrt{i}}  \geq a  \right\}. 
\]
By Markov's inequality, we have
\begin{align}
\ex{\mathbf 1_{A_{n,a}}   \mid \Delta_0,\ldots,\Delta_{n-1} } \leq \frac{1}{a} \sum_{i=1}^{n-1} \frac{ \ex{|\epsilon_i| \Delta_{i-1} \mid }}{\sqrt{i}}. \label{eq:PJ_proof_3}
\end{align}
Using \eqref{eq:Polyak_Juditsky_cond3} and \eqref{eq:PJ_additional_cond}, there exists $K'$ and $\lambda'>0$ such that
\begin{align*}
& \ex{ |\epsilon_i| | \Delta_{i-1}}  \leq |\psi(\Delta_{i-1})| + \left| \varphi(Z_i) - \varphi(\Delta_{i-1}+Z_i) \right|  \\
& \leq K' |\Delta_{i-1}|^{1+\lambda}. 
\end{align*}
Plugging this bound in \eqref{eq:PJ_proof_3} and using Lemma~\ref{lem:PJ_converging_sum}, we obtain 
\[
\Prob(A_{n,a}) = \ex{ \ex{\mathbf 1_{A_{n,a}}   \mid \Delta_0,\ldots,\Delta_{n-1} } } \leq  \frac{K''}{a}
\]
for some constant $K''$. It follows that for any $\epsilon$, we may choose $a$ large enough such that 
\[
\sup_{n} \Prob(A_{n,a}) < \epsilon.
\]
This implies that for any $\epsilon>0$,
\[
\Prob \left( \sum_{i=1}^\infty \frac{|\epsilon_i|}{\sqrt{i}} < \infty \right) \geq 1-\epsilon,
\]
and hence 
\[
\sum_{i=1}^\infty \frac{|\epsilon_i|}{\sqrt{i}} < \infty
\]
almost surely. The Kronecker lemma now implies 
\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n-1} \left|\epsilon_i\right| \to 0,
\]
hence the term \eqref{eq:PJ_proof2} satisfies
\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n-1} \left( \frac{1}{\psi'(0)}  + w_i^n \right) \epsilon_i \leq \frac{K'''}{\sqrt{n}} \sum_{i=1}^{n-1} |\epsilon_i| \to 0. 
\]
This conclude the proof of part (i).

\subsubsection*{Part (ii)}
Use \eqref{eq:normal_expansion_lem} to write
\[
\sqrt{n} \bar{\Delta}_n = G_n + o_{p,n}(1),
\]
where
\[
G_n \triangleq -\frac{1}{\sqrt{n}} \frac{1}{\psi'(0)} \sum_{i=1}^{n} \varphi(Z_i). 
\]
From \cite[Exm. 7.8]{van2000asymptotic}, the location model $f(x-\theta)$ with continuously differentiable $f(x)$ is differentiable in quadratic mean. % with the score function $-f'(x-\theta)/f(x-\theta)$. 
This fact implies the following expansion \cite[Thm. 7.2]{van2000asymptotic}:
\begin{align}
\label{eq:PJ_LAN}
& \log \frac{\Prob^n_{h_n/\sqrt{n}}}{\Prob^n_0} (Z_1,\ldots,Z_n) =  \log \prod_{i=1}^{n} \frac{f(Z_i-h_n/\sqrt{n})}  {f(X_i-\theta)} = h J_n - \frac{1}{2}h^2 I_{\theta} + o_{p,n}(1),
\end{align}
for any converging sequence $h_n \to h$, 
where 
\[
J_n \triangleq -\frac{1}{\sqrt{n}} \sum_{i=1}^n \frac{f'(Z_i)}{f(Z_i)} .
\]
We have
\begin{align*}
& \ex{ G_n J_n} = \frac{1}{\psi'(0)}\frac{1}{n} \sum_{i=1}^n \ex{ \varphi(Z_i) \frac{f'(Z_i)}{f(Z_i)} }  \\
& = \frac{1}{ \psi'(0)} \ex{ \varphi(Z_1) \frac{f'(Z_1)}{f(Z_1)}} =  \frac{1}{ \psi'(0)} \int_{\R} \varphi(x) f'(x) dx , 
\end{align*}
Since both $\sqrt{n}G_n$ and $\sqrt{n}J_n$ are the sum of $n$ i.i.d. random variables with zero mean and finite variance, we obtain, from the central limit theorem and Slutsky's theorem, that
\[
\left(\sqrt{n} \bar{\Delta}_n, \log \frac{\Prob^n_{h/\sqrt{n}}}{\Prob^n_0} \right) \overset{d}{\to} \Ncal \left( \left(0,-\frac{h^2}{2} I_\theta \right),  \begin{pmatrix}
\frac{\chi(0)}{\psi'^2(0)} & \frac{-h}{ \psi'(0)} \int_{\R} \varphi(x) f'(x) dx \\
\frac{-h}{ \psi'(0)} \int_{\R} \varphi(x) f'(x) dx & h^2 I_\theta 
\end{pmatrix}  \right)
\]
Le Cam's third lemma \cite[Exm. 6.7]{van2000asymptotic} implies that under $\Prob^n_{h/\sqrt{n}}$, 
\[
\sqrt{n}\bar{\Delta}_n \overset{d}{\to} \Ncal\left(\frac{-h}{ \psi'(0)} \int_{\R} \varphi(x) f'(x) dx,  \frac{\chi(0)}{\psi'^2(0)} \right).
\]
\QEDA

\subsection{Proof of Theorem~\ref{thm:LAN1}
\label{proof:thm:LAN1}
}
The log probability mass distribution of $B^n=(B_1,\ldots,B_n)$ is given by
\begin{align*}
& \log \Prob_\theta(b^n) = \sum_{i=1}^n \left( \frac{b_i+1}{2} \log  \Prob(X_i\in A_i) + 
\frac{1-b_i}{2} \log \Prob(X_i\in A_i) \right), \quad b^n \in \{-1,1\}^n. 
\end{align*}
Consequently, 
\begin{align}
& \log \frac{ \Prob_{\theta + \frac{h}{\sqrt{n}}}(b^n)} { \Prob_\theta (b^n) } = \sum_{i=1}^n    \frac{b_i+1}{2} 
\log \frac{\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)} + 
\sum_{i=1}^n  \frac{1-b_i}{2} 
\log \frac{\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}. 
\label{eq:LAN_proof1}
\end{align}
For each $i=1,\ldots,n$, write 
\[
A_i = \bigcup_{k=1}^{K_i} \left(t_{i,k},t_{i,k+1} \right),
\]
where $t_{i,1}<\ldots<t_{i,K_i}$ and, with a slight abuse of notation, $t_{i,1}$ and $t_{i,K_i}$ may take the values $-\infty$ or $+\infty$, respectively. Thus
\[
\Prob_{\theta}(X_i \in A_i) = \sum_{k=1}^{K_i} (-1)^k F(x_{i,k}-\theta).
\]
In particular, since $f$ is differentible, $\Prob_{\theta}(X_i \in A_i)$ is twice differentiable, and we may write
\begin{align*}
\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \in A_i)  = \Prob_{\theta} (X_i \in A_i) + \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \frac{h}{\sqrt{n}} + o(h), 
\end{align*}
and thus
\begin{align*}
& \log \frac{\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)} = \log \left(1 + \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \frac{h}{\sqrt{n}} + o(h) \right) \\
& = \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \frac{h}{\sqrt{n}} - \frac{h}{2n} \left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \right)^2 + o(h^2). 
\end{align*}
Similarly, we have
\begin{align*}
& \log \frac{\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)} \\
& = \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}  \frac{h}{\sqrt{n}} - \frac{h}{2n} \left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}  \right)^2 + o(h^2). 
\end{align*}
From \eqref{eq:LAN_proof1} we obtain
\begin{align*}
& \log \frac{ \Prob_{\theta + \frac{h}{\sqrt{n}}}(b^n)} { \Prob_\theta (b^n) }   = \frac{h}{\sqrt{n}} \sum_{i=1}^n  
\left( 
\frac{b_i+1}{2}  \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  + 
 \frac{1-b_i}{2}  \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}   \right) \\
 & - \frac{h^2}{2n} 
 \sum_{i=1}^n 
\left( 
 \frac{b_i+1}{2} 
\left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \right)^2
+ 
 \frac{1-b_i}{2} \left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}  \right)^2 \right) + o(h^2) \\
\end{align*}
%
Noting that 
\[
\frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)} = \frac{-\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)},
\]
the proof is completed by proving the following two claims:
\begin{itemize}
\item[I.] 
For $i=1,\ldots,n$ denote 
\[
U_i = \frac{B_i+1}{2}  \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  + 
 \frac{1-B_i}{2}  \frac{-\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)}. 
\]
Then 
\[
\frac{1}{\sqrt{n}} \sum_{i=1}^n U_i \overset{d}{\rightarrow} \Ncal\left(0, \kappa(\theta) \right). 
\]
\item[II.]
For $i=1,\ldots,n$ denote 
\[
V_i =  \frac{B_i-1}{2} 
\left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \right)^2
+ 
 \frac{1-B_i}{2} \left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)}  \right)^2.
\]
Then 
\[
\frac{1}{n} \sum_{i=1}^n V_i \overset{a.s.}{\rightarrow} \kappa(\theta). 
\]
\end{itemize}
\subsubsection*{Proof of Claim I}
First note that 
\begin{align*}
\ex{U_i} & = 
 \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}   \Prob (B_i=1)   + 
 \frac{-\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)}   \Prob (B_i=-1)  = 0. 
\end{align*}
In addition,
\begin{align*}
& \ex{ U_i^2} = 
\left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \right)^2  \Prob (B_i=1)   + 
\left( \frac{-\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)}  \right)^2 \Prob (B_i=-1) \\
& =
 \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{\Prob_{\theta}(X_i \in A_i)} +  
  \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{1-\Prob_{\theta}(X_i \in A_i)} \\
 & =  
  \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{\Prob_{\theta}(X_i \in A_i) \left(1-\Prob_{\theta}(X_i \in A_i) \right)} 
\end{align*}
Therefore
\[
\frac{1}{n} \sum_{i=1}^n \ex{ U_i^2} = L_n(A_1,\ldots,A_n) \overset{a.s.}{\longrightarrow} \kappa(\theta)
\]
for any $\theta \in \Theta$ such that the limit above exists. We now verify that the sequence $\{ U_i,\,i=1,2,\ldots \}$ satisfies Lyaponov's condition for his version of the central limit time: for any $\delta>0$ we have that 
\begin{align*}
& \ex{ \left| U_i \right|^{2+\delta} }=
 \frac{ \left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta}} {(\Prob_{\theta}(X_i \in A_i))^{1+\delta}} +  
  \frac{ \left|\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta}} {(1-\Prob_{\theta}(X_i \in A_i))^{1+\delta}} 
\end{align*}
and
\begin{equation}
\frac{\sum_{i=1}^n\ex{\left| U_i \right|^{2+\delta}} } { \left( \sum_{i=1}^n \ex{U_i^2}  \right)^\delta} = 
\frac{ \frac{1}{n^{1+\delta}} \sum_{i=1}^n \ex{ |U_i|^{2+\delta}} }{ \left(\frac{1}{n} \sum_{i=1}^n \ex{U_i^2 } \right)^\delta}. 
\label{eq:Lyaponov}
\end{equation}
Next, we claim that there exits $\delta>0$ and $K>0$, that are independent of $n$, such that
\begin{align}
\frac{1}{n} \sum_{i=1}^n \ex{ |U_i|^{2+\delta}}  < K \label{eq:Lyaponov_num}
\end{align} 
for all $n$ large enough. To see this, note that
\begin{align*}
& \ex{ |U_i|^{2+\delta} }= 
 \frac{ \left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta}} {(\Prob_{\theta}(X_i \in A_i))^{1+\delta}} +  
  \frac{ \left|\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta}} {(1-\Prob_{\theta}(X_i \in A_i))^{1+\delta}}  \\
  &  = \frac{\left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta} }
  { (\Prob_{\theta}(X_i \in A_i))^{1+\delta} (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta} } 
  \left( (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta} + (\Prob_{\theta}(X_i \in A_i))^{1+\delta}   \right) \\
  &  \leq \frac{\left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta} }
  { (\Prob_{\theta}(X_i \in A_i))^{1+\delta} (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta}}, 
\end{align*}
where the last transition is because 
\[
 \left( (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta} + (\Prob_{\theta}(X_i \in A_i))^{1+\delta}  \right) \leq 1. 
\]
We now use the fact that each $A_i$ is a finite union of intervals. Under the assumption that there exists $\delta>0$ such that $\eta^{1+\delta}(x)/f^\delta(x)$ is maximized by the origin, we get
\[
\eta^{1+\delta}(x)/f^\delta(x) = \frac{(f(x))^{2+\delta}}{\left(F(x)(1-F(x))\right)^{1+\delta}} \leq 4^{1+\delta} (f(0))^{2+\delta}. 
\]
Lemma~\ref{lem:bound_intervals_delta} implies
\begin{align*}
  &  \leq \frac{\left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta} }
  { (\Prob_{\theta}(X_i \in A_i))^{1+\delta} (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta}} = \frac{\left| \sum_{k=1}^{K_i} (-1)^k f(x_{i,k}-\theta) \right|^{2+\delta} }
  { \left( \sum_{k=1}^{K_i} (-1)^k F(x_{i,k}-\theta) \right)^{1+\delta} \left( 1 - \sum_{k=1}^{K_i} (-1)^k F(x_{i,k}-\theta) \right)^{1+\delta}} \\
  & \leq 4^{1+\delta}(f(0))^{2+\delta}.
\end{align*}
It follows that
\[
\frac{1}{n} \sum_{i=1}^n \ex{ |U_i|^{2+\delta}} \leq 4^{1+\delta}(f(0))^{2+\delta}, 
\]
and thus the numerator of \eqref{eq:Lyaponov}, as well as the entire expression, goes to zero. From Lyaponov's central limit theorem we conclude that 
\[
\frac{1}{\sqrt{n}} \sum_{i=1}^n U_i \overset{d}{\rightarrow} \Ncal\left(0,\kappa(\theta) \right). 
\]

\subsubsection*{Proof of Claim II} 
We have:
\begin{align*}
\ex{ V_i}  & = 
 \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{\Prob_{\theta}(X_i \in A_i)}  
+ 
 \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{1-\Prob_{\theta}(X_i \in A_i)}  \\
 & = 
  \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{\Prob_{\theta}(X_i \in A_i) \left( 1-\Prob_{\theta}(X_i \in A_i) \right)}  
\end{align*}
 We conclude that:
\begin{equation} \label{eq:LAN_limit_cond}
\frac{1}{n} \sum_{i=1}^n \ex{ V_i} =   L_n(A_1,\ldots,A_n)  \to \kappa(\theta)
\end{equation}
Since the $V_i$s are independent of each other, Kolmogorov's law of large numbers implies
%(e.g. \cite[Theorem 10.2.3]{sen1994large}) 
\[
\frac{1}{n} \sum_{i=1}^n  V_i \overset{a.s.}{\longrightarrow} \kappa(\theta)
\]
for any $\theta \in \Theta$ for which the limit \eqref{eq:LAN_limit_cond} exists.
\QEDA


\subsection{Proof of Theorem~\ref{thm:non_existence}
\label{proof:thm:non_existence}
}

Let $\Xi$ be the set of points $\theta \in \Theta$ for which $\kappa(\theta) = \eta(0)$. 
%
Since $B_1,B_2,\ldots$ satisfy the conditions in Theorem~\ref{thm:LAN1}, $\theta$ is in $\Xi$ if and only if
\begin{equation}
\label{eq:non_existence_proof}
\lim_{n\to \infty} L_n(A_1,\ldots,A_n;\theta) = \eta(0). 
\end{equation}
By assumption, we have $B_i^{-1} = A_i$ where $A_i$ can be expressed as
\[
A_i = \cup_{i=1}^K (a_{i,k},b_{i,k}), 
\]
where $a_{i,1} \leq b_{i,1} \leq \ldots \leq a_{i,K}, b_{i,K}$, and $a_{i,1}$ and $b_{i,K}$ may take the values $-\infty$ and $\infty$, respectively. Denote
\[
\mathcal B_i = \cup_{k=1}^{K}\{a_{i,k},b_{i,k}\}.
\]
For any $\theta$ and $\epsilon>0$, denote 
\[
S_n(\theta, \epsilon) \triangleq \left\{ i\leq n \,:\, (\theta-\epsilon,\theta+\epsilon) \cap \mathcal B_i \neq \emptyset \right\}
\]
In words, $S_n$ contains all integers smaller than $n$ in which an $\epsilon$-ball around $\theta$ contains an endpoint of one of the intervals consisting $A_i$. 
%
We now claim that 
if $\theta \in \Xi$ then $\card(S_n(\theta, \epsilon))/n \to 1$. Indeed, for such $\theta$ we have
\begin{align}
& L_n(A_1,\ldots,A_n; \theta) \nonumber \\
& = \frac{1}{n} \sum_{i \in S_n(\epsilon,\theta)}  
\frac{ \left(\sum_{k=1}^{K}  f(\theta - b_{i,k})- f(\theta - a_{i,k}) \right)^2}{ \sum_{k=1}^{K} \left( F(\theta - b_{i,k})- F(\theta - a_{i,k}) \right) \left(1-\sum_{k=1}^{K} \left( F(\theta - b_{i,k})- F(\theta - a_{i,k}) \right)\right)} \nonumber \\
& 
+ \frac{1}{n}\sum_{i \notin S_n(\epsilon,\theta) } \frac{ \left(\sum_{k=1}^{K}  f(b_{i,k}-\theta) - f(a_{i,k}-\theta) \right)^2} { \sum_{k=1}^{K} \left( F(\theta - b_{i,k})- F(\theta - a_{i,k}) \right) \left(1-\sum_{k=1}^{K} \left( F(\theta - b_{i,k})- F(\theta - a_{i,k}) \right)\right)} \nonumber \\
& \overset{(a)}{\leq}
\frac{\card\left(S_n(\theta,\epsilon)\right)}{n} \eta(0) + \frac{n-\card\left(S_n(\theta,\epsilon) \right) }{n} \eta(\epsilon) 
 \label{eq:non_existence_proof1}
\end{align}
where $(a)$ follows from Lemma~\ref{lem:bound_intervals_delta} with $\delta =0$, and the fact that for $i \in S_n(\theta, \epsilon)$, 
\[
\max\left\{ \max_k \eta(b_{i,k}-\theta) , \max_k \eta(a_{i,k}-\theta)  \right\} \leq \eta(\epsilon) < \eta(0). 
\]
Unless  $\card \left(S_n(\theta, \epsilon) \right)/n \to 1$, we get that \eqref{eq:non_existence_proof1}, hence $L_n(A_1,\ldots,A_n ; \theta)$, are bounded from above by a constant that is smaller then $\eta(0)$ in contradiction to the fact that $\theta \in \Xi$. \par
For $k\in \N$, assume by contradiction that there exists $N \geq 2K + 1$ distinct elements
$\theta_1,\ldots,\theta_N \in \Xi$. Since each $A_i$ consists of at most $K$ intervals, we have that 
\begin{equation}
\label{eq:few_optimality_points_proof}
\card (\cup_{i=1}^n \mathcal B_i) \leq 2 n K. 
\end{equation}
Fix $\epsilon>0$ such that 
\[
\epsilon < \frac{1}{2}\min_{i\neq j} |\theta_i - \theta_j|. 
\]
Since for each $\theta \in \Theta$ we have $S_n(\theta, \epsilon) \to 1$, there exists $n$ large enough such that 
\[
\card \left(S_n(\theta_i, \epsilon) \right) \geq n \left(1-\frac{1}{2N} \right)
\]
for all $i=1,\ldots,N$. However, $S_n(\theta_1,\epsilon), \ldots S_n(\theta_N,\epsilon)$ are disjoint, so the cardinallity of their union is at least $n\left(1-\frac{1}{2N} \right)N$ which is grater than $2nK + n/2$ in contradiction to \eqref{eq:few_optimality_points_proof}. 

