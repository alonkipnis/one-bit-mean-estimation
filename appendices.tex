% -*- Mode: latex -*- %

\section{}

\subsection{Fast convergence of uniform estimators under bit constraints}
\label{sec:uniform-weirdos}

Here we consider the uniform distribution as our location family,
demonstrating that in the adaptive setting~\eqref{item:adaptive} or even the
one-step adaptive setting~(\ref{item:one-step-adaptive}'), constrained
estimators can attain rates faster than the $1 / \sqrt{n}$ rates regular
estimands allow. Indeed, define $c(x) = -\log 2$ for $x \in [-1, 1]$ and
$c(x) = -\infty$ for $x \not \in [-1, 1]$. Then $f(x) = e^{-c(x)}$ is
log-concave and symmetric, and we may consider the location family with
densities $f(x - \theta)$. For notational simplicity, we assume we have
a sample of size $2n$. We provide a proof sketch that
there is a one-step adaptive estimator $\theta_n$ such that
\begin{equation}
  \label{eqn:uniforms-are-easy}
  \sup_{|\theta| \le \log n}
  P_\theta\left(|\theta_n - \theta| \ge \frac{8 \log n}{n^{3/4}}\right)
  \le \frac{2}{n^2}.
\end{equation}
for all large $n$,
and so (by the Borel-Cantelli lemmas), for any $\theta \in \R$ we have
$P_\theta(|\theta_n - \theta| \le \sqrt{2 \log n} / n^{3/4} ~
\mbox{eventually}) = 1$. This is of course faster than the $1/\sqrt{n}$
rates we prove throughout.

To prove inequality~\eqref{eqn:uniforms-are-easy}, we proceed in two steps,
both quite similar.
First, we define an initial estimator $\theta\init_n$.
Let $\epsilon > 0$, which we will determine presently, though we will
take $n \epsilon \to \infty$ as $n \to \infty$, so that we may assume
w.l.o.g.\ that $\theta \in [-n\epsilon/2, n \epsilon/2]$. Take the interval
$[-n\epsilon, n\epsilon]$, and construct
$m$ thresholds at intervals of size $2 n \epsilon / m$; let
the $j$th such threshold be
\begin{equation*}
  t_j \defeq -n \epsilon + \frac{2 n (j-1) \epsilon}{m}
\end{equation*}
Then we ``assign'' observations to each pair of thresholds, so that
threshold $j$ corresponds to observations $I_j \defeq \{\frac{n (j - 1)}{m}
+ 1, \ldots, \frac{n j}{m}\}$, of which there are $n/m$.  For each index $i
\in I_j$, we set
\begin{equation*}
  B_i = \begin{cases}
    1 & \mbox{if~} X_i \ge t_j \\
    0 & \mbox{otherwise}.
  \end{cases}
\end{equation*}
Then we simply set $\theta_n\init$ to be the maximal threshold for which
$B_i = 0$ for all observations $X_i$ corresponding to that threshold.

Let us now consider the probability that $\theta_n\init$ is substantially
wrong. For notational simplicity, let $U_i = (1 + X_i - \theta) / 2$, so that
the $U_i$ are uniform on $[0, 1]$.
First, note that we always have $\theta_n\init \ge \theta
- \frac{2 n \epsilon}{m}$, because no observations will be below
the appropriate threshold. Let $j\opt$ be the smallest index $j$
for which $\theta \le t_{j\opt}$, and consider
the index sets $I_{j\opt}$, $I_{j\opt + 1}$, and so on.
The event $\theta\init_n \ge t_{j\opt} + \frac{2 n \epsilon}{m}$ may
occur only if for each of the $n/m$ observations in the set
$I_{j\opt + 1}$, we have $U_i \ge \frac{n \epsilon}{m}$. Thus,
\begin{equation*}
  P_\theta\left(\theta_n\init \ge t_{j\opt} + \frac{2 n \epsilon}{m}
  \right)
  \le \left(1 - \frac{n \epsilon}{m}\right)^\frac{n}{m}
  \le \exp\left(-\frac{n^2\epsilon}{m^2}\right).
\end{equation*}
Setting the number of bins $m = \sqrt{n}$,
the resolution $\epsilon = 2 \log n / n$, we obtain
$P_\theta(\theta_n\init \ge t_{j\opt} + 4 \log n / \sqrt{n})
\le n^{-2}$. Thus we have
\begin{equation}
  \label{eqn:quality-of-initial-estimate}
  \sup_{|\theta| \le \log n} P_\theta\left(|\theta_n\init - \theta| \ge
  \frac{8 \log n}{\sqrt{n}}\right) \le \frac{1}{n^2}.
\end{equation}

The second stage estimator follows roughly the same strategy, except that
the resolution of the bins is tighter. In particular, let us assume that
$|\theta_n\init - \theta| \le \frac{8 \log n}{\sqrt{n}}$, which happens
eventually by inequality~\eqref{eqn:quality-of-initial-estimate}.  (We will
assume this tacitly for the remainder of the argument.)  Consider the
interval $\Theta_n \defeq \theta_n\init + [-\frac{16 \log n}{\sqrt{n}},
  \frac{16 \log n}{\sqrt{n}}]$ centered at $\theta_n\init$; we know that the
interval includes $[\theta - \frac{8 \log n}{\sqrt{n}}, \theta + \frac{8
    \log n}{\sqrt{n}}]$. Without loss of generality we assume $\theta_n\init
= 0$.  Following precisely the same discretization strategy as that for
$\theta_n\init$, we divide $\Theta_n$ into $m$ equal intervals, with
thresholds $t_j = -\frac{16 \log n}{\sqrt{n}} + \frac{32 (j - 1) \log n}{m
  \sqrt{n}}$; let $\epsilon_n = \frac{32 \log n}{m \sqrt{n}}$ be the width of
these intervals.  Then following exactly the same reasoning as above, we
assign indices $I_j = \{\frac{n(j - 1)}{m} + 1, \ldots, \frac{n j}{m}\}$ and
for $i \in I_j$, set $B_i = 1$ if $X_i \ge t_j$. We define $\theta_n$ to be
the maximal threshold $t_j$ for which $B_i = 0$ for all observations $X_i
\in I_j$. Then following precisely the reasoning above, we have (on the
event that $|\theta_n\init - \theta| \le \frac{8 \log n}{\sqrt{n}}$)
\begin{equation*}
  P_\theta(|\theta_n - \theta|
  \ge 2 \epsilon_n)
  \le (1 - \epsilon_n)^\frac{n}{m}
  \le \exp\left(-\frac{n \epsilon_n}{m}\right)
  = \exp\left(-\frac{32 \sqrt{n} \log n}{m^2}\right).
\end{equation*}
Set $m = 4 n^{1/4}$ to obtain the claimed
result~\eqref{eqn:uniforms-are-easy}.



\subsection{Proof of Proposition~\ref{prop:CEO}
\label{app:proof:CEO}}

%
Denote by $D^\star$ the optimal MSE in the Gaussian CEO with $L$ observers and under a total sum-rate $r = r_1 + \ldots +r_L$. An expression for $D^\star$ as a function of $r$ is give as \cite[Eq. 10]{chen2004upper}:
\begin{equation} \label{eq:ceo_optimal_sumrate}
r = \frac{1}{2} \log^+ \left[ \frac{\sigma_\theta^2}{D^\star} \left( \frac{D^\star L}{ D^\star L - \sigma^2 + D^\star \sigma^2 / \sigma_\theta^2 }\right)^L  \right].
\end{equation}
For the special case where $r = n$ and $L=n$, we have
\begin{equation} \label{eq:ceo_optimal_sumrate2}
n = \frac{1}{2} \log_2 \left[ \frac{\sigma_\theta^2}{D^\star} \left(\frac{ D^\star n }{D^\star n - \sigma^2 + D^\star \sigma^2/\sigma_\theta^2 }  \right)^n  \right].
\end{equation}
Consider the distributed encoding setting (iii) in the case where $f(x) = \Ncal(0,\sigma^2)$ and the prior on $\Theta$ is $\pi = \Ncal(0,\sigma_\theta^2)$. The Gaussian CEO problem of \cite{viswanathan1997quadratic} with a unit bitrate $r_1=\ldots = r_n =1$ at each terminal and blocklength $k=1$ reduces to our distributed setting (iii). Since $D^\star$ satisfying \eqref{eq:ceo_optimal_sumrate2} describes the MSE in the CEO setting under an optimal allocation of the sum-rate $r = n$ among $n$ encoders, it provides a lower bound to the minimal MSE in estimating $\theta$ in the distributed setting. By considering the limit $n\rightarrow \infty$ in \eqref{eq:ceo_optimal_sumrate2}, we see that 
\[
D^\star = \frac{ 4\sigma^2 }{3n + 4 \sigma^2 / \sigma_\theta^2 } + o(n^{-1}) =  \frac{4\sigma^2}{3n} + o(n^{-1}). 
\]
This implies Proposition~\ref{prop:CEO}. 
%We note that although the lower bound \eqref{eq:ceo_bound} was derived assuming the optimal allocation of $n$ bits per observation among the encoders, this bound cannot be tightened by considering the MSE in the CEO setting while enforcing the condition $r_1=\ldots = r_n = 1$. Indeed, an upper bound for the CEO MSE under the condition $r_1=\ldots = r_n = 1$ follows from \cite{KipnisRini2019}, and leads to
%\[
%D^\star \leq  \left( \frac{1}{\sigma_\theta^2} +  \frac{3n}{4\sigma^2 + \sigma_\theta^2} \right)^{-1}   =
%\frac{4 \sigma^2}{3n} +  \frac{\sigma_\theta^2}{3n} + O(n^{-2}),
%\]
%which is equivalent to \eqref{eq:ceo_bound} when $\sigma_\theta$ is small. 

\subsection{Proof of Theorem~\ref{thm:adpative_lower_bound}
\label{proof:thm:adpative_lower_bound}
}
Consider the following two Lemmas:
\begin{lem} \label{lem:bound_intervals}
Let $f(x)$ be log-concave, symmetric, and differentiable density function such that Assumption~1 holds. For any $x_1 \geq \ldots \geq x_n \in \R$,
\begin{equation}
\frac{ \left| \sum_{k=1}^n (-1)^{k+1} f(x_k) \right|^2 }
{\left( \sum_{k=1}^n (-1)^{k+1} F(x_k) \right) \left(1- \sum_{k=1}^n (-1)^{k+1} F(x_k) \right) } 
\leq  4f^2(0). 
\label{eq:lem_bound_intervals}
\end{equation}
\end{lem}
\begin{lem} \label{lem:fisher_bound}
Let $X$ be a random variable with a symmetric, log-concave, and continuously differentiable density function $f(x)$ such that Assumption~1 holds. For a Borel measurable $A$ set, 
\[
B(X) = \begin{cases} 1,& X \in A, \\
-1, & X \notin A.
\end{cases}
\]
Then the Fisher information of $B$ with respect to $\theta$ is bounded from above by $\eta(0)$.
\end{lem}

Lemma~\ref{lem:bound_intervals} is obtained as the special case $\delta = 0$ of lemma \ref{lem:bound_intervals_delta}. 
We now prove Lemma~\ref{lem:fisher_bound}.
\subsubsection*{Proof of Lemma~\ref{lem:fisher_bound}}
When $f(x)$ is the normal density function, this lemma follows from \cite[Thm. 3]{Barnes2018}. The proof below is based on a different techique than in \cite{Barnes2018}, and is valid for any log-concave symmetric density satisfying Assumption~\ref{assump:failure_rate}. \\

The Fisher information of $B$ with respect to $\theta$ is given by
\begin{align}
I_\theta & =  \ex{ \left( \frac{d}{d\theta} \log P\left( B | \theta \right) \right)^2 |\theta } \nonumber \\
& = \frac{ \left(\frac{d}{d\theta} P(B=1|\theta) \right)^2}{P(B=1| \theta)} + \frac{ \left(\frac{d}{d\theta} P(B=-1|\theta) \right)^2} {P(B=-1| \theta)} \nonumber \\
& =  \frac{ \left( \frac{d}{d\theta} \int_A f \left( x-\theta\right)dx \right)^2} { P(B=1| \theta) } + \frac{ \left( \frac{d}{d\theta}\int_A f \left( x-\theta \right)dx \right)^2} { P(B=-1| \theta) } \nonumber \\ 
& \overset{(a)}{=} \frac{ \left( - \int_A f' \left( x-\theta \right)dx \right)^2} { P(B=1| \theta) } + \frac{ \left(- \int_A f' \left( x-\theta \right)dx \right)^2} { P(B=-1| \theta) } \nonumber \\ 
& = \frac{\left( \int_A f'\left( x-\theta \right) dx \right)^2 }{  P(B=1 | \theta) \left(1-P(B=1|\theta) \right)  }, \nonumber \\
& = \frac{\left( \int_A f'\left( x-\theta \right) dx \right) \left( \int_A f'\left( x-\theta \right) dx \right)}{ \left( \int_A f \left( x-\theta \right) dx \right)  \left(1- \int_A f \left( x-\theta \right) dx \right) }, \label{eq:lem_fisher_bound_proof1}
\end{align}
where differentiation under the integral sign in $(a)$ is possible since $f(x)$ is differentiable with continuous derivative $f'(x)$. Regularity of the Lebesgue measure implies that for any $\epsilon>0$, there exists a finite number $k$ of disjoint open intervals $I_1,\ldots I_k$ such that 
\[
\int_{A\setminus \cup_{j=1}^k I_j }  dx < \epsilon,
\]
which implies that for any $\epsilon' > 0$, the set $A$ in \eqref{eq:lem_fisher_bound_proof1} can be replaced by a finite union of disjoint intervals without increasing $I_\theta$ by more than $\epsilon'$. It is therefore enough to proceed in the proof assuming that $A$ is of the form
\[
A = \cup_{j=1}^k (a_j,b_j),
\]
with $\infty \leq a_1 \leq \ldots a_k$, $b_1 \leq b_k \leq \infty$ and $a_j \leq b_j$ for $j=1,\ldots,k$. Under this assumption we have
\begin{align*}
\mathbb P(B_n=1| \theta) & = \sum_{j=1}^k \mathbb P\left(X_n \in (a_j,b_j) \right)  \\
& = \sum_{j=1}^k \left( F \left(b_j-\theta\right) -  F \left(a_j-\theta\right)  \right),
\end{align*}
so \eqref{eq:lem_fisher_bound_proof1} can be rewritten as
\begin{align}
& =   \frac { \left( \sum_{j=1}^{k} f \left(a_j-\theta \right) - f \left( b_j-\theta \right)  \right)^2 } 
{ \left( \sum_{j=1}^k F \left( b_j-\theta \right) - F \left( a_j-\theta \right)  \right) }  \nonumber \\
& \times \frac {1} 
{1- \left( \sum_{j=1}^k F \left(  b_j-\theta \right) - F \left( a_j-\theta \right)  \right) } 
\label{eq:lemma_J}
\end{align}
It follows from Lemma~\ref{lem:bound_intervals} that for any $\theta \in \R$ and any choice of the intervals endpoints, \eqref{eq:lemma_J} is smaller than 
\[
\max_{t \in \{a_j,b_j, j=1,\ldots,k\} } 4f^2(t) \leq 4 f^2(0), 
\]
where the last transition is due to Assumption~1. 
\QEDA \\


We now finish the proof of Theorem~\ref{thm:adpative_lower_bound}. In order to bound from above the Fisher information of any set of $n$ one-bit messages with respect to $\theta$, we first note that without loss of generality, each message $B_i$ can is of the form
\begin{equation}
\label{eq:general_messages}
B_i = \begin{cases}
X_i \in A_i & 1, \\
X_i \notin A_i & -1,
\end{cases} 
\end{equation}
where $A_i \subset \R$ is a Borel measurable set. 
%Indeed, any measurable function $(X_i) \in \{-1,1\}$ can be written in the form \eqref{eq:general_messages} with $A_i = B^{-1}(1)$.
Consider the conditional distribution $P({B_1,\ldots,B_n|\theta})$ of $(B_1,\ldots,B_n)$ given $\theta$. We have 
\begin{align}
P\left( B_1,\ldots,B_n | \theta \right) & =  \prod_{i=1}^n P\left(B_i | \theta, B_1,\ldots,B_{i-1} \right), \label{eq:adpt_lower_bound_proof:1}
\end{align}
where $P\left(B_i =1 | \theta, B_1,\ldots,B_{i-1}  \right) = \mathbb P\left( X_i \in A_i\right)$, so that the Fisher information of $B_1,\ldots,B_n$ with respect to $\theta$ is given by 
\begin{align}
I_\theta(B_1,\ldots,B_N) = \sum_{i=1}^n I_\theta (B_i|B_1,\ldots,B_{i-1}),
\label{eq:fisher_information}
\end{align}
where $I_\theta (B_i|B_{i-1},\ldots,B_1)$ is the Fisher information of the distribution of $B_i$ given $B_1,\ldots,B_{i-1}$. From Lemma~\ref{lem:fisher_bound} it follows that $I_\theta (B_i|B_{i-1},\ldots,B_1) \leq 4f^2(0)$. The Van Trees inequality \cite{van2004detection, gill1995applications} now implies 
\begin{align*}
\ex{ \left( \theta_n - \theta \right)^2} &  \geq \frac{1}{ \ex{ I_\theta(B_1,\ldots,B_n)} + I_0} \\
& = \frac{1}{ \sum_{i=1}^n I_\theta (B_i | B^{i-1} ) + I_0} \\
& \geq \frac{1}{ 4f^2(0) n + I_0}.
\end{align*}
\QEDA


\subsection{Isoperimetric Lemma
\label{sec:bound_intervals_delta} }
The following lemma is used in the proof Theorems~\ref{thm:adpative_lower_bound}, \ref{thm:LAN1}, and \ref{thm:non_existence}. 

\begin{lem} \label{lem:bound_intervals_delta}
Let $f(x)$ be log-concave, symmetric, and differentiable density function. Let $\delta\geq 0$. Assume that the function
\[
\eta_\delta(x) \triangleq  \eta^{1+\delta}(x)/f^\delta(x) = \frac{  \left( f(x) \right)^{2+\delta}}{\left(F(x)(1-F(x))\right)^{1+\delta}}
\]
is non-increasing in $|x|$. For any $x_1 \ge \ldots \ge x_n \in \R^n$,
\begin{equation}
\frac{ \left| \sum_{i=1}^n (-1)^{i+1} f(x_i) \right|^{2+\delta} }
{\left| \sum_{i=1}^n (-1)^{i+1} F(x_i) \right|^{1+\delta} \left|1- \sum_{k=1}^n (-1)^{i+1} F(x_i) \right|^{1+\delta} } 
\leq  \max_{i} \eta_\delta(x_i).
\label{eq:lem_bound_intervals_delta}
\end{equation}
\end{lem}
In particular, 
\[
\frac{ \left| \sum_{i=1}^n (-1)^{i+1} f(x_i) \right|^{2+\delta} }
{\left| \sum_{i=1}^n (-1)^{i+1} F(x_i) \right|^{1+\delta} \left|1- \sum_{i=1}^n (-1)^{i+1} F(x_i) \right|^{1+\delta} } 
\leq \eta_\delta(0) = 4^{1+\delta} f^{2+\delta}(0).
\]

\subsubsection*{
Proof of Lemma~\ref{lem:bound_intervals_delta}}
Denote 
\[
\delta_n(x_1,\ldots,x_n) \triangleq \sum_{i=1}^{n} s_i f(x_i),
\]
\[
 \Delta_n(x_1,\ldots,x_n) \triangleq  \sum_{i=1}^n s_i F(x_i), 
\]
where $s_i \triangleq (-1)^{i+1}$. We use induction on $n \in \N$ to show that 
\begin{align}
\label{eq:lemm:interval_bounds:to_show}
\frac{ \left| \delta_n(x_1,\ldots,x_n) \right|^{2+\delta}} 
{\left|\Delta_n(x_1,\ldots,x_n)\left(1- \Delta_n(x_1,\ldots,x_n) \right) \right|^{1+\delta} } \leq \max_{i}\eta_{\delta}(x_i).
\end{align}
 Since 
\[
\eta_\delta(x) =  \frac{  \left|\delta_1(x) \right|^{2+\delta}}{\left|\Delta_1(x)
(1-\Delta_1(x)) \right|^{1+\delta}}, 
\]
The case $n=1$ is trivial.  
%follows from the assumption that $\eta_\delta(x)$ is non-increasing in $|x|$. 
Assume that \eqref{eq:lemm:interval_bounds:to_show} holds for all integers up to $n = N$ and for any $x_1 \ge \ldots \ge x_N$. Consider the case $n = N+1$. Let $i^*$ be the index such that $x_{i^*}$ has minimal absolute value among $x_1,\ldots,x_N$. The assumption on $\eta_\delta(x)$ implies that
\[
\eta_\delta(x_{i^*}) = \max_i \eta_\delta(x_i).
\]
Since the LHS of \eqref{eq:lem_bound_intervals_delta} is invariant to a sign flip of all $x_1,\ldots,x_{N+1}$, we may assume that $x_{i^*}$ is positive without loss of generality. 
%For simplicity, we also assume that $s_{i^*} = 1$ and note that the case $s_{i^*} = -1$ is obtained using identical arguments. 
Set $x^* = x_{i^*}$ and let $k=i^*-1$. In what follows, variables with subscript of non-positive index are ignored in summations and in lists of arguments to functions. Consider the function
\begin{align}
& g(y_1,\ldots,y_N) \triangleq g(y_1,\ldots,y_N|x^*,k) \\
&  \triangleq  \frac{\left| \delta_{N+1}(y_1,\ldots,y_k,x^*,y_{k+1}\ldots,y_N) )\right|^{2+\delta}}{
\left|\Delta_{N+1}(y_1,\ldots,y_k,x_{i^*},y_{k+1}\ldots,y_N)  (1 -\Delta_{N+1}(y_1,\ldots,y_k,x^*,y_{k+1}\ldots,y_N)  \right|^{1+\delta}
} \label{eq:g_def}.
\end{align}
The LHS of \eqref{eq:lemm:interval_bounds:to_show} is obtained by taking $y_i=x_{k_i}$ where $k_i$ is the $i$th element in $\{1,\ldots,N+1\}\setminus \{i^*\}$. It is therefore enough to prove that 
\[
\max_{(y_1,\ldots,y_N) \in A_N} g(y_1,\ldots,y_N) \leq \eta_{\delta}(x^*),
\]
where 
\[
A_N(x^*,k) \triangleq \left\{ (y_1,\ldots,y_N) \in \R^N\, : \, y_1 \ge y_k \ge x^* \ge -x^* \ge y_{k+1} \ldots \ge y_N
\right\}.
\]
%
Since $f(x)$ is log-concave, symmetric, and differentiable, we may write $f(x) = e^{c(x)}$ where $c(x)$ is concave, symmetric, and differentiable with derivative 
\[
c'(x) \triangleq \frac{f'(x)}{f(x)} 
\]
that is non-increasing. We first prove the lemma under the assumption that $c'(x)$ is an injection, or, equivalently, that $c(x)$ is strictly decreasing for all $x\in \R$. \par
%
The maximal value of $g(y_1,\ldots,y_N)$ is attained for the same $(y_1,\ldots,y_N) \in A_N(x^*,k)$ that maximizes
\begin{align*}
 \log(g)(y_1,\ldots, y_N) =  (2+\delta) \log \left( \delta_N  \right)  - (1+\delta) \log \left( \Delta_N  \right) - (1+\delta) \log \left(1 - \Delta_N \right),
\end{align*}
where in the last display and henceforth we suppress the arguments $y_1,\ldots,y_k,x^*,y_{k+1},\ldots, y_N$ of the functions $\delta_N$ and $\Delta_N$.
%
Within the interior of $A_N(x^*,k)$, all three expressions in \eqref{eq:g_def} within an absolute value are positive. It follows that partial derivative of $\log(g)(y_1,\ldots,y_N)$ with respect to $y_i$ within the interior of $A_N(x^*,k)$ is given by
\[
\frac{\partial \log(g)}{\partial y_i} = \frac{(2+\delta) s_i f'(x_i)}{\delta_N } -\frac{(1+\delta) s_i f(x_i)}{\Delta_N  } + \frac{(1+\delta)s_i f(y_i)}{1-\Delta_N }.
\]
We conclude that the gradient of $\log(g)$ vanishes if and only if
\begin{equation}
\label{eq:gradient_zero}
c'(y_i) = \frac{f'(y_i)}{f(y_i)} = \frac{1+\delta}{2+\delta} \frac{\delta_N}{2} \left( \frac{1}{\Delta_N} - \frac{1}{1-\Delta_N } \right),\quad i=1,\ldots,N.
\end{equation}
%
Since we assumed that $c'(x)$ is an injection, \eqref{eq:gradient_zero} is satisfied if and only if $y_1 = \ldots = y_N$. In this case, 
$g(y_1,\ldots,y_N) = \eta_\delta(x_{i^*})$  
if $N$ is even. If $N$ is odd and $y_1 = \ldots = y_N > x^*$, then 
\begin{align*}
& g(y_1,\ldots,y_N) = \frac{\left| f(y_1)-f(x_{i^*})  \right|^{2+\delta}} { 
\left|  F(y_1) - F(x_{i^*}) \right|^{1+ \delta} 
\left| 1 - (F(y_1) -F(x_{i^*})) \right|^{1+ \delta} } 
\end{align*} 
which is bounded from from above by $\eta_\delta(x_{i^*})$ by the induction hypothesis. The case where $N$ is odd and $-x^* \leq y_1 = \ldots = y_N$ is similar. 
%
We now consider the possibility that the maximum of $g(y_1,\ldots,y_N)$ is attained at the boundaries of $A_N(x^*,k)$. At boundary points for which $y_i = y_{i+1}$ for some $i$, the contribution of $y_i$ and $y_{i+1}$ to $g(y_1,\ldots,y_N)$ is zero and the induction assumption for $n=N-1$ implies that 
\[
g(y_1,\ldots,y_N) \leq \eta_{\delta}(x^*)
\]
The remaining boundary points of $A_N(x^*,k)$ are covered by the following cases:
\begin{itemize}
\item[(1)]  $y_N \to -\infty$. 
\item[(2)] $y_1 \to \infty$.
\item[(3)] $y_k = x_{i^*}$.
\item[(4)] $y_{k+1} = -x_{i^*}$. 
\end{itemize}
For case (1), 
\begin{align*}
g(y_1,\ldots,y_N) \to \frac{ \left| \sum_{i=1}^{k} s_i f(y_i) + s_{i^*} f(x_{i^*}) - \sum_{i=k+1}^{N-1} s_i f(y_i) \right|^{2+\delta}} 
{\left| \sum_{i=1}^{k} s_i F(y_i) + s_{i^*} F(x_{i^*}) - \sum_{i=k+1}^{N-1} s_i F(y_i) \right|^{1+\delta}\left|1- \left( \sum_{i=1}^{k} s_i F(y_i) + s_{i^*} F(x_{i^*}) - \sum_{i=k+1}^{N-1} s_i F(y_i) \right)  \right|^{1+\delta} },
\end{align*}
which is smaller than $\eta_\delta(x_{i^*})$ by the induction hypothesis. Similarly, under case (2),
\begin{align*}
& g(y_1,\ldots,y_N) \to 
\frac{ \left| \sum_{i=2}^{k} s_i f(y_i) + s_{i^*} f(x_{i^*}) - \sum_{i=k+1}^{N} s_i f(y_i) \right|^{2+\delta}} 
{\left| 1+ \sum_{i=2}^{k} s_i F(y_i) + s_{i^*} F(x_{i^*}) - \sum_{i=k+1}^{N} s_i F(y_i) \right|^{1+\delta}\left|-\left( \sum_{i=2}^{k} s_i F(y_i) + s_{i^*} F(x_{i^*}) - \sum_{i=k+1}^{N} s_i F(y_i) \right)  \right|^{1+\delta} },
 \\
& = \frac{ \left| -\sum_{i=2}^{k} s_i f(y_i) - s_{i^*} f(x_{i^*}) + \sum_{i=k+1}^{N} s_i f(y_i) \right|^{2+\delta}} 
{ \left|1 - \left(-\sum_{i=2}^{k} s_i F(y_i) - s_{i^*} F(x_{i^*}) + \sum_{i=k+1}^{N} s_i F(y_i) \right) \right|^{1+\delta} 
\left|-\sum_{i=2}^{k} s_i F(y_i) - s_{i^*} F(x_{i^*}) + \sum_{i=k+1}^{N} s_i F(y_i) \right|^{1+\delta} }
\end{align*}
which is smaller than $\eta_{\delta}(x_{i^*})$ by the induction hypothesis. Under case (3), the terms in $\delta_N$ and $\Delta_N$ corresponding to $y_k$ and $x_{i^*}$ cancel each other. As a result,  $g(y_1,\ldots,y_N)$ reduces to an expression with $n=N-1$ variables hence this case is handled by the induction hypothesis. 
%
Finally, under case (4), set 
\[
d \triangleq s_k F(-x^*) + s_{i^*} F(x^*) = s_{i^*}\left(1-2F(-x^*) \right), 
\]
\[
\sigma \triangleq \sum_{i=1}^{k-1} s_i f(y_i) - \sum_{i=k+1}^{N} s_i f(y_i). 
\]
and 
\[
\Sigma \triangleq \sum_{i=1}^{k-1} s_i F(y_i) - \sum_{i=k+1}^{N} s_i F(y_i). 
\]
We have
\begin{align*}
& g(y_1,\ldots,y_N) =\nonumber  \\ 
& =
\frac{ \left| \sum_{i=1}^{k-1} s_i f(y_i) - \sum_{i=k+1}^{N} s_i f(y_i) \right|^{2+\delta}} 
{\left| \sum_{i=1}^{k-1} s_i F(y_i) + d(x^*) - \sum_{i=k+1}^{N} s_i F(y_i) \right|^{1+\delta} \left|1- \sum_{i=1}^{k-1} s_i F(y_i) - d(x^*) + \sum_{i=k+1}^{N} s_i F(y_i)   \right|^{1+\delta} }, \\
& = \frac{ \left| \sigma \right|^{2+\delta}} 
{\left| \Sigma+d \right|^{1+\delta} \left|1- \Sigma - d \right|^{1+\delta} }  =  
\frac{ \left| \sigma \right|^{2+\delta}} 
{\left| \Sigma \right|^{1+\delta} \left|1- \Sigma \right|^{1+\delta} }  \left| \frac{\Sigma(1-\Sigma) } { \Sigma(1-\Sigma) + d(1-2\Sigma)-d^2} \right|^{1+\delta}. 
\end{align*}
By the induction hypothesis,
\[
\frac{ \left| \sigma \right|^{2+\delta}} 
{\left| \Sigma \right|^{1+\delta} \left|1- \Sigma \right|^{1+\delta} } \leq \eta_\delta(x^*), 
\]
hence it is left to show that 
\[
 \frac{\Sigma(1-\Sigma) } { \Sigma(1-\Sigma) + d(1-2\Sigma)-d^2} \leq 1.
\]
Whenever $d>0$, 
\begin{align*}
& \frac{ \Sigma(1-\Sigma) + d(1-2\Sigma)-d^2} {\Sigma(1-\Sigma)} \geq 1 \Leftrightarrow  1-2\Sigma \geq d, 
\end{align*}
while for $d<0$,
\begin{align*}
& \frac{ \Sigma(1-\Sigma) + d(1-2\Sigma)-d^2} {\Sigma(1-\Sigma)} \geq 1 \Leftrightarrow  1-2\Sigma \leq d. 
\end{align*}
Therefore, it is enough to show that $\Sigma \leq F(-x^*)$ if $s_{i^*}=1$ and 
$\Sigma \geq F(-x^*)$ if $s_{i^*}=-1$. 
%
Indeed, if $s_{i^*}=1$, then $s_{k+1}=-1$ and monotonicity of $F(x)$ implies that 
\[
\Sigma + d \leq F(y_1) - F(y_k) + F(x^*) - F(-x^*) + F(y_{k+2}) - F(y_N), 
\]
and hence
\[
\Sigma \leq 1-F(x^*) = F(-x^*). 
\]
Similarly, if $s_{i^*}=-1$ then 
\[
1 - \Sigma \leq 1 -  F(-x^*).
\]
This conclude the proof in the case where $c'(x)$ is an injection. 
\par  
In the case where $c'(x)$ is not necessarily strictly decreasing, we approximate $c(x)$ using another concave symmetric function whose derivative is always negative except, perhaps, at the origin. For $\alpha>0$ consider the function  $f_\alpha(x) = \kappa(\alpha) e^{\sgn(c(x))|c(x)|^{1+\alpha}}$, where $\kappa(\alpha)$ is chosen such that $f_\alpha(x)$ is a probability density function. Then $c_\alpha(x)$ is concave, symmetric, and differentiable with
\[
c_\alpha'(x) \triangleq \frac{f'_\alpha(x)}{f_\alpha(x)} = (1+\alpha)|c(x)|^{\alpha} c'(x). 
\]
Now $c_\alpha'(x)$ is non-increasing since it is the derivative of a concave function. Furthermore, since $c(x)$ is non-constant on any interval and $c'(x)$ is non-increasing, $c_\alpha'(x)$ is non-constant on any interval hence an injection. It follows from the first part of the proof that, for any $\alpha>0$,
\begin{align}
\label{eq:proof:lem:bound_intervals}
\frac{(\delta_{n,\alpha})^2}{\Delta_{n,\alpha}(1-\Delta_{n,\alpha})} \leq \max_i \eta_{\alpha}(x_i),
\end{align}
where 
\[
\delta_{n,\alpha} \triangleq  \sum_{k=1}^{n} (-1)^{k+1} f_{\alpha}(x_k),
\]
\[
\Delta_{n,\alpha} \triangleq \sum_{k=1}^n (-1)^{k+1} F_{\alpha}(x_k), 
\]
and 
\[
\eta_{\delta,\alpha}(x) \triangleq \frac{(f_\alpha(x))^{2+\delta}}{\left(F_{\alpha}(x)(1-F(x)) \right)^{1+\delta}}. 
\]
The proof is completed by noting that 
\begin{align*}
\lim_{\alpha \to 0} \frac{(\delta_{n,\alpha})^{2+\delta} }{ \left(\Delta_{n,\alpha}(1-\Delta_{n,\alpha})\right)^{1+\delta}}  = \frac{(\delta_{n})^{2+\delta }}{\left(\Delta_{n}(1-\Delta_{n}) \right)^{1+\delta}},  
\end{align*}
and, since the maximum is over a finite set,
\begin{align*}
\lim_{\alpha \to 0}  \max_i \eta_{\delta,\alpha}(x_i)  = \max_i\eta_\delta(x_i).
\end{align*}

%Our goal is to construct an approximation to $f(x)$ using a log-concave, symmetric, and differentiable function $\hat{f}(x)$ such that $\hat{f}'(x)/\hat{f}(x)$ is strictly decreasing. We achieve such an approximation by modifying $f(x)$ over intervals over which $c'(x)$ is a constant. Assume first $c'(x)$ is only a constant over an interval $(a_1,b_1)\subset \R$ containing $x_1>0$ and on the mirror image of this interval around $x=0$. Over this interval, $f(x)$ is necessarily of the form $f(x) = e^{-c_1|x| + d_1}$ for some $c_1\geq 0$ and $d_1$. % and For $\alpha>0$, consider
%\[
%F_1(x,\alpha) \triangleq  F(a_1) + \frac{1}{c_1} e^{-c_1 (|x|^{1+\alpha} +d_1}, \quad x \in (a_1,b_1),
%\]
%and 
%\[
%f_1(x,\alpha) \triangleq  F_1'(x,\alpha) =  -\sgn(x)(1+\alpha)|x|^\alpha e^{-c_1 |x|^{1+\alpha} + d_1}, \quad x \in (a_1,b_1).
%\]



\QEDA 


\subsection{Proof of Theorem~\ref{thm:LAN1}
\label{proof:thm:LAN1}
}
The log-probability mass distribution of $(B_1,\ldots,B_n)$ is given by
\begin{align*}
& \log \Prob_\theta(b_1,\ldots,b_n) = \sum_{i=1}^n \left( \frac{b_i+1}{2} \log  \Prob(X_i\in A_i) + 
\frac{1-b_i}{2} \log \Prob(X_i\in A_i) \right), \quad b_i \in \{-1,1\}, \quad i=1,\ldots,n. 
\end{align*}
Consequently, 
\begin{align}
& \log \frac{ \Prob_{\theta + \frac{h}{\sqrt{n}}}(b_1,\ldots,b_n)} { \Prob_\theta (b_1,\ldots,b_n) } = \sum_{i=1}^n    \frac{b_i+1}{2} 
\log \frac{\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)} + 
\sum_{i=1}^n  \frac{1-b_i}{2} 
\log \frac{\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}. 
\label{eq:LAN_proof1}
\end{align}
For each $i=1,\ldots,n$, write 
\[
A_i = \bigcup_{k=1}^{K_i} \left(t_{i,2k-1},t_{i,2k} \right),
\]
where $t_{i,1}<\ldots<t_{i,K_i}$ and, with a slight abuse of notation, $t_{i,1}$ and $t_{i,K_i}$ may take the values $-\infty$ or $+\infty$, respectively. Thus
\[
\Prob_{\theta}(X_i \in A_i) = \sum_{k=1}^{K_i} (-1)^k F(t_{i,k}-\theta).
\]
In particular, since $f$ is differentible in $\theta$, $\Prob_{\theta}(X_i \in A_i)$ is twice differentiable, and we may write
\begin{align*}
\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \in A_i)  = \Prob_{\theta} (X_i \in A_i) + \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \frac{h}{\sqrt{n}} + o(h), 
\end{align*}
and thus
\begin{align*}
& \log \frac{\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)} = \log \left(1 + \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \frac{h}{\sqrt{n}} + o(h) \right) \\
& = \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \frac{h}{\sqrt{n}} - \frac{h}{2n} \left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \right)^2 + o(h^2). 
\end{align*}
Similarly, we have
\begin{align*}
& \log \frac{\Prob_{\theta+\frac{h}{\sqrt{n}}}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)} \\
& = \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}  \frac{h}{\sqrt{n}} - \frac{h}{2n} \left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}  \right)^2 + o(h^2). 
\end{align*}
From \eqref{eq:LAN_proof1} we obtain
\begin{align*}
& \log \frac{ \Prob_{\theta + \frac{h}{\sqrt{n}}}(b_1,\ldots,b_n)} { \Prob_\theta (b_1,\ldots,b_n) }   = \frac{h}{\sqrt{n}} \sum_{i=1}^n  
\left( 
\frac{b_i+1}{2}  \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  + 
 \frac{1-b_i}{2}  \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}   \right) \\
 & - \frac{h^2}{2n} 
 \sum_{i=1}^n 
\left( 
 \frac{b_i+1}{2} 
\left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \right)^2
+ 
 \frac{1-b_i}{2} \left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)}  \right)^2 \right) + o(h^2) \\
\end{align*}
%
Noting that 
\[
\frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \notin A_i)}{\Prob_{\theta}(X_i \notin A_i)} = \frac{-\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)},
\]
the proof is completed by proving the following two claims:
\begin{itemize}
\item[I.] 
For $i=1,\ldots,n$ denote 
\[
U_i = \frac{B_i+1}{2}  \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  + 
 \frac{1-B_i}{2}  \frac{-\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)}. 
\]
Then 
\[
\frac{1}{\sqrt{n}} \sum_{i=1}^n U_i \overset{d}{\rightarrow} \Ncal\left(0, \kappa(\theta) \right). 
\]
\item[II.]
For $i=1,\ldots,n$ denote 
\[
V_i =  \frac{B_i-1}{2} 
\left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \right)^2
+ 
 \frac{1-B_i}{2} \left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)}  \right)^2.
\]
Then 
\[
\frac{1}{n} \sum_{i=1}^n V_i \overset{a.s.}{\rightarrow} \kappa(\theta). 
\]
\end{itemize}
\subsubsection*{Proof of Claim I}
First note that 
\begin{align*}
\ex{U_i} & = 
 \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}   \Prob (B_i=1)   + 
 \frac{-\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)}   \Prob (B_i=-1)  = 0. 
\end{align*}
In addition,
\begin{align*}
& \ex{ U_i^2} = 
\left( \frac{\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{\Prob_{\theta}(X_i \in A_i)}  \right)^2  \Prob (B_i=1)   + 
\left( \frac{-\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i)}{1-\Prob_{\theta}(X_i \in A_i)}  \right)^2 \Prob (B_i=-1) \\
& =
 \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{\Prob_{\theta}(X_i \in A_i)} +  
  \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{1-\Prob_{\theta}(X_i \in A_i)} \\
 & =  
  \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{\Prob_{\theta}(X_i \in A_i) \left(1-\Prob_{\theta}(X_i \in A_i) \right)} 
\end{align*}
Therefore
\[
\frac{1}{n} \sum_{i=1}^n \ex{ U_i^2} = L_n(A_1,\ldots,A_n) \overset{a.s.}{\longrightarrow} \kappa(\theta)
\]
for any $\theta \in \Theta$ such that the limit above exists. We now verify that the sequence $\{ U_i,\,i=1,2,\ldots \}$ satisfies Lyaponov's condition for his version of the central limit time: for any $\delta>0$ we have that 
\begin{align*}
& \ex{ \left| U_i \right|^{2+\delta} }=
 \frac{ \left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta}} {(\Prob_{\theta}(X_i \in A_i))^{1+\delta}} +  
  \frac{ \left|\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta}} {(1-\Prob_{\theta}(X_i \in A_i))^{1+\delta}} 
\end{align*}
and
\begin{equation}
\frac{\sum_{i=1}^n\ex{\left| U_i \right|^{2+\delta}} } { \left( \sum_{i=1}^n \ex{U_i^2}  \right)^\delta} = 
\frac{ \frac{1}{n^{1+\delta}} \sum_{i=1}^n \ex{ |U_i|^{2+\delta}} }{ \left(\frac{1}{n} \sum_{i=1}^n \ex{U_i^2 } \right)^\delta}. 
\label{eq:Lyaponov}
\end{equation}
Next, we claim that there exits $\delta>0$ and $K(\delta)>0$, that are independent of $n$, such that
\begin{align}
\frac{1}{n} \sum_{i=1}^n \ex{ |U_i|^{2+\delta}}  < K(\delta) \label{eq:Lyaponov_num}
\end{align} 
for all $n$ large enough. To see this, note that
\begin{align*}
& \ex{ |U_i|^{2+\delta} }= 
 \frac{ \left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta}} {(\Prob_{\theta}(X_i \in A_i))^{1+\delta}} +  
  \frac{ \left|\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta}} {(1-\Prob_{\theta}(X_i \in A_i))^{1+\delta}}  \\
  &  = \frac{\left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta} }
  { (\Prob_{\theta}(X_i \in A_i))^{1+\delta} (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta} } 
  \left( (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta} + (\Prob_{\theta}(X_i \in A_i))^{1+\delta}   \right) \\
  &  \leq \frac{\left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta} }
  { (\Prob_{\theta}(X_i \in A_i))^{1+\delta} (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta}}. 
\end{align*}
%where the last transition is because 
%\[
% \left( (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta} + (\Prob_{\theta}(X_i \in A_i))^{1+\delta}  \right) \leq 1. 
%\]
We now use the fact that each $A_i$ is a finite union of intervals. Under the assumption that there exists $\delta>0$ such that $\eta^{1+\delta}(x)/f^\delta(x)$ is uniquely maximized by the origin and is non-increasing in $|x|$, Lemma~\ref{lem:bound_intervals_delta} implies
\begin{align*}
  &  \leq \frac{\left| \frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right|^{2+\delta} }
  { (\Prob_{\theta}(X_i \in A_i))^{1+\delta} (1-\Prob_{\theta}(X_i \in A_i))^{1+\delta}} = \frac{\left| \sum_{k=1}^{K_i} (-1)^k f(x_{i,k}-\theta) \right|^{2+\delta} }
  { \left( \sum_{k=1}^{K_i} (-1)^k F(x_{i,k}-\theta) \right)^{1+\delta} \left( 1 - \sum_{k=1}^{K_i} (-1)^k F(x_{i,k}-\theta) \right)^{1+\delta}} \\
  & \leq 4^{1+\delta}(f(0))^{2+\delta}.
\end{align*}
It follows that
\[
\frac{1}{n} \sum_{i=1}^n \ex{ |U_i|^{2+\delta}} \leq 4^{1+\delta}(f(0))^{2+\delta}, 
\]
and thus the numerator of \eqref{eq:Lyaponov}, as well as the entire expression, goes to zero. From Lyaponov's central limit theorem we obtain 
\[
\frac{1}{\sqrt{n}} \sum_{i=1}^n U_i \overset{d}{\rightarrow} \Ncal\left(0,\kappa(\theta) \right). 
\]

\subsubsection*{Proof of Claim II} 
We have:
\begin{align*}
\ex{ V_i}  & = 
 \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{\Prob_{\theta}(X_i \in A_i)}  
+ 
 \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{1-\Prob_{\theta}(X_i \in A_i)}  \\
 & = 
  \frac{ \left(\frac{d}{d\theta} \Prob_{\theta}(X_i \in A_i) \right)^2}{\Prob_{\theta}(X_i \in A_i) \left( 1-\Prob_{\theta}(X_i \in A_i) \right)}  
\end{align*}
 We conclude that:
\begin{equation} \label{eq:LAN_limit_cond}
\frac{1}{n} \sum_{i=1}^n \ex{ V_i} =   L_n(A_1,\ldots,A_n)  \to \kappa(\theta)
\end{equation}
Since the $V_i$s are independent of each other, Kolmogorov's law of large numbers implies
%(e.g. \cite[Theorem 10.2.3]{sen1994large}) 
\[
\frac{1}{n} \sum_{i=1}^n  V_i \overset{a.s.}{\longrightarrow} \kappa(\theta)
\]
for any $\theta \in \Theta$ for which the limit \eqref{eq:LAN_limit_cond} exists.
\QEDA


\subsection{Proof of Theorem~\ref{thm:non_existence}
\label{proof:thm:non_existence}
}

Let $\Xi$ be the set of points $\theta \in \Theta$ for which $\kappa(\theta) = \eta(0)$. 
%
Since $B_1,B_2,\ldots$ satisfy the conditions in Theorem~\ref{thm:LAN1}, $\theta$ is in $\Xi$ if and only if
\begin{equation}
\label{eq:non_existence_proof}
\lim_{n\to \infty} L_n(A_1,\ldots,A_n;\theta) = \eta(0). 
\end{equation}
By assumption, we have $B_i^{-1} = A_i$ where $A_i$ can be expressed as
\[
A_i = \cup_{i=1}^K (a_{i,k},b_{i,k}), 
\]
where $a_{i,1} \leq b_{i,1} \leq \ldots \leq a_{i,K}, b_{i,K}$, and $a_{i,1}$ and $b_{i,K}$ may take the values $-\infty$ and $\infty$, respectively. Denote
\[
C_i = \cup_{k=1}^{K}\{a_{i,k},b_{i,k}\}.
\]
For any $\theta$ and $\epsilon>0$, denote 
\[
S_n(\theta, \epsilon) \triangleq \left\{ i\leq n \,:\, (\theta-\epsilon,\theta+\epsilon) \cap C_i \neq \emptyset \right\}
\]
In words, $S_n$ contains all integers smaller than $n$ in which an $\epsilon$-ball around $\theta$ contains an endpoint of one of the intervals consisting $A_i$. 
%
We now claim that 
if $\theta \in \Xi$ then $\card(S_n(\theta, \epsilon))/n \to 1$. Indeed, for such $\theta$ we have
\begin{align}
& L_n(A_1,\ldots,A_n; \theta) \nonumber \\
& = \frac{1}{n} \sum_{i \in S_n(\epsilon,\theta)}  
\frac{ \left(\sum_{k=1}^{K}  f(\theta - b_{i,k})- f(\theta - a_{i,k}) \right)^2}{ \sum_{k=1}^{K} \left( F(\theta - b_{i,k})- F(\theta - a_{i,k}) \right) \left(1-\sum_{k=1}^{K} \left( F(\theta - b_{i,k})- F(\theta - a_{i,k}) \right)\right)} \nonumber \\
& 
+ \frac{1}{n}\sum_{i \notin S_n(\epsilon,\theta) } \frac{ \left(\sum_{k=1}^{K}  f(b_{i,k}-\theta) - f(a_{i,k}-\theta) \right)^2} { \sum_{k=1}^{K} \left( F(\theta - b_{i,k})- F(\theta - a_{i,k}) \right) \left(1-\sum_{k=1}^{K} \left( F(\theta - b_{i,k})- F(\theta - a_{i,k}) \right)\right)} \nonumber \\
& \overset{(a)}{\leq}
\frac{\card\left(S_n(\theta,\epsilon)\right)}{n} \eta(0) + \frac{n-\card\left(S_n(\theta,\epsilon) \right) }{n} \eta(\epsilon) 
 \label{eq:non_existence_proof1}
\end{align}
where $(a)$ follows from Lemma~\ref{lem:bound_intervals_delta} with $\delta =0$, and the fact that for $i \in S_n(\theta, \epsilon)$, 
\[
\max\left\{ \max_k \eta(b_{i,k}-\theta) , \max_k \eta(a_{i,k}-\theta)  \right\} \leq \eta(\epsilon) < \eta(0). 
\]
Unless  $\card \left(S_n(\theta, \epsilon) \right)/n \to 1$, we get that \eqref{eq:non_existence_proof1}, hence $L_n(A_1,\ldots,A_n ; \theta)$, are bounded from above by a constant that is smaller then $\eta(0)$ in contradiction to the fact that $\theta \in \Xi$. \par
For $k\in \N$, assume by contradiction that there exists $N \geq 2K + 1$ distinct elements
$\theta_1,\ldots,\theta_N \in \Xi$. Since each $A_i$ consists of at most $K$ intervals, we have that 
\begin{equation}
\label{eq:few_optimality_points_proof}
\card (\cup_{i=1}^n \mathcal B_i) \leq 2 n K. 
\end{equation}
Fix $\epsilon>0$ such that 
\[
\epsilon < \frac{1}{2}\min_{i\neq j} |\theta_i - \theta_j|. 
\]
Since for each $\theta \in \Theta$ we have $S_n(\theta, \epsilon) \to 1$, there exists $n$ large enough such that 
\[
\card \left(S_n(\theta_i, \epsilon) \right) \geq n \left(1-\frac{1}{2N} \right)
\]
for all $i=1,\ldots,N$. However, $S_n(\theta_1,\epsilon), \ldots S_n(\theta_N,\epsilon)$ are disjoint, so the cardinallity of their union is at least $n\left(1-\frac{1}{2N} \right)N$ which is grater than $2nK + n/2$ in contradiction to \eqref{eq:few_optimality_points_proof}. 

