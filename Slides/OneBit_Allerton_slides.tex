\documentclass[mathserif]{beamer}
%\usepackage{beamerthemesplit}

\usepackage{bm}
\usepackage{tikz}

\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{pstool}

\usepackage[miktex]{gnuplottex}

\tikzstyle{sum}=[circle, fill=blue!10, draw=black,line width=1pt,minimum size = 0.5cm, thick ]
\tikzstyle{ssum}=[circle, fill=red!10,draw=black,line width=1pt,minimum size = 0.05cm]
\tikzstyle{int1}=[draw, fill=blue!10, minimum height = 0.5cm, minimum width=0.5cm,thick ]
\tikzstyle{int}=[draw, fill=blue!10, minimum height = 1cm, minimum width=1.5cm,thick ]
\tikzstyle{Est}=[draw, fill=blue!10, minimum height = 1cm, minimum width=1cm,thick ]

\usetikzlibrary{arrows, positioning}
%\usetikzlibrary{shapes.geometric, ,positioning}
\tikzstyle{box}=[draw, fill=blue!20, scale= 0.8, minimum size=2em]
\tikzstyle{circ}=[draw, circle, fill=red!20, scale= 0.8, minimum size=2em]
\tikzstyle{circ_blue}=[draw, circle, fill=blue!20, scale= 0.8, minimum size=2em]
\tikzstyle{box_red}=[draw, fill=red!20, minimum size=2em, scale= 0.8]


\newcommand{\mmse}{\mathsf{mmse}}
\newcommand{\thetac}{{\color{red} \theta}}
\newcommand{\enc}{\mathsf{enc}}
\newcommand{\qnt}{\mathsf{qnt}}
\newcommand{\sgn}{\mathsf{sign}}
\newcommand{\unif}{\mathsf{unif}}

\newtheorem{prop}{Proposition}
\newtheorem{lem}{Lemma}

\newcommand{\ex}[1]{\ensuremath{\mathbb{E}\left[ #1\right]}}

% bold charactors
\newcommand{\bA}{\bm{A}}
\newcommand{\bB}{\bm{B}}
\newcommand{\bC}{\bm{C}}
\newcommand{\bD}{\bm{D}}
\newcommand{\bE}{\bm{E}}
\newcommand{\bF}{\bm{F}}
\newcommand{\bG}{\bm{G}}
\newcommand{\bH}{\bm{H}}
\newcommand{\bI}{\bm{I}}
\newcommand{\bJ}{\bm{J}}
\newcommand{\bK}{\bm{K}}
\newcommand{\bL}{\bm{L}}
\newcommand{\bM}{\bm{M}}
\newcommand{\bN}{\bm{N}}
\newcommand{\bO}{\bm{O}}
\newcommand{\bP}{\bm{P}}
\newcommand{\bQ}{\bm{Q}}
\newcommand{\bR}{\bm{R}}
\newcommand{\bS}{\bm{S}}
\newcommand{\bT}{\bm{T}}
\newcommand{\bU}{\bm{U}}
\newcommand{\bV}{\bm{V}}
\newcommand{\bW}{\bm{W}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bY}{\bm{Y}}
\newcommand{\bZ}{\bm{Z}}

\newcommand{\ba}{\bm{a}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bc}{\bm{c}}
\newcommand{\bd}{\bm{d}}
\newcommand{\be}{\bm{e}}
%\newcommand{\bf}{\bm{f}}
\newcommand{\bg}{\bm{g}}
\newcommand{\bh}{\bm{h}}
\newcommand{\bi}{\bm{i}}
\newcommand{\bj}{\bm{j}}
\newcommand{\bk}{\bm{k}}
\newcommand{\bl}{\bm{l}}
%\newcommand{\bm}{\bm{m}}
\newcommand{\bn}{\bm{n}}
\newcommand{\bo}{\bm{o}}
\newcommand{\bp}{\bm{p}}
\newcommand{\bq}{\bm{q}}
\newcommand{\br}{\bm{r}}
\newcommand{\bs}{\bm{s}}
\newcommand{\bt}{\bm{t}}
\newcommand{\bu}{\bm{u}}
\newcommand{\bv}{\bm{v}}
\newcommand{\bw}{\bm{w}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bz}{\bm{z}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\snr}{\mathsf{snr}}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\gmid}{\! \mid \!}



%%% Use any fonts you like.
%\usepackage{helvet}

\title{Mean Estimation from One-bit Measurements}
%\subtitle{ISIT 2017}
\author{
\textbf{Alon Kipnis} (Stanford) \\
John Duchi (Stanford)\\ 
}
\date[Allerton 2017]{Allerton \\ October 2017} 

\setbeamertemplate{navigation symbols}{}
\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

%\setbeamerfont{page number in head/foot}{size=\large}
\graphicspath{{../Figs}}

\setbeamertemplate{footline}[frame number]

\begin{document}
\graphicspath{{../Figs/}}

\frame[plain]{\titlepage}



\section{Introduction}


\subsection{Motivation}
\begin{frame}
\frametitle{Motivation}
\framesubtitle{}
Point estimation under communication constraints:\\
\begin{center}
\begin{tikzpicture}[>= latex', node distance = 2cm]
\node at (0,0) (p_th) {$p_{\thetac}(x)$};
\node[right of = p_th] (sample) {$X_1,\ldots,X_n$};
\node[box, right = .75cm of sample, align = left](pm) {limited bits \\ per sample
};
\node[right = .75 cm of pm](th_hat) {$\widehat{\thetac}$};
%\node[below = .1cm of  Xhat]{\textcolor{blue}{reconstruction}};
\draw[->, thick] (p_th) -- (sample);
\draw[->, thick] (sample) -- (pm);
\draw[->, thick] (pm) -- (th_hat);
\end{tikzpicture}
\end{center}
Estimation error is due to:
\begin{itemize}
\item[(i)] limited data 
\item[(ii)] limited bits
\end{itemize}
\bigskip

Relevant scenarios:
\begin{itemize}
\item big data
\item low-power sensors 
\item distributed computing / optimization (bottleneck is due to communication between processing units)

\end{itemize}

\end{frame}

\begin{frame}
\frametitle{This talk: }
\framesubtitle{}
Estimating the mean $\thetac$ of a normal distribution $\mathcal N(\thetac,\sigma^2)$ from one-bit per sample ($\sigma$ is known)
\bigskip

\end{frame}
%

\begin{frame}
\frametitle{Three Encoding Scenarios}

\begin{center}
\begin{tikzpicture}[node distance=2cm,auto,>=latex]
  \node at (0,0) (source) {$X_1$} ;
  \node[int1, right of = source, node distance = 2cm, scale = 0.75] (enc1) {Enc 1};  
\draw[->,line width = 2pt] (source) -- (enc1); 

 \node[below of = source, node distance = 1.5cm] (source2) {$X_2$};
\node[int1, right of = source2, node distance = 2cm, scale = 0.75] (enc2) {Enc 2};  
\draw[->,line width = 2pt] (source2) -- (enc2); 

\node[below of = source2, node distance = 2.5cm] (source3) {$X_n$};
\node[int1, right of = source3, node distance = 2cm, scale = 0.75] (enc3) {Enc n};  
\draw[->,line width = 2pt] (source3) -- (enc3); 

%\node[above of = source, node distance = 1cm] (dist) {${\mathcal N} \left(\theta, \sigma^2 \right)$};

\node[Est, right of = enc2, node distance = 5cm] (est) {Est};
\draw[->,line width = 2pt] (enc1) -| node[above, xshift = -2.5cm] (mes1) {$M_1 \in \left\{-1,1\right\}$} (est);   
\draw[->,line width = 2pt] (enc2) -- node[above, xshift = 0cm] (mes2) {$M_2 \in \left\{-1,1\right\}$} (est);   
\draw[->,line width = 2pt] (enc3) -| node[above, xshift = -2.5cm]  {$M_n \in \left\{-1,1\right\}$} (est);   
\node[right of = est, node distance = 2.5cm] (dest) {$\widehat{\theta}(M^n)$};
\draw[->, line width=2pt] (est) -- (dest);

\node[below of = source2, node distance = 0.5cm] {$\vdots$};
\node[below of = enc2, node distance = 0.5cm] {$\vdots$};
\node[below of = mes2, node distance = 0.7cm] {$\vdots$};

\visible<2>{
\draw[<->,line width = 2pt, color = red] (enc1) -- (enc2);   
\draw[<->,line width = 2pt, color = red] (enc2) -- (enc3);
\draw[dashed,line width = 2pt, color = white] (enc2)+(0,-0.5) -- +(0,-1.5);}

\visible<3->{
\draw[->,line width = 2pt, color = blue] (enc1) -- (enc2);   
\draw[->,line width = 2pt, color = blue] (enc2) -- (enc3);
\draw[dashed,line width = 2pt, color = white] (enc2)+(0,-0.7) -- +(0,-1.7);}

\end{tikzpicture}
\end{center}

\begin{itemize}
\item Distributed: $M_i = f_i(X_i)$
\visible<2->{
\item {\color{red} Centralized}: $M^n = (M_1,\ldots,M_n) = f(X_1,\ldots,X_n)$ }
\visible<3->{
\item {\color{blue} Adaptive / Sequential}: $M_i = f_i(X_i,M^{i-1})$ }
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Related Work}
\begin{itemize} 
\item Estimation via compressed information [Han '87], [Zhang \& Berger '88]
\item Distributed hypothesis testing under quantization [Tsitsiklis '88]
\item Estimation from multiple machines subject to a bit constraint [Zhang, Duchi, Jordan, Wainwright '13]
\item Remote multiterminal source coding (CEO) [Berger, Zhang, Wiswanathan '96], [Oohama '97]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Consistency}
Q: in what setting consistent estimation is possible? \\
\bigskip
\pause
A: all !
\[
M_i = \mathbf 1(X_i>0),\quad i=1,\ldots,n
\]
(as in the distributed setting)
\[
\frac{1}{n}\sum_{i=1}^n M_i \rightarrow \Prob\left(X<0 \right) = \Phi\left(X/\sigma<\thetac\right)
\]
\end{frame}

\subsection{Preliminary}

\begin{frame}
\frametitle{Efficiency}
\textbf{Definition:} \emph{asymptotic relative efficiency (ARE)} of an estimator:
\[
\mathrm{ARE}(\widehat{\theta}) \triangleq \lim_{n\rightarrow \infty} \frac{\mathbb E \left[ \left(\widehat{\theta}-\theta \right)^2 \right]}{\sigma^2 / n}
\]
($\sigma^2/n$ is the minimax risk without communication constraints)
%MSE of sample mean $\bar{\theta}$: 
%\[
%\mathbb E \left[ \left(\bar{\theta}-\theta \right)^2 \right] = \frac{\sigma^2}{n}
%\]
%(minimax estimator without communication constraint) 

\bigskip
\pause
Q: in what scenarios the ARE is finite? \\
\bigskip
\pause
This talk: all three scenarios

\end{frame}

\begin{frame}
\frametitle{ARE under Centralized Encoding}

\begin{prop} If the parameter space $\Theta$ is bounded, then the ARE under centralized encoding is $1$
\end{prop}
\textbf{Proof:}
\[
\mathbb E \left( \theta- \widehat{\theta} \right)^2 = \overbrace{\mathbb E \left( \theta- \bar{\theta} \right)^2}^{\sigma^2/n} + \mathbb E \left( \bar{\theta}- \widehat{\theta} \right)^2 
\]
\begin{itemize}
\item Encoder is required to describe $\bar{\theta}$ using $n$ bits
\begin{itemize}
\item divide parameter space $\Theta$ into $2^n$ regions of equal size
\item send region index where $\bar{\theta}$ falls
\end{itemize}
\item MSE in estimating $\bar{\theta}$ decreases exponentially in $n$ 
\end{itemize}
\bigskip
Note: globally optimal strategy for a finite $n$ is hard to derive since mean of $\bar{\theta}$ is unknown 
\end{frame}

\begin{frame}
\frametitle{Relation to CEO}

\begin{center}
\begin{tikzpicture}[node distance=2cm,auto,>=latex]
  \node at (0,0) (source) {$\thetac$} ;
  \node[ssum, right of = source, scale = 0.75] (sum1) {+};
  \node[above of = sum1, node distance = 1.2cm] (z1) {$Z_1$};
  \node[int1, right of = sum1, node distance = 2cm, scale = 0.75] (enc1) {Enc 1};  
\draw[->,line width = 2pt] (source) -- (sum1); 
\draw[->,line width = 2pt] (sum1) -- node[above] {$X_1$} (enc1); 
\draw[->,line width = 2pt] (z1) -- (sum1); 

\node[below of = sum1, node distance = 0.5cm] (vdts) {$\vdots$};
\node[below of = enc1, node distance = 0.5cm] (vdts2) {$\vdots$};

 \node[coordinate,below of = source, node distance = 2.5cm] (source2) {};
 \node[ssum, right of = source2, scale = 0.75] (sum2) {+};
  \node[above of = sum2, node distance = 1.2cm] (z2) {$Z_n$};
\node[int1, right of = sum2, node distance = 2cm, scale = 0.75] (enc2) {Enc n};  

\node[Est, right of = enc2, node distance = 5cm, yshift = 1cm] (est) {Est};

\draw[->,line width = 2pt] (source) -- (sum2); 
%\draw[->,line width = 1pt, dashed] (source) -- (vdts);
%\draw[->,line width = 1pt, dashed] (vdts)--(vdts2);
%\draw[->,line width = 1pt, dashed] (vdts2) -- (est);

\draw[->,line width = 2pt] (sum2) -- node[above] {$X_n$} (enc2); 
\draw[->,line width = 2pt] (z2) -- (sum2); 

%\draw[fill=blue!10] (enc1)+(5,0.5) rectangle (enc2)+(2,-0.5); 

%\node[int1, right of = enc2, node distance = 6cm, scale = 0.75] (est) {Est};
\draw[->,line width = 2pt] (enc1) -- node[above, xshift = 0cm] (mes1) {$R_1$} (est);   
\draw[->,line width = 2pt] (enc2) -- node[above, xshift = 0cm] (mes2) {$R_n$} (est);   
\node[right of = est, node distance = 2cm] (dest) {$\widehat{\theta}$};
\draw[->, line width=2pt] (est) -- (dest);
\end{tikzpicture}
\end{center}

Assume:
\begin{itemize}
\item $\theta \sim \mathcal N(0,\sigma_\theta^2)$, ~~~ 
\item $Z_1,\ldots,Z_n \overset{\mathrm{i.i.d}}{\sim}\mathcal N(0,\sigma^2)$
\end{itemize}
Encode $k$ instances:
\begin{itemize}
\item $R_1 = \ldots = R_n =1 $
\item $D_{CEO} = \frac{1}{k} \sum_{j=1}^k \mathbb E \left(\theta_j-\widehat{\theta}_j \right)^2$
\end{itemize}
\end{frame}

\begin{frame}

From [K., Rini, Goldsmith '17]: 
\[
D_{CEO} \leq \frac{4}{3} \frac{\sigma^2}{n} + o(1)
\]
%\begin{itemize}
%\item[] CEO distortion satisfies [Chen, Zhang, Berger, Wicker '04]:
%\[ D^\star = \frac{4}{3} %\frac{\sigma^2}{n} + o(n^{-1}) \]
%\end{itemize}
ARE of $4/3$ can be attained in a fully distributed encoding (with encoding over blocks of multiple problem instances)
\bigskip
\begin{alertblock}{Conclusion}
Distributed encoding is almost not a limiting factor 
(although inability to exploit concentration of measure in high dimension -- might!)
\end{alertblock}
\end{frame}

\section{Adaptive Encoding} 

\begin{frame}
\frametitle{Main Results (adaptive encoding)}

\begin{theorem}[converse]
No estimator have ARE lower than $\pi/2$
\end{theorem}

\begin{theorem}[achievability]
Assume that $\Theta$ is a bounded interval. There exists an estimator with ARE equals to $\pi/2$
\end{theorem}

\begin{theorem}[one-step optimal strategy]
The next step one-bit message that minimizes the MSE is of the form $M = \sgn (X-\tau)$ where $\tau$ satisfies the fixed-point equation 
\[
\tau = \frac{1}{2} \left( \frac{ \int_{-\infty}^\tau \theta \pi(d\theta) }{\int_{-\infty}^\tau \pi(d\theta)}  + \frac{ \int_{\tau}^\infty \theta \pi(d\theta)}{\int_{\tau}^\infty \pi(d\theta)} \right)
\]
\end{theorem}
\end{frame}

\begin{frame}
\frametitle{Proof}
\framesubtitle{converse (ARE $\geq \pi/2$)}
Assume a prior $\pi(d\theta)$ on $\Theta$ with location Fisher information $I_\pi$. \\

The van-Trees inequality (e.g. [Tsybakov '08]) implies
\[
\mathbb E \left(\theta - \widehat{\theta} \right)^2 \geq \frac{1}{\mathbb E I_\theta(M^n) + I_\pi} \geq \frac{1}{\sum_{i=1}^n I_\theta(M_i|M^{i-1}) + I_\pi} 
\]
\begin{lem}
\[
I_\theta(M_i|M^{i-1}) \leq 2/(\pi\sigma^2)
\]
\end{lem}
(proof by induction over a finite set of intervals approximating $M_i^{-1}(1)$ given $M^{i-1}$)
\end{frame}

\begin{frame}
\frametitle{Proof}
\framesubtitle{achievability (existence of an estimator with ARE $= \pi/2$)}
[Polyak \& Juditsky '92]: \\
\[
\begin{cases}
\theta_i = \theta_{i-1} + \gamma_i \varphi(X_i - \theta_i)& i=1,\ldots,n\\
\widehat{\theta} = \frac{1}{n} \sum_{i=1}^n \theta_i &
\end{cases}
\]
where:
\begin{itemize}
\item[(i)] $\gamma_n \rightarrow 0^+$ ``not too slow'' 
\item[(ii)] $\psi(x) = \mathbb E \varphi(x+Z)$ 
\item [(iii)] $\chi(x) = \mathbb E \varphi^2(x+Z)$ 
\item[(iv)] some regularity  conditions on $\varphi$, $\chi$, $\psi$
\end{itemize}
Then 
\[
\sqrt{n}(\theta- \widehat{\theta}) \rightarrow \mathcal N\left(0,V\right)
\]
where $V = \chi(0)/\psi'^2(0)$.  \\
\medskip
\pause
Proof of theorem: take $\varphi(x) = \sgn(x)$
\end{frame}

\begin{frame}
\frametitle{One-step optimality}
%Upon observing $M^{i-1}$, what is the optimal detection region $M_i^{-1}(1)$ that minimizes the MSE ? \\
\begin{theorem}
Let $\pi(\theta)$ be an absolutely continuous log-concave distribution. For $X\sim \mathcal N(\theta,\sigma^2)$ let
\[
M = \sgn(X-\tau)
\]
where $\tau$ is the unique solution to
\[
\tau = \frac{1}{2} \left( \frac{ \int_{-\infty}^\tau \theta \pi(d\theta) }{\int_{-\infty}^\tau \pi(d\theta)}  + \frac{ \int_{\tau}^\infty \theta \pi(d\theta)}{\int_{\tau}^\infty \pi(d\theta)} \right)
\]
Then for any $M'(X) \in \{-1,1\}$ and $\widehat{\theta}(M')$:
\[
\mathbb E \left(\theta- \widehat{\theta}(M') \right)^2 \geq \mathbb E \left(\theta - \mathbb E[\theta|M] \right)^2
\]
\end{theorem}
\vspace{-20pt}
\begin{alertblock}{Interpertation:}
The optimal one-bit message is a threshold detector. The threshold is the fixed-point that balances conditional center of masses given the message
\end{alertblock}

\end{frame}


\begin{frame}
\frametitle{One-step Optimal Scheme}
Initialization: $P_0(t) = \pi(\theta)$ \\
Repeat for $n \geq 1$:
\begin{itemize}
\item[(i)] $P_n(t) = \Prob(\theta = t|M^n) = \alpha_n P_{n-1}(t) \Phi \left(M_n \frac{t-\tau_{n-1}}{\sigma} \right)$ 
\item[(ii)] $\widehat{\theta} = \mathbb E[\theta | M^n] = \int t P_n(t)dt $ 
\item[(iii)] Find $\tau_n$ from 
\[
\tau_n = \frac{1}{2} \left( \frac{ \int_{-\infty}^\tau t P_n(t)dt }{\int_{-\infty}^\tau P_n(t)dt}  + \frac{ \int_{\tau}^\infty t P_n(t)dt}{\int_{\tau}^\infty P_n(t)dt} \right)
\]
\item[(iv)] $M_{n+1} = \sgn(X_{n+1}-\tau_n)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Numerical Example}
\begin{center}
Normalized empirical risk versus number of samples $n$ \\
(500 Monte Carlo experiments)
\begin{tikzpicture}
\node at (0,0) {\includegraphics[scale=0.5]{one_bit_adpative}};
%\node[color = blue,  minimum width=1cm, align = left] at (1.5,1.5) {assymptotically optimal \eqref{eq:sgd_est}};
%\node[color = red, minimum width=1cm, align = left] at (1.5,0.55) {one-step optimal \eqref{eq:message_update}} ;
\node[rotate = 90, scale = 1] at (-5,0) {$n \mathbb E \left(\widehat{\theta}_n - \theta \right)^2$};
\node[scale = 1] at (0,-3) {$n$};
%\draw[dashed] (-3.5,1.18) node[left, scale = 0.7] {$\frac{\pi}{2}$}-- (-3,1.18);
\end{tikzpicture}
$\theta \sim \unif(-3,3)$
\end{center}
\end{frame}

\section{Distributed Encoding}

\begin{frame}
\frametitle{Distributed Encoding}
\framesubtitle{Threshold Detection}
We consider only messages of the form
\[
M_i = \sgn(X_i - t_i),\quad i=1,\ldots,n
\]
\begin{center}
\begin{tikzpicture}
\node[coordinate] at(0,0) (ti) {};
\draw[color = red, line width = 1.5pt] (ti)+(0,0.2) -- node[above, yshift = 0.07cm] {\color{black} $t_i$} +(0,-0.2);
\draw[<-,line width = 1pt] (-3,0) -- node[above] {$M_i=-1$} (ti);
\draw[->,line width = 1pt] (ti) -- node[above] {$M_i=1$} (6,0);
\end{tikzpicture}
\end{center}
Assume:
\[
\lambda_n([a,b]) = \frac{1}{n} \left| [a,b] \cap \left\{t_i \right\}\right|
\]
converges weakly to a probability distribution $\lambda$
\\
\bigskip
\pause
\textbf{Example:} $t_1,\ldots,t_n$ are drawn independently from a probability distribution $\lambda$ on $\mathbb R$ 

\end{frame}

\begin{frame}
\frametitle{Main Results (distributed encoding)}

\begin{theorem}
\begin{itemize}
\item[(i)] For any estimator $\widehat{\theta}$:
\[
\liminf_{c\rightarrow \infty}\, \liminf_{n\rightarrow \infty} \sup_{\tau\,:\,| \tau - \theta| \leq \frac{c}{\sqrt{n}} }  n \mathbb E \left(\widehat{\theta} - \tau \right)^2 \geq \sigma^2/K(\theta),
\]
where:
\begin{align*}
K(\theta) & = \int_{\mathbb R} \eta\left( \frac{t-\theta}{\sigma}\right) \lambda(dt) \\
\eta(x) & = \frac{\phi^2(x)}{\Phi(x)\Phi(-x)}
\end{align*}
\item[(ii)] The Maximum likelihood estimator $\widehat{\theta}_{ML}$  satisfies
\[
\sqrt{n}(\theta - \widehat{\theta}_{ML}) \rightarrow \mathcal N\left(0,\sigma^2/K(\theta) \right)
\]
\end{itemize}
\end{theorem}
%{\color{blue} Proof:} $P(M^n|\theta)$ is LAN
\end{frame}


\begin{frame}
\frametitle{Interpretations}
\begin{itemize}
\item ML estimator is local asymptotically minimax 
\item ARE of ML is $1/K(\theta)$ --  depends only in the asymptotic threshold density $\lambda$
\end{itemize}

\begin{itemize}
\item \[
1/K(\theta) = \frac{1}{\int \eta \left( \frac{t-\theta}{\sigma}\right) \lambda(dt)}
\geq \frac{1}{\int \eta \left(0\right) \lambda(dt)} = \pi / 2
\]
(attained by $\lambda(dt) = \delta_{\theta}$)
\item Minimax $\lambda$ for $\theta \in (-b\sigma,b\sigma)$:
\begin{align*}
\mathrm{maximize} \quad &  \inf_{\tau \in (-b,b)} \int \eta(t-\tau) \lambda(dt)
\\ \nonumber
\mathrm{subject~to} 
\quad & \lambda(dt)\geq 0,\quad \int \lambda(dt) \leq 1. 
\end{align*}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Minimax $\lambda$}
\begin{center}
support of optimal threshold density $\lambda^\star$
\vspace{10pt}
\hspace{-27pt}
\begin{tikzpicture}
\node[int] (b1) at (0,0)  {\includegraphics[scale=0.23]{minimax_b_only1}};
\node[int] (b2) at (3.9,0)  {\includegraphics[scale=0.23]{minimax_b_only2}};
\node[int] (b3) at (7.8,0)  {\includegraphics[scale=0.23]{minimax_b_only5}};
\node[int] (b4) at (0,-3)  {\includegraphics[scale=0.23]{minimax_b_only10}};
\node[int] (b5) at (3.9,-3)  {\includegraphics[scale=0.23]{minimax_b_only50}}; 
\node[int] (b6) at (7.8,-3)  {\includegraphics[scale=0.23]{minimax_b_only100}}; 
\end{tikzpicture}
\end{center}
\[
K^\star = \inf_{\theta} K^\star(\theta) = \inf_\theta \int \eta(t-\theta) \lambda^\star(dt)
\]
\end{frame}

\begin{frame}
\frametitle{Minimax $\lambda$}
\begin{center}
Minimax ARE vs size of parameter space %$\Theta= (-\sigma b, \sigma,b)$

\begin{tikzpicture}
\node at (-6,0)  {\includegraphics[scale=0.35]{k_vs_b2}};
\node at (0,-2) {$2b$};
\node[rotate = 90] at (-8.5,0) {$1/K^\star$};
\node at (-6,-2) {$2b$};

\node at (0,0)  {\includegraphics[scale=0.35]{K_vs_b}};
\node at (0,-2) {$2b$};
\node[rotate = 90] at (-2.7,0) {$\frac{1}{2b} \times 1/K^\star$};
\end{tikzpicture}
\end{center}
\begin{itemize}
\item ARE increases with size of parameter space
\end{itemize}

\end{frame}



\section{Summary}

\begin{frame}
\frametitle{Summary}
\begin{itemize}
\item Asymptotic relative efficiency in adaptive setting is $\pi/2$ regardless of size of parameter space -- only $\sim 1.57$ more samples are required due to 1-bit constraints
\item One-step optimal one-bit message is a threshold detector
\item ARE in distributed setting is finite
\item 
ML estimator is local asymptotically optimal for threshold detection
\item ARE of ML is characterized by asymptotic density of threshold values
\item Minimax ARE of ML depends on size of parameter space
\end{itemize}

\bigskip
\begin{alertblock}{Open question}
Is there a distributed encoding scheme with ARE that is both finite and independent of size of parameter space ?
\end{alertblock}


\end{frame}


\end{document}