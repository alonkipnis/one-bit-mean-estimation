% -*- Mode: latex -*- %

\section{Introduction}
\label{sec:Intro}

We consider estimation of parameters from data collected by multiple units
under communication constraints between the units.  Such scenarios arise in
sensor arrays, where sensor motes collect information, which they transmit
to a central estimation unit~\cite{LesserOrTa03,LiWoHuSa02}. More generally,
communication is substantially more expensive than computation in modern
computing infrastructure~\cite{FullerMi11}.  It is thus of interest to
understand the extent to which communication constraints induce fundamental
accuracy and efficiency limits in parametric estimation problems.
%%
\begin{figure*}
  \include{Figs/encoding-settings}
  \caption{\label{fig:setup} Three encoding settings: (i) Centralized -- an
    encoder sends $n$ bits after observing $n$ samples. (ii) Adaptive
    (sequential) -- the $i$th encoder sends the bit $B_i$ depending on its
    private sample $X_i$ and previous bits $B_1,\ldots,B_{i-1}$. (iii)
    Distributed -- each encoder send the bit $B_i$ based on its private
    sample $X_i$ only.}
\end{figure*}
%%

We answer this question in a sylized version of this problem: the estimation
of the mean $\theta$ of a symmetric log-concave distribution under the
constraint that only a single bit can be communicated about each observation
from this distribution.
Different information sharing schemes strongly affect the
performance of estimators for $\theta$; we illustrate
the three main settings we consider in Figure~\ref{fig:setup}.
\begin{enumerate}[(i)]
\item \emph{Centralized} encoding: all $n$ encoders confer and produce a
  single message consists of $n$ bits.
 \item \emph{Adaptive} or \emph{sequential} encoding: The $n$th encoder
   observes the $n$th sample and the $n-1$ previous bits.
 \item \emph{Distributed} encoding: The $n$th message is only a function of
   the $n$th sample.
\end{enumerate}
The distributed setting~(iii) is the most restrictive; as it turns out,
(ii) is slightly more restrictive than the fully centralized setting~(i),
and in our setting, a variant of the adaptive setting~(ii)
in which there is only \emph{one} round of adaptivity---as we make formal
later---is enough to achieve the same efficiency as the fully sequential
setting~(ii).
Each setting has natural applications:
\begin{itemize}
\item {\bf Signal acquisition (i):} A quantity is measured $n$ times at
  different instances. The results are averaged in order to reduce
  measurement noise and the averaged result is then stored or communicated
  using $n$ bits.
\item {\bf Analog-to-digital conversion (ii):} A sigma-delta modulator (SDM)
  converts an analog signal into a sequence of bits by sampling the signal
  at a very high rate and then using one-bit threshold detector combined
  with a feedback loop to update an accumulated error state
  \cite{1092194}. Therefore, the expected error in tracking an analog signal
  using an SDM falls under our setting (ii) when we assume that the signal
  at the input to the modulator is a constant (direct current) corrupted by,
  say, thermal noise \cite{53738}. Since the sampling rates in SDM are
  usually many times more than the bandwidth of its input, analyzing SDM
  under a constant input provides meaningful lower bound even for
  non-constant signals.
\item {\bf Privacy (ii)--(iii):} A business entity is interested in
  estimating the average income of its clients. In order to keep this
  information as confidential as possible, each client independently
  provides an answer to a yes/no question related to its
  income~\cite{DuchiJoWa18}.
\end{itemize}

Let us provide an informal description of our results and setting. For an
estimator $\theta_n$ with finite quadratic risk (mean squared error (MSE)) $R_n =
\E_\theta[(\theta_n - \theta)^2]$, we study the limit
\begin{equation}
  \label{eq:ARE_def}
  \limsup_{n\to \infty} n R_n.
\end{equation}
By comparing this quantity to achievable rates of convergence without
communication constraints, we can evaluate the efficiency
losses---asymptotic relative efficiency---of the estimator to appropriately
optimal (unconstrained) estimators. (We shall be more formal in the sequel.)
By lower bounding the quantity~\eqref{eq:ARE_def}, we also provide limits on
estimation of single-bit-per-measurement constrained signals in more general
settings~\cite{baraniuk2017exponential, jacques2013robust, plan2013one,
  li2017channel, choi2016near}.

% given a prior distribution on the space $\Theta$ where $\theta$ resides. \\

In setting (i), the estimator can evaluate any optimal estimator of location
(e.g., the sample mean if the data is Gaussian), then quantize it using
$n$ bits. As the accuracy in describing the empirical mean decreases
exponentially in the number of bits, the quantization error is negligible compared
to the statistical error in mean estimation~\cite{720540, cai2020distributed}. That is,
centralized encoding induces no asymptotic efficiency loss.
%
The story is different in settings (ii) and (iii). Precisely, we show that
in the adaptive setting~(ii), the optimal efficiency of a one-bit scheme is
(asypmtotically) precisely that of the sample median, and that this
efficiency is achievable. As a concrete example, when $X_i$ are
i.i.d.\ Gaussian, we necessarily lose a factor of $\pi/2 \approx 1.57$ in
the asymptotic risk; the one-bit constraint decreases the effective sample
size by a factor of $\pi/2$ compared to estimating it without the bit
constraint. It turns out that, in the settings we consider,
only a \emph{single round} of adaptivity (see Fig.~\ref{fig:one_round} for
an illustration) is sufficient to achieve optimal convergence rates.
%
In distinction from setting (ii), in setting (iii) when the messages must be
independent, there is no distributed estimation scheme that achieves the
efficiency of the sample median uniformly over $\theta$.  We establish this
result via Le Cam's local asyptotic normality theory, allowing us
to provide exact characterizations of the asymptotic efficiency
of suitably regular encoding schemes.
%% To summarize our
%% contributions, we establish that the optimal ARE in settings (i) is $1$, the
%% optimal ARE in setting (ii) is ARE of the sample median, and no uniformly
%% optimal procedure exists in setting (iii).

% Instead, in setting (iii) we restrict ourselves to estimators from messages obtained by comparison against a prescribed value (that may be different for each sample). We show that the maximum likelihood (ML) estimator for $\theta$ from these messages is asymptotically local minimax, and its asymptotic variance is strictly greater than the variance of the median. Thus, at least when limited to threshold detection, the ability to adapt the threshold allows for a more efficient estimation. %This is in contrast to the \\ counterpart of our setting where $\theta$ is taken from a finite space 

Our asymptotic setting is important in that it allows us to elide
difficulties present in finite sample settings. For example, in setting~(i),
developing an optimal quantizer at finite $n$ requires choosing a $2^n$
level scalar quantizer, which is non-trivial~\cite{gray1998quantization}.
In interactive and sequential settings (e.g.~(ii)), the situation is more
challenging, as it is unclear whether any type of compositionality applies,
in that an $n-1$-step optimal estimator may be only vaguely related to the
$n$-step optimal estimator. Thus, to provide our lower bounds, we rely on
stronger information-based inequalities, including the Van Trees
inequality~\cite{Tsybakov09} and Le Cam's local asymptotic normality
theory~\cite{LeCam86,LeCamYa00,VanDerVaart98}.

%% It is important to note that although the ARE in setting (i) is one, this
%% scheme already poses a non-trivial challenge for the design and analysis of
%% optimal encoding and estimation procedures. Indeed, representing an unknown
%% random quantity using $n$ bits is equivalent to designing a $2^n$ levels
%% scalar quantizer \cite{gray1998quantization}. However, the optimal design of
%% this quantizer depends on the distribution of its input, which is the goal
%% of our estimation problem and hence its exact value is unknown. As a result,
%% a non-trivial exploration-exploitation trade-off arises even in setting (i).
%% %Note that the only missing parameter in our setting is the mean, which, under setting (i), is known to the encoder with uncertainty interval proportional to $\sigma/\sqrt{n}$. 
%% Therefore, while uncertainty due to quantization decreases exponentially in
%% the number of bits $n$, hence the ARE is $1$, it is still difficult to
%% derive an exact expression for the MSE in this setting.
%
%% The situation is even more involved in the adaptive encoding setting (ii): an encoding and estimation strategy that is optimal for $n-1$ adaptive one-bit messages of a sample of size $n-1$ may not lead to a globally optimal strategy upon the recipient of the $n$th sample. Conversely, any one-step optimal strategy, in the sense that it finds the best one-bit message as a function of the current sample and the previous $n-1$ messages, is not guaranteed to be globally optimal. Therefore, while we characterize the optimal one-bit message given the previous messages, this characterization does not necessarily lead to an upper bound on the ARE. Instead, our result on the maximal ARE is obtained by bounding the Fisher information of any $n$ adaptive messages and using an appropriate information inequality. \par
%
%In addition to encoding and estimation schemes that lead to optimal results, we also consider two additional ``natural'' schemes. Specifically, in setting (ii) we consider the one-bit optimal scheme, i.e., the case of a greedy encoder that given the $n$th sample and the previous $n-1$ bits, provides a bit that minimizes the $n$-step MSE. In setting (iii) we also consider the case where the messages are obtained by comparing each sample against a prescribed threshold. This threshold may be different across samples and is assumed deterministic (independent of the data). 

\subsection*{Related Work}

The many challenges of estimation under communication constraints have given
rise to a large literature investigating different aspects of constrained
estimation. While our setting---in which we observe a single bit per signal
$X_i$---is restrictive, it inspires substantial work.  Perhaps the most
related is that of Wong and Gray~\cite{53738}, who study one-bit
analog-to-digital conversion of a constant input corrupted by Gaussian noise
using a Sigma-Delta Modulator (SDM). They show almost sure convergence, but
provide no rate (and no rates follow from their analysis); in contrast, we
provide an optimal procedure and matching lower bound achieving risk
$\frac{\pi}{2} \sigma^2$ in the limit~\eqref{eq:ARE_def} when $X_i \simiid
\normal(\theta, \sigma^2)$. A growing literature on one-bit measurements in
high-dimensional problems \cite{baraniuk2017exponential, DavenportPlVaWo15,
  PlanVe13} shows how to reconstruct sparse signals, where Baraniuk et
al.~\cite{baraniuk2017exponential} show that in noiseless settings,
exponential decay in MSE is possible; our results make precise the penalty
for noise under one-bit sensing, showing that the error can decay (under
Gaussian noise) at best as $\frac{\pi}{2} \frac{\sigma^2}{n}$.

In fully distributed settings (iii), the challenges are different, and there
is also a substantial literature with one-bit (quantized)
measurements~\cite{904560,4244748, 6882252, chen2010performance, 5184907}.
We complement these results by providing precise lower bounds and optimality
results; previous performance bounds are suboptimal.  Work on the remote
multiterminal source coding problem, or CEO problem~\cite{berger1996ceo,
  viswanathan1997quadratic, oohama1998rate, prabhakaran2004rate}, provides
lower bounds on the MSE in setting~(iii); because of the somewhat distinct
setting, these bounds are looser than ours (which have optimal constants).
In settings more similar to our statistical estimation scenario---such as
estimation of parameters in a multi-dimensional linear model---a line of
work provides lower bounds on statistical
estimation~\cite{zhang2013information, duchi2014optimality, GargMaNg14,
  BravermanGaMaNgWo16, DBLP:journals/corr/abs-1802-08417,
  zhang1988estimation, han2018distributed, xu2017information}. These results
are finite sample and apply more broadly than ours, but as a consequence,
they have unusable constants, while our stylized model allows precise
identification of exact constants.  Work subsequent to the initial draft of
this paper~\cite{Barnes2018} uses an approach similar to ours---bounding
quantized Fisher information---to derive lower bounds on the error in
parametric estimation problems from quantized measurements in non-adaptive
settings.

Testing (and discrete estimation) problems also enjoy a robust literature,
though as a consequence of our results to come, the results for testing,
i.e., when the parameter space $\Theta$ is finite, are quite different from
those for estimation, as it is possible to construct optimal decision
(testing) rules in a completely distributed fashion. In this context, Longo
et al.~\cite{52470} propose procedures for distributed testing based on
optimizing a Bhattacharyya distance.
Tsitsiklis~\cite{tsitsiklis1988decentralized} shows that when the
cardinality of $\Theta$ is at most $M$ and the probability of error
criterion is used, then no more than $M(M-1)/2$ different detection rules
are necessary in order to attain probability of error with optimal exponent.
Moreover, in a distributed setting, feedback is unnecessary for optimal
testing/detection~\cite{5751320}, in strong distinction to the estimation
case we consider.

%% , it was shown that, with specific two-stage feedback, there is no gain in feedback compared to the fully distributed setting. Our results imply that the ARE in the distributed setting with threshold detection rules is strictly larger than that in the adaptive setting for almost all points in $\Theta$, suggesting that the case where the cardinality of $\Theta$ is finite is different from the case where $\Theta$ is an open set.\par

%% As we explain in detail in Section~ \ref{sec:preliminary}, the remote
%% multiterminal source coding problem, also known as the CEO problem
%% \cite{}, leads to lower bounds on the MSE in setting
%% (iii). For the case of a Gaussian distribution, this lower bound bounds the
%% ARE to be at most $3/4$. Thus, while this bound on the ARE provides no new
%% information compared to the upper bound of $2/\pi$ we derive for setting
%% (ii), it shows that the distributed nature of the problem is not a limiting
%% factor in achieving MSE close to optimum even under the one bit per sample
%% constraint. 
%% % 

%% Finally, we note that minimax estimation under limited communication with an
%% arbitrary number of bits per node was considered in
%% \cite{zhang2013information, duchi2014optimality}. The specialization of the
%% results in \cite{zhang2013information, duchi2014optimality} to our settings
%% (ii) and (iii) leads to looser lower bounds then the ones we derive in this
%% paper. Looser bounds can also be obtained from works considering general
%% inference and distributed estimation problems under data compression
%% constraint \cite{DBLP:journals/corr/abs-1802-08417, zhang1988estimation,
%%   han2018distributed, xu2017information, Barnes2018}. In particular, the
%% subsequent work of \cite{Barnes2018} uses an approach similar to ours to
%% derive lower bound on the error in various parametric estimation problems
%% from quantized measurements. \par


The remainder of this paper is organized as follows. In
Section~\ref{sec:problem} we describe the problem, notation, and
our basic assumptions. In
Section~\ref{sec:preliminary} we provide two simple bounds on the efficiency
and MSE. Our main results for the adaptive and distributed cases are given
in Sections~\ref{sec:sequential} and \ref{sec:distributed}, respectively. In
Section~\ref{sec:conclusions} we provide concluding remarks.
