\relax 
\citation{53738}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:Intro}{{I}{1}}
\citation{gray1998quantization}
\citation{gill1995applications}
\citation{cicalese2002least}
\citation{Karp:2007:NBS:1283383.1283478}
\citation{53738}
\citation{53738}
\citation{baraniuk2017exponential}
\citation{904560}
\citation{4244748}
\citation{6882252}
\citation{chen2010performance}
\citation{5184907}
\citation{52470}
\citation{tsitsiklis1988decentralized}
\citation{5751320}
\citation{berger1996ceo}
\citation{viswanathan1997quadratic}
\citation{oohama1998rate}
\citation{prabhakaran2004rate}
\citation{zhang2013information}
\citation{zhang2013information}
\citation{han1987hypothesis}
\citation{zhang1988estimation}
\citation{singh2009limits}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Centralized encoding using one bit per sample on average.\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:centralized}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Adaptive single-bit encoding: the $i$th encoder delivers a single bit message which is a function of its private sample $X_i$ and the previous messages $M_1,\ldots  ,M_{i-1}$.\relax }}{3}}
\newlabel{fig:sequential}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Distributed single-bit encoding: the single-bit message produced by each encoder is only a function of its private sample $X_i$.\relax }}{3}}
\newlabel{fig:distributed}{{3}{3}}
\newlabel{sec:problem}{{II}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Problem Formulation }{3}}
\citation{lehmann2006theory}
\citation{Samford1953}
\citation{hammersley1950estimating}
\newlabel{eq:error_def}{{1}{4}}
\newlabel{sec:preliminary}{{III}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Preliminary Results }{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The function $\eta (x) = f^2(x) / F(x)F(-x)$ (blue) for $f(x) = \phi (x)$ the standard normal density (red).  \relax }}{4}}
\newlabel{fig:eta}{{4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Consistent Estimation}{4}}
\newlabel{eq:estimator_naive}{{2}{4}}
\newlabel{eq:eta_def}{{3}{4}}
\newlabel{prop:eta}{{1}{4}}
\citation{berger1996ceo}
\citation{chen2004upper}
\citation{KipnisRini2017}
\newlabel{sec:ceo}{{\unhbox \voidb@x \hbox {III-B}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Lower bound from the CEO }{5}}
\newlabel{prop:ceo_lower_bound}{{2}{5}}
\newlabel{eq:ceo_bound}{{4}{5}}
\newlabel{eq:ceo_optimal_sumrate}{{5}{5}}
\newlabel{eq:ceo_optimal_sumrate}{{6}{5}}
\newlabel{sec:sequential}{{IV}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Adaptive Estimation }{5}}
\citation{tsybakov2008introduction}
\citation{gill1995applications}
\citation{polyak1992acceleration}
\citation{polyak1990new}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Maximal efficiency of adaptive one-bit schemes}{6}}
\newlabel{thm:adpative_lower_bound}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Asymptotically optimal estimator}{6}}
\newlabel{eq:sgd_alg}{{7}{6}}
\newlabel{eq:sgd_est}{{8}{6}}
\newlabel{thm:sgd}{{4}{6}}
\newlabel{eq:conditions1}{{9}{6}}
\newlabel{eq:conditions2}{{10}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}One-step optimal estimation}{6}}
\citation{1619423}
\newlabel{thm:opt_one_step}{{5}{7}}
\newlabel{eq:adaptive_main_message}{{11}{7}}
\newlabel{eq:fixed_point}{{12}{7}}
\newlabel{eq:opt_cond}{{13}{7}}
\newlabel{lem:unique}{{6}{7}}
\newlabel{eq:lem_fixed_point}{{14}{7}}
\newlabel{lem:adaptive}{{7}{7}}
\newlabel{eq:density_update}{{15}{7}}
\newlabel{eq:estimator_update}{{16}{7}}
\newlabel{eq:message_update}{{17}{7}}
\newlabel{sec:distributed}{{V}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Distributed Estimation from Threshold Detectors }{7}}
\citation{van2000asymptotic}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Normalized empirical risk $n\left (\mathaccentV {hat}05E{\theta }_n-\theta \right )^2$ versus number of samples $n$ for $500$ Monte Carlo trials. In each trial, $\theta $ is chosen uniformly in the interval $(-3,3)$.  \relax }}{8}}
\newlabel{fig:adaptive_error}{{5}{8}}
\newlabel{eq:threshold_message}{{18}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Local Asymptotic Minimax Estimation}{8}}
\newlabel{thm:LAN}{{8}{8}}
\citation{papadimitriou1998combinatorial}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Minimax Threshold Density}{9}}
\newlabel{eq:var_cvx_minimax}{{19}{9}}
\newlabel{eq:cvx_minimax}{{20}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The optimal density $\lambda ^\star $ (blue) that minimizes the maximum asymptotic ML risk for $f(x)$ a standard normal density $\Theta = [-b,b]$, and various values of $\sigma $. The continuous curve (red) represents the asymptotic risk for a fixed $\theta \in [-b,b]$ under the optimal density, so its maximal value is minimax risk. The dashed curve is the asymptotic risk a fixed $\theta \in [-b,b]$ under a uniform distribution over $[-b,b]$, so its maximal value is given by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 21\hbox {}\unskip \@@italiccorr )}}. \relax }}{9}}
\newlabel{fig:minimax_support}{{6}{9}}
\newlabel{eq:uniform_risk}{{21}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Asymptotic Bayes Risk}{9}}
\newlabel{eq:cvx_average}{{22}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Minimax ARE versus $\sigma /b$ for $P_X = \mathcal  N(\theta ,\sigma ^2)$. The dashed curve represents the minimal ARE under a uniform threshold density given by $\sigma ^2/K_{\mathsf  {unif}}$, where $K_{\mathsf  {unif}}$ is given by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 21\hbox {}\unskip \@@italiccorr )}}. \relax }}{10}}
\newlabel{fig:minimax_ARE}{{7}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  The threshold density $\lambda (dt)$ that minimizes the asymptotic Bayes risk \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 22\hbox {}\unskip \@@italiccorr )}} for a uniform prior with $\sigma /\sigma _\theta =1,2,3,4$, where $\sigma _\theta ^2=b^2/3$ is the variance of the prior. \relax }}{10}}
\newlabel{fig:opt_density}{{8}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Asymptotic Bayes risk $R_{\pi }^\star $ in estimating the mean of a normal distribution ($P_X = \mathcal  N(\theta , \sigma ^2)$under an optimal threshold distribution $\lambda ^\star $ for $\pi $ the uniform distribution over $\Theta = [-0.5,0.5]$. The distribution $\lambda ^\star $ is the minimizer of \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 22\hbox {}\unskip \@@italiccorr )}}. It is illustrated for various cases in Fig.\nobreakspace  {}8\hbox {}. The dashed curve represents the upper bound \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 23\hbox {}\unskip \@@italiccorr )}}.  \relax }}{10}}
\newlabel{fig:dist_bound_uniform}{{9}{10}}
\newlabel{prop:upper_bound}{{9}{10}}
\newlabel{eq:upper_bound}{{23}{10}}
\newlabel{eq:bound_Taylor}{{24}{10}}
\newlabel{eq:upper_bound_proof}{{25}{11}}
\newlabel{ex:bound}{{1}{11}}
\newlabel{sec:conclusions}{{VI}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusions }{11}}
\@writefile{toc}{\contentsline {section}{Appendix}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Proof of Proposition\nobreakspace  {}1\hbox {}}{11}}
\newlabel{eq:eta_proof}{{26}{11}}
\newlabel{lem:bound_intervals}{{10}{12}}
\newlabel{eq:bound_intervals}{{27}{12}}
\newlabel{lem:fisher_bound}{{11}{12}}
\newlabel{eq:gradient_zero}{{28}{12}}
\newlabel{eq:lem_fisher_bound_proof1}{{29}{12}}
\citation{van2004detection}
\citation{gill1995applications}
\citation{polyak1992acceleration}
\citation{polyak1990new}
\citation{polyak1992acceleration}
\citation{polyak1992acceleration}
\newlabel{eq:lemma_J}{{30}{13}}
\newlabel{eq:general_messages}{{31}{13}}
\newlabel{eq:adpt_lower_bound_proof:1}{{32}{13}}
\newlabel{eq:fisher_information}{{33}{13}}
\newlabel{thm:polyak_juditsky}{{12}{13}}
\newlabel{eq:Polyak_Juditsky_cond3}{{34}{13}}
\citation{polyak1990new}
\citation{1056489}
\newlabel{thm:polyak_new}{{13}{14}}
\newlabel{eq:polyak_new_measurements}{{35}{14}}
\citation{van2000asymptotic}
\newlabel{eq:one_step_proof_derivative}{{36}{15}}
\newlabel{eq:LAN_proof1}{{37}{15}}
\newlabel{lem:LAN1}{{14}{15}}
\newlabel{lem:LAN2}{{15}{15}}
\citation{sen1994large}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,/Users/Alon1/LaTex/bibtex/sampling}
\bibcite{53738}{1}
\bibcite{gray1998quantization}{2}
\bibcite{gill1995applications}{3}
\bibcite{cicalese2002least}{4}
\bibcite{Karp:2007:NBS:1283383.1283478}{5}
\bibcite{baraniuk2017exponential}{6}
\bibcite{904560}{7}
\bibcite{4244748}{8}
\bibcite{6882252}{9}
\bibcite{chen2010performance}{10}
\bibcite{5184907}{11}
\bibcite{52470}{12}
\bibcite{tsitsiklis1988decentralized}{13}
\bibcite{5751320}{14}
\bibcite{berger1996ceo}{15}
\bibcite{viswanathan1997quadratic}{16}
\bibcite{oohama1998rate}{17}
\bibcite{prabhakaran2004rate}{18}
\newlabel{eq:Lyaponov}{{38}{16}}
\@writefile{toc}{\contentsline {section}{References}{16}}
\bibcite{zhang2013information}{19}
\bibcite{han1987hypothesis}{20}
\bibcite{zhang1988estimation}{21}
\bibcite{singh2009limits}{22}
\bibcite{lehmann2006theory}{23}
\bibcite{Samford1953}{24}
\bibcite{hammersley1950estimating}{25}
\bibcite{chen2004upper}{26}
\bibcite{KipnisRini2017}{27}
\bibcite{tsybakov2008introduction}{28}
\bibcite{polyak1992acceleration}{29}
\bibcite{polyak1990new}{30}
\bibcite{1619423}{31}
\bibcite{van2000asymptotic}{32}
\bibcite{papadimitriou1998combinatorial}{33}
\bibcite{van2004detection}{34}
\bibcite{1056489}{35}
\bibcite{sen1994large}{36}
\bibcite{720540}{37}
\bibcite{csiszar1998method}{38}
\citation{han1987hypothesis}
\citation{zhang1988estimation}
\citation{720540}
\citation{han1987hypothesis}
\citation{zhang1988estimation}
\citation{720540}
\citation{720540}
\citation{csiszar1998method}
\citation{53738}
\citation{53738}
\citation{53738}
\citation{berger1996ceo}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Statistical Inference under Communication Constraints}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Sigma-Delta Encoding}{17}}
