\relax 
\citation{720540}
\citation{zhang2013information}
\citation{zhang1988estimation}
\citation{Shamir}
\citation{han1987hypothesis}
\citation{zhang1988estimation}
\citation{720540}
\citation{720540}
\citation{csiszar1998method}
\citation{53738}
\citation{53738}
\citation{berger1996ceo}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Background}{1}}
\newlabel{sec:Intro}{{I}{1}}
\citation{zhang2013information}
\citation{zhang2013information}
\citation{zhang2013information}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Centralized encoding using one bit per sample on average.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:centralized}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Adaptive single-bit encoding: the $i$th encoder delivers a single bit message which is a function of its private sample $X_i$ and the previous messages $M_1,\ldots  ,M_{i-1}$.\relax }}{2}}
\newlabel{fig:sequential}{{2}{2}}
\newlabel{sec:problem}{{II}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Problem Formulation }{2}}
\citation{berger1996ceo}
\citation{prabhakaran2004rate}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Distributed single-bit encoding: the single-bit message produced by each encoder is only a function of its private sample $X_i$.\relax }}{3}}
\newlabel{fig:distributed}{{3}{3}}
\newlabel{eq:error_def}{{1}{3}}
\newlabel{eq:relative_efficiency}{{2}{3}}
\newlabel{sec:ceo}{{III}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Relation to Remote Multiterminal Source Coding }{3}}
\newlabel{eq:Gaussian_channel}{{3}{3}}
\citation{chen2004upper}
\citation{KipnisRini2017}
\citation{tsitsiklis1988decentralized}
\citation{5751320}
\citation{904560}
\citation{4244748}
\citation{6882252}
\citation{chen2010performance}
\citation{5184907}
\newlabel{prop:ceo_lower_bound}{{1}{4}}
\newlabel{eq:ceo_bound}{{4}{4}}
\newlabel{eq:ceo_optimal_sumrate}{{5}{4}}
\newlabel{eq:ceo_optimal_sumrate}{{6}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Relation to Decentralized Detection}{4}}
\citation{tsybakov2008introduction}
\citation{gill1995applications}
\citation{polyak1992acceleration}
\newlabel{sec:sequential}{{V}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Adaptive Estimation }{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}A lower bound on adaptive one-bit schemes}{5}}
\newlabel{thm:adpative_lower_bound}{{2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Asymptotically optimal estimator}{5}}
\newlabel{eq:sgd_alg}{{7}{5}}
\newlabel{eq:sgd_est}{{8}{5}}
\newlabel{thm:sgd}{{3}{5}}
\citation{1619423}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}One-step optimal estimation}{6}}
\newlabel{thm:opt_one_step}{{4}{6}}
\newlabel{eq:adaptive_main_message}{{9}{6}}
\newlabel{eq:fixed_point}{{10}{6}}
\newlabel{eq:opt_cond}{{11}{6}}
\newlabel{lem:unique}{{5}{6}}
\newlabel{eq:lem_fixed_point}{{12}{6}}
\newlabel{lem:adaptive}{{6}{6}}
\newlabel{eq:density_update}{{14}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Normalized empirical risk $n\left (\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \theta $}\mathaccent "0362{\theta }_n-\theta \right )^2$ versus number of samples $n$ for $500$ Monte Carlo trials. In each trial, $\theta $ is chosen uniformly in the interval $(-3,3)$.  \relax }}{7}}
\newlabel{fig:adaptive_error}{{4}{7}}
\newlabel{eq:estimator_update}{{15}{7}}
\newlabel{eq:message_update}{{16}{7}}
\newlabel{sec:distributed}{{VI}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Distributed Estimation }{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-A}}Threshold Detection}{7}}
\citation{van2000asymptotic}
\citation{van2000asymptotic}
\newlabel{thm:LAN}{{7}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-B}}Worst case $\theta $}{8}}
\newlabel{eq:variational}{{17}{8}}
\newlabel{eq:variational2}{{18}{8}}
\citation{papadimitriou1998combinatorial}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Support of the optimal density $\lambda ^\star $ that minimizes the asymptotic ML risk for the worst choice of $\theta \in (-\sigma b, \sigma b)$ where the parameter space is the interval $(-b,b)$, for $b=1,2,5,10,50,100$. The continuous curve represents the inverse of the asymptotic risk under the optimal density and the dashed curves corresponds the inverse of the asymptotic risk under a uniform distribution over $(-a,a)$ with $a = b+2$. \relax }}{9}}
\newlabel{fig:minimax_support}{{5}{9}}
\newlabel{eq:var_cvx_minimax}{{19}{9}}
\newlabel{eq:cvx_minimax}{{20}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-C}}Uniform Threshold Distribution}{9}}
\newlabel{eq:uniform_risk}{{22}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Asymptotic risk of the ML estimator under an optimal choice of the threshold density $\lambda $ versus the support of the parameter space $\Theta = (-b,b)$. \relax }}{10}}
\newlabel{fig:minimax_optval}{{6}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  The optimal asymptotic threshold density $\lambda $ that minimizes the expected asymptotic ML risk \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 23\hbox {}\unskip \@@italiccorr )}} for a Gaussian prior for $\sigma /\sigma _\theta =0.25,1,2$ (left to right), where $\sigma _\theta ^2$ is the variance of the prior. \relax }}{10}}
\newlabel{fig:opt_density}{{7}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-D}}Asymptotic Bayes Risk}{10}}
\newlabel{eq:cvx_average}{{23}{10}}
\newlabel{prop:upper_bound}{{8}{10}}
\newlabel{eq:upper_bound}{{24}{10}}
\newlabel{eq:upper_bound_proof}{{25}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Expected asymptotic risk $\mathbb  E (1/K(\theta ))$ in estimation from threshold detectors versus $\sigma /\sigma _\theta $ for a Gaussian prior with variance $\sigma _\theta ^2$. The optimal asymptotic threshold density $\lambda $ (red) is obtained by solving \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 23\hbox {}\unskip \@@italiccorr )}}. The bound (blue) is obtained using \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 24\hbox {}\unskip \@@italiccorr )}}.  \relax }}{11}}
\newlabel{fig:dist_bound_Gaussian}{{8}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Expected asymptotic risk $\mathbb  E (1/K(\theta ))$ in estimation from threshold detectors versus $\sigma /\sigma _\theta $ for a uniform prior with variance $\sigma _\theta ^2$. The optimal asymptotic threshold density $\lambda $ (red) is obtained by solving \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 23\hbox {}\unskip \@@italiccorr )}}. The bound (blue) is obtained using \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 24\hbox {}\unskip \@@italiccorr )}}.  \relax }}{11}}
\newlabel{fig:dist_bound_uniform}{{9}{11}}
\newlabel{prop:asymp}{{9}{11}}
\newlabel{eq:asymp}{{26}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-E}}Discussion}{12}}
\newlabel{sec:conclusions}{{VII}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Conclusions }{12}}
\@writefile{toc}{\contentsline {section}{Appendix}{12}}
\newlabel{lem:bound_intervals}{{10}{12}}
\newlabel{eq:bound_intervals}{{27}{12}}
\newlabel{lem:fisher_bound}{{11}{12}}
\newlabel{eq:induction_base}{{28}{13}}
\citation{van2004detection}
\citation{gill1995applications}
\newlabel{eq:lem_fisher_bound_proof1}{{29}{14}}
\newlabel{eq:lemma_J}{{30}{14}}
\newlabel{eq:general_messages}{{31}{14}}
\newlabel{eq:adpt_lower_bound_proof:1}{{32}{14}}
\citation{polyak1992acceleration}
\citation{polyak1992acceleration}
\citation{polyak1992acceleration}
\newlabel{eq:fisher_information}{{33}{15}}
\newlabel{thm:polyak_juditsky}{{12}{15}}
\citation{1056489}
\newlabel{eq:one_step_proof_derivative}{{34}{16}}
\citation{van2000asymptotic}
\newlabel{eq:LAN_proof1}{{35}{17}}
\newlabel{lem:LAN1}{{13}{17}}
\newlabel{lem:LAN2}{{14}{17}}
\citation{sen1994large}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,/Users/Alon1/LaTex/bibtex/sampling}
\bibcite{720540}{1}
\bibcite{zhang2013information}{2}
\bibcite{zhang1988estimation}{3}
\bibcite{han1987hypothesis}{4}
\bibcite{csiszar1998method}{5}
\bibcite{53738}{6}
\bibcite{berger1996ceo}{7}
\bibcite{prabhakaran2004rate}{8}
\bibcite{chen2004upper}{9}
\bibcite{KipnisRini2017}{10}
\bibcite{tsitsiklis1988decentralized}{11}
\bibcite{5751320}{12}
\bibcite{904560}{13}
\bibcite{4244748}{14}
\bibcite{6882252}{15}
\bibcite{chen2010performance}{16}
\bibcite{5184907}{17}
\bibcite{tsybakov2008introduction}{18}
\bibcite{gill1995applications}{19}
\bibcite{polyak1992acceleration}{20}
\bibcite{1619423}{21}
\newlabel{eq:Lyaponov}{{36}{18}}
\@writefile{toc}{\contentsline {section}{References}{18}}
\bibcite{van2000asymptotic}{22}
\bibcite{van2004detection}{23}
\bibcite{1056489}{24}
\bibcite{sen1994large}{25}
\bibcite{horstein1963sequential}{26}
\bibcite{1053879}{27}
\bibcite{DBLP:journals/corr/VarastehSG16}{28}
\citation{han1987hypothesis}
\citation{zhang1988estimation}
\citation{720540}
\citation{han1987hypothesis}
\citation{zhang1988estimation}
\citation{720540}
\citation{720540}
\citation{csiszar1998method}
\citation{53738}
\citation{53738}
\citation{53738}
\citation{berger1996ceo}
\citation{horstein1963sequential}
\citation{1053879}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Statistical Inference under Communication Constraints}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Sigma-Delta Encoding}{19}}
\newlabel{eq:channel}{{37}{19}}
\citation{DBLP:journals/corr/VarastehSG16}
\newlabel{eq:rdf_bound}{{38}{20}}
\newlabel{eq:drf_bound}{{39}{20}}
