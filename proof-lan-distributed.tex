% -*- Mode: latex -*- %

\section{Proof of Theorem~\ref{theorem:non-adaptive-minimax}}
\label{sec:proof-non-adaptive-minimax}

We follow a similar outline to the optimality results we establish
in the proof of Theorem~\ref{thm:sgd}\eqref{item:sgd-regular} in
Sec.~\ref{sec:proof-sgd-regular}.
Roughly, we establish that the family $P_\theta$ of distributions
on the bits $B_i$ is locally asymptotically normal
(Definition~\ref{definition:lan}) via a quadratic
mean differentiability argument. After this, the result
follows by standard local asymptotic minimax theory.

We begin with an argument on the smoothness properties of the densities,
which is important for our Taylor expansions to come.
\begin{lemma}
  \label{lemma:derivative-bounds}
  Let
  Assumption~\ref{assumption:detection-regions}\eqref{item:lipschitz-density}
%  and \eqref{item:finite-intervals} 
 hold. Then for any $A = \cup_{i = 1}^k
  \{[a_i, b_i]\}$ and $h \in \R$,
  \begin{equation*}
    \left|P_{\theta + h}(A) - P_\theta(A) - \dPtheta(A) h \right| \leq 
    k \cdot \lip(f) h^2,
  \end{equation*}
  where
  \begin{equation}
    \dPtheta(A) = \sum_{i = 1}^k f(a_i - \theta) - f(b_i - \theta).
    \label{eqn:expansion-dPtheta}
  \end{equation}
  Additionally, we have the bounds
  \begin{equation}
    \label{eqn:bound-density-diffs}
    |f(b) - f(a)| \le 2 \sqrt{\lip(f) P([a, b])}
    ~~ \mbox{and} ~~
    |\dPtheta(A)| \le 2 \sqrt{k \lip(f)}.
  \end{equation}
\end{lemma}
\noindent
See Section~\ref{sec:proof-derivative-bounds} for a proof.

The second lemma provides the local asymptotic normality we require.
\begin{lemma}
  \label{lemma:lan-bits}
  Let
  Assumption~\ref{assumption:detection-regions}\eqref{item:lipschitz-density}
  and~\eqref{item:finite-intervals} hold, and let $B_i = \indic{X_i \in
    A_i}$.  Let $h_n \to h \in \R$. Then for any $\theta \in
  \mbox{int}\Theta$,
  \begin{equation*}
    \sum_{i = 1}^n \log \frac{P_{\theta + h_n/\sqrt{n}}(B_i)}{
      P_\theta(B_i)}
    = \frac{h}{\sqrt{n}}
    \sum_{i = 1}^n \score_\theta(B_i)
    - \frac{h^2}{4n} \sum_{i = 1}^n \var(\score_\theta(B_i))
    - \frac{h^2}{4n} \sum_{i = 1}^n \score_\theta(B_i)^2
    + o_P(1).
  \end{equation*}
  If additionally
  Assumption~\ref{assumption:detection-regions}\eqref{item:limit-variance}
  holds, then
  \begin{equation*}
    \sum_{i = 1}^n \log \frac{P_{\theta + h_n/\sqrt{n}}(B_i)}{
      P_\theta(B_i)}
    = \frac{h}{\sqrt{n}}
    \sum_{i = 1}^n \score_\theta(B_i)
    - \frac{h^2}{2} \kappa(\theta) + o_P(1).
  \end{equation*}
\end{lemma}
\noindent
The proof of Lemma~\ref{lemma:lan-bits} is quite technical,
so we defer it to Section~\ref{sec:proof-lan-bits}.

With this lemma, it is not too challenging to demonstrate the local
asymptotic normality (Definition~\ref{definition:lan}) of the family
$\{P_\theta\}$. Indeed, Lemma~\ref{lemma:derivative-bounds} guarantees that
$|\dPtheta(A_n)| \le 2\sqrt{k_n \lip(f)}$ for all $n$, so that
$\E_\theta[|\score_\theta(B_i)|^3] \le C \frac{k_i^{3/2}
  \lip(f)^{3/2}}{P_\theta(A_i)^2 (1 - P_\theta(A_i))^2}$, while
Assumption~\ref{assumption:detection-regions}\eqref{item:finite-intervals}
guarantees that
$\frac{1}{n^3} \sum_{i = 1}^n \E_\theta[|\score_\theta(B_i)|^3]
\to 0$. Because $\E[\score_\theta(B_i)] = 0$,
the Lyapunov central limit theorem
applies to give
\begin{equation*}
  \frac{1}{\sqrt{n}} \sum_{i = 1}^n \score_\theta(B_i)
  \cd \normal\left(0, \kappa(\theta)\right)
\end{equation*}
under
Assumption~\ref{assumption:detection-regions}\eqref{item:limit-variance},
so that the family $\{P_\theta\}$ is locally asymptotically normal
(Def.~\ref{definition:lan}).

We now recall the familiar H\'{a}jek-Le-Cam local asymptotic minimax
result~\cite[Thm.~8.11]{VanDerVaart98}: if the family
$\{P_\theta\}$ is LAN with precision $\kappa(\theta)$, then
\begin{equation*}
  \liminf_{c \to \infty} \liminf_n \sup_{\norm{\tau - \theta} \le
    c / \sqrt{n}} \E_\tau\left[L(\sqrt{n}(\theta_n - \tau))\right]
  \ge \E[L(Z / \sqrt{\kappa(\theta)})]
\end{equation*}
for any symmetric quasi-convex loss $L$, where $Z \sim \normal(0, 1)$.
This immediately gives Theorem~\ref{theorem:non-adaptive-minimax}.

%% Returning to the expansion~\eqref{eqn:almost-at-the-end}, we thus obtain
%% %% \begin{equation*}
%% %%   \sum_{i = 1}^n \log \frac{p_n(B_i)}{p(B_i)}
%% %%   = \frac{h}{\sqrt{n}} \sum_{i = 1}^n \score_\theta(B_i)
%% %%   - \frac{1}{4} \sum_{i = 1}^n W_{n,i}^2 + o_P(1).


%% %% \begin{equation*}
%% %%   W_{n,i}
%% %%   = \frac{h_n}{\sqrt{n}} \score_\theta(B_i)
%% %%   \pm \frac{h_n^2 k \lip(f)}{n p(B_i)}
%% %%   \pm \frac{1}{2}
%% %% \end{equation*}

%% \begin{equation*}
%%   g(b) = h \score_\theta(b),
%% \end{equation*}

%% Under the assumption that $0 < \inf_i P_{\theta}(A_i) \le \sup_i P_{\theta}(A_i)
%% < 1$, the random 

\subsection{Proof of Lemma~\ref{lemma:derivative-bounds}}
\label{sec:proof-derivative-bounds}

To see the first claim of the lemma, we consider the simpler special case
that $A = [a, b]$. Then as $f$ is Lipschitz (and hence absolutely
continuous and a.e.\ differentiable with $\linf{f'} \le \lip(f)$),
we have
\begin{align*}
  P_{\theta + h}(A)
  - P_\theta(A)
  & = \int_a^b (f(z - \theta - h) - f(z - \theta)) dz \\
  & = -\int_a^b \int_0^h f'(z - \theta - u) du dz \\
  & = -\int_0^h \int_a^b f'(z - \theta - u) dz du \\
  & = \int_0^h f(a - \theta - u) - f(b - \theta - u) du \\
  & \lesseqgtr \int_0^h (f(a - \theta) - f(b - \theta)) du
  \pm 2 \int_0^h \lip(f) u du \\
  & = \left[f(a - \theta) - f(b - \theta)\right] h \pm \lip(f) h^2.
\end{align*}
This gives the first two claims of the lemma.

For the second, we require a bit more work.
Let $L = \lip(f)$ for shorthand. Let $a < b$.
Then we always have
\begin{equation}
  \label{eqn:interval-length-thing}
  P([a, b])
  \ge \int_a^b f(z) dz
  \ge \int_a^b \max\{f(b) - L (b - z), f(a) - L(z - a), 0\} dz.
\end{equation}
If $f(a) + f(b) \ge L (b - a)$, then the point $\hat{z} =
\frac{a + b}{2} - \frac{f(b) - f(a)}{2L}$ satisfies both $f(b) - L(b -
\hat{z}) \ge 0$ and $f(a) - L(\hat{z} - a) \ge 0$. The
integral~\eqref{eqn:interval-length-thing} then becomes
\begin{align*}
  \lefteqn{\int_a^{\hat{z}}
    (f(a) - L (z - a) dz)
    + \int_{\hat{z}}^b
    (f(b) - L (b - z)) dz} \\
  & = \frac{f(a) + f(b)}{2}
  \left(\frac{b - a}{2}\right)
  - L \left(\frac{b - a}{2} \right)^2
  + \frac{(f(b) - f(a))^2}{4L},
\end{align*}
and using the assumption that $\frac{f(a) + f(b)}{2} \ge L(b - a)$,
we obtain
\begin{align*}
  \frac{(f(b) - f(a))^2}{4L}
  & \le \frac{f(b) + f(b)}{2} \frac{b - a}{2}
  - L \left(\frac{b - a}{2}\right)^2
  + \frac{(f(b) - f(a))^2}{4L}
  \le P([a, b]).
\end{align*}
That is, $|f(b) - f(a)| \le 2 \sqrt{\lip(f) P([a, b])}$.
In the converse case that $f(a) + f(b) \le L(b - a)$, then
the integral~\eqref{eqn:interval-length-thing} becomes
\begin{align*}
  P([a, b])
  & \ge \int_a^{a + \frac{f(a)}{L}}
  (f(a) - L (z - a)) dz
  + \int_{b - \frac{f(b)}{L}}^b (f(b) - L (b - z)) dz \\
  & = \frac{f(a)^2}{L}
  - \frac{f(a)^2}{2L}
  + \frac{f(b)^2}{L}
  - \frac{f(b)^2}{2L},
\end{align*}
so that $\sqrt{f(a)^2 + f(b)^2} \le \sqrt{2 \lip(f) P([a, b])}$.  In sum,
we have demonstrated that always the first
bound~\eqref{eqn:bound-density-diffs} holds.
To show the second inequality in expression~\eqref{eqn:bound-density-diffs},
note that
$\sum_i P([a_i, b_i]) \le 1$, and apply Cauchy-Schwarz.

\subsection{Proof of Lemma~\ref{lemma:lan-bits}}
\label{sec:proof-lan-bits}

Our proof follows that of \cite[Thm.~7.2]{VanDerVaart98} closely. We first
demonstrate a type of uniform quadratic mean differentiability
(Definition~\ref{definition:qmd}) for sets $A$ that are finite unions of
intervals. By a Taylor approximation and concavity of
$\sqrt{\cdot}$, we have
\begin{equation*}
  \sqrt{a} + \frac{b}{2 \sqrt{a}} -
  \frac{b^2}{4\sqrt{a}}
  \le \sqrt{a + b} \le \sqrt{a}
  + \frac{b}{2 \sqrt{a}}
\end{equation*}
for any $a > 0$ and $|b| \le 3a/4$. Consequently,
recalling that $\score_\theta(A) = \dPtheta(A) / P_\theta(A)$,
for any $h \in \R$ and $A = \cup_{i = 1}^k \{[t_i^-, t_i^+]\}$ the
union of $k$ intervals, the expansion~\eqref{eqn:expansion-dPtheta} yields
\begin{equation*}
  \left(\sqrt{P_{\theta + h}(A)} -
  \sqrt{P_\theta(A)} - \half h \score_\theta(A) \sqrt{P_\theta(A)}
  \right)^2
  \le
  \left(
  \frac{k \lip(f)}{2 \sqrt{P_\theta(A)}} h^2
  + \frac{(|\dPtheta(A) h| + h^2 \lip(f))^2}{P_\theta(A)^{3/2}}
  \right)^2,
\end{equation*}
valid for $h$ such that
$|\dPtheta(A) h| \le P_\theta(A) / 4$ and
$k h^2 \lip^2(f) \le P_\theta(A) / 4$.
Thus, under
Assumption~\ref{assumption:detection-regions}\eqref{item:finite-intervals},
there exists a numerical constant $C < \infty$ such that
\begin{subequations}
  \label{eqn:h-fourth}
  \begin{align}
    \nonumber \left(\sqrt{P_{\theta + h}(A)} -
    \sqrt{P_\theta(A)} - \half
    h \score_\theta(A) \sqrt{P_\theta(A)}\right)^2
    & \le
    \left(\frac{h^2 k \cdot \lip(f)}{2 \sqrt{P_\theta(A)}}
    + \frac{(|\dPtheta(A) h| + k h^2 \lip(f))^2}{
      P_\theta(A)^{3/2}}\right)^2 \\
    & \le \frac{C}{P_\theta(A)} \left[k^2 \lip^2(f)
      + \score_\theta(A)^2
      + \frac{k^4 h^4 \lip^4(f)}{P_\theta(A)^2}
      \right] \cdot h^4,
  \end{align}
  valid whenever $|\dPtheta(A) h|
  \le P_\theta(A) / 4$ and $k h^2 \lip^2(f) \le P_\theta(A) / 4$,
  and similarly, we have
  \begin{equation}
    \left(\sqrt{P_{\theta + h}(A^c)} -
    \sqrt{P_\theta(A^c)} - \half
    h \score_\theta(A^c) \sqrt{P_\theta(A^c)}\right)^2
    \le \frac{C}{P_\theta(A^c)}
    \left[k^2 \lip^2(f)
      + \score_\theta(A^c)^2
      + \frac{k^4 h^4 \lip^4(f)}{P_\theta(A^c)^2}
      \right] \cdot h^4.
  \end{equation}
\end{subequations}
That is, the family $\{P_\theta\}$ with bit observations $B_n$ satisfies a
uniform type of quadratic-mean differentiability
(Def.~\ref{definition:qmd}).

For shorthand, define $P_n = P_{\theta + h_n / \sqrt{n}}$ and $P =
P_\theta$, and let $p_n, p$ be shorthand for the p.m.f.s of the two
distributions.  For the sets $A_i$ we recall that $B_i = \indic{X_i \in
  A_i}$.  The random variables
\begin{equation*}
  W_{n,i} \defeq 2 \left(\sqrt{\frac{p_n}{p}}(B_i) - 1\right)
\end{equation*}
are with $P$-probability 1 well-defined, and by the
inequalities~\eqref{eqn:h-fourth}, we have
that
\begin{align}
  \lefteqn{\var\left(W_{n,i} - \frac{h_n}{\sqrt{n}} \score_\theta(B_i)\right)
    \le 
    C \frac{k_i^2 \lip^2(f) + \score_\theta(A_i)^2
      + \score_\theta(A_i^c)^2}{P_\theta(A_i) P_\theta(A_i^c)}
    \cdot \frac{h_n^4}{n^2}
    + C \frac{k^4 \lip^4(f)}{
      P_\theta(A_i)^3 P_\theta(A_i^c)^3}
    \frac{h_n^8}{n^4}} \nonumber \\
  & \qquad\qquad\qquad\qquad\qquad \le 
  C \frac{k_i^2 \lip^2(f) + \score_\theta(A_i)^2
  + \score_\theta(A_i^c)^2}{P_\theta(A_i) P_\theta(A_i^c)}
  \cdot \frac{h_n^4}{n^2}
  + C \frac{k^4 \lip^4(f)}{
    P_\theta(A_i)^3 P_\theta(A_i^c)^3}
  \frac{h_n^8}{n^4}
  \label{eqn:var-wni-expansion}
\end{align}
whenever
\begin{equation*}
  \frac{h}{\sqrt{n}} \max\{\score_\theta(A_i),
  \score_\theta(A_i^c)\}
  \le \frac{1}{4}
  ~~ \mbox{and} ~~
  \frac{k_i h_n^2}{n} \lip^2(f)
  \le \frac{\min\{P_\theta(A_i), P_\theta(A_i^c)\}}{4}
\end{equation*}
Now, we use
Assumption~\ref{assumption:detection-regions}\eqref{item:finite-intervals},
coupled with Lemma~\ref{lemma:derivative-bounds} to show that the summed
variances converge to zero.  Indeed, Lemma~\ref{lemma:derivative-bounds} and
inequality~\eqref{eqn:var-wni-expansion} give that
\begin{equation*}
  \var\left(W_{n,i} - \frac{h_n}{\sqrt{n}} \score_\theta(B_i)\right)
  \le C \cdot
  \left[\frac{k_i^2}{P_\theta(A_i) P_\theta(A_i^c)}
    \frac{1}{n}
    + \frac{k_i}{P_\theta(A_i) P_\theta(A_i^c)}
    \frac{1}{n}
    + \frac{k_i^4}{P_\theta(A_i)^3 P_\theta(A_i^c)^3}
    \frac{1}{n^3}\right] \frac{1}{n},
\end{equation*}
where $C < \infty$ depends only on $\lip(f)$ and $h_n$ (both of which are
uniformly bounded) whenever
\begin{equation*}
  \frac{k_i}{P_\theta(A_i) P_\theta(A_i^c)} \frac{1}{n} \le
  \frac{1}{C}.
\end{equation*}
Assumption~\ref{assumption:detection-regions}\eqref{item:finite-intervals}
thus implies that $\E[\score_\theta(B_i)] = 0$ and
\begin{equation}
  \label{eqn:summed-variances-to-zero}
  \var\left(\sum_{i = 1}^n W_{n,i} - \frac{h_n}{\sqrt{n}} \score_\theta(B_i)
  \right)
  = \sum_{i = 1}^n \var\left(W_{n,i} - \frac{h_n}{\sqrt{n}} \score_\theta(B_i)
  \right)
  \to 0.
\end{equation}

We now control the expectation of the $W_{n,i}$. Defining $\mu_i$ to be the
induced counting measure on $B_i = \indic{X_i \in A_i}$,
\begin{align*}
  \sum_{i = 1}^n \E[W_{n,i}]
  & = 2\sum_{i = 1}^n
  \left(\int \sqrt{p_n(b)} \sqrt{p(b)} d\mu_i(b) - 1 \right)
  = -\sum_{i = 1}^n \int \left(\sqrt{p_n(b)} - \sqrt{p(b)}\right)^2
  d\mu_i(b) \\
  & = -\frac{h_n^2}{4 n} \sum_{i = 1}^n \E[\score_\theta(B_i)^2]
  - \sum_{i = 1}^n \int \left(\sqrt{p_n(b)} - \sqrt{p(b)}
  - \frac{h_n}{\sqrt{n}} \score_\theta(b) \sqrt{p(b)}\right)^2 d\mu_i(b) \\
  & \qquad \qquad ~
  - \sum_{i = 1}^n \int \left(\sqrt{p_n(b)} - \sqrt{p(b)}
  - \frac{h_n}{\sqrt{n}} \score_\theta(b) \sqrt{p(b)}\right)
  \frac{h_n}{\sqrt{n}} \score_\theta(b) \sqrt{p(b)}d\mu_i(b) \\
  & = -\bigg(\frac{h^2}{4n} \sum_{i = 1}^n \E[\score_\theta(B_i)^2]\bigg)
  - o(1)
\end{align*}
uniformly in $h$, with a derivation completely paralleling that above.
Therefore, we obtain
\begin{equation*}
  \sum_{i = 1}^n W_{n,i}
  = \sum_{i = 1}^n \left(W_{n,i} - \frac{h_n}{\sqrt{n}} \score_\theta(B_i)\right)
  + \frac{h_n}{\sqrt{n}} \sum_{i = 1}^n \score_\theta(B_i)
  = -\frac{h^2}{4n} \sum_{i = 1}^n \E[\score_\theta(B_i)^2]
  + \frac{h}{\sqrt{n}} \sum_{i = 1}^n \score_\theta(B_i)
  + o_P(1),
\end{equation*}
where we have used that $h_n \to h$.

Now, we write the log-likelihood ratio. We have
\begin{align*}
  \sum_{i = 1}^n \log \frac{p_n(B_i)}{p(B_i)}
  & = 2 \sum_{i = 1}^n \log\left(1 + \half W_{n,i}\right) \\
  & = \sum_{i = 1}^n W_{n,i}
  - \frac{1}{4} \sum_{i = 1}^n W_{n,i}^2
  + \half \sum_{i = 1}^n W_{n,i}^2 R(W_{n,i})
\end{align*}
where the remainder $|R(W_{n,i})| \le |W_{n,i}|$ for $|W_{n,i}| \le 1$.
Using the Taylor expansions of $\sqrt{\cdot}$ and
Lemma~\ref{lemma:derivative-bounds}, we have
\begin{align}
  \nonumber
  \left| \half W_{n,i} \right|
  & \leq \half \left| \score_\theta(B_i) \right|
  \frac{h_n}{\sqrt{n}}
  + \left|\frac{h_n^2}{n} \frac{k_i \lip(f)}{p(B_i)}
  + \frac{h_n^2}{n} \score_\theta(B_i)^2
  + \frac{ h_n^4}{n^2} \frac{k_i^2 \lip(f)^2}{p(B_i)^2}\right| \\
  & = \half \score_\theta(B_i)
  \frac{h_n}{\sqrt{n}} +
  C \left|\frac{\sqrt{k_i}}{\sqrt{n} p(B_i)}
  + \frac{k_i}{n p(B_i)}
  + \frac{\sqrt{k_i}}{p(B_i)^2 n}
  + \frac{k_i^2}{p(B_i)^2 n^2}
  \right|
  \label{eqn:max-wni-zero}
\end{align}
where $|C| < \infty$ depends only on $\lip(f)$ and $h_n$ and so is
uniformly bounded. From Assumption~\ref{assumption:detection-regions}\eqref{item:finite-intervals} we get
\[
C \left|\frac{\sqrt{k_i}}{\sqrt{n} p(B_i)}
  + \frac{k_i}{n p(B_i)}
  + \frac{\sqrt{k_i}}{p(B_i)^2 n}
  + \frac{k_i^2}{p(B_i)^2 n^2}
  \right| \to 0. 
\]
Consequently $\max_i W_{n,i} \to 0$, so that
\begin{equation}
  \label{eqn:almost-at-the-end}
  \sum_{i = 1}^n \log \frac{p_n(B_i)}{p(B_i)}
  = \frac{h_n}{\sqrt{n}} \sum_{i = 1}^n \score_\theta(B_i)
  - \frac{1}{4} \sum_{i = 1}^n \E[\score_\theta(B_i)^2]
  - \frac{1}{4} \sum_{i = 1}^n W_{n,i}^2 + o_P(1).
\end{equation}

It remains to compute $\E[W_{n,i}^2]$. Using the bounds
that $|\score_\theta(B_i)| \le C \sqrt{k_i} / p(B_i)$ from
Lemma~\ref{lemma:derivative-bounds},
the
expansion~\eqref{eqn:max-wni-zero}
yields
\begin{align*}
  \lefteqn{\left|\E\left[W_{n,i}^2 - \frac{h_n^2}{2n} \score_\theta(B_i)^2\right]
    \right|} \\
  &
  \le \frac{C}{n} \left[
    \frac{k_i^{3/2}}{p(A_i)(1 - p(A_i)) \sqrt{n}}
    + \frac{k_i^{3/2}}{p(A_i)^2 (1 - p(A_i))^2 \sqrt{n}}
    + \frac{k_i^2}{p(A_i)(1 - p(A_i))} \frac{1}{n^{3/2}}
    \right] \\
  & \qquad ~ +
  \frac{C}{n}
  \left[\frac{k_i^2}{p(A_i)(1 - p(A_i))} \frac{1}{n}
    + \frac{k_i}{p(A_i)^3(1 - p(A_i))^3} \frac{1}{n}
    + \frac{k_i^4}{p(A_i)^3 (1 - p(A_i))^3} \frac{1}{n^3} \right],
\end{align*}
where $C$ depends only on $h$ and $\lip(f)$.
Thus
\begin{equation*}
  \sum_{i = 1}^n W_{n,i}^2
  = \frac{h_n^2}{n} \sum_{i = 1}^n \score_\theta(B_i)^2
  + o(1),
\end{equation*}
giving Lemma~\ref{lemma:lan-bits}.

