\section{Distributed Estimation \label{sec:distributed}}

We now consider the distributed encoding setting in
Figure~\ref{fig:setup}-(iii) where each one-bit message $B_i$ is a
function only of its private sample $X_i$. In this case, the $i$th encoder is
of the form $B_i = \indic{X_i \in A_i}$, where the detection region
$A_i$ is a Borel set independent of $X_1, X_2, \ldots$.

\subsection{Optimal Efficiency}
We begin by making a few restrictions on the collections of the sets $A_i$,
which we believe not unreasonable, but which allow us to develop fundamental
limits for estimation. We require a bit of notation to define the
assumptions. As we work with a location family based on a density $f$
with associated probability distribution $P$ on variables $Z$,
we define
\begin{equation*}
  P_\theta(A) \defeq P(Z - \theta \in A)
\end{equation*}
for $Z$ with density $f$. Whenever $A$ is a collection of disjoint intervals $A = \cup_i [t_i^-, t_i^+]$, we may define
\begin{equation*}
  \dPtheta(A) \defeq \frac{\partial}{\partial \theta} P_\theta(A)
  = \sum_i \left(f(t_i^- - \theta) - f(t_i^+ - \theta)\right),
\end{equation*}
and similarly we define the score
function $\score_\theta(A) \defeq \dPtheta(A) / P_\theta(A)$.
For $B = \indic{X \in A}$, we abuse notation and also write
$\score_\theta(B) = \score_\theta(A)$ and similarly for $\dPtheta$.
With this, we may define the variance of the
scores $\score_\theta(B_i)$ under $P_\theta$ via
\begin{equation}
  \label{eq:precision_general}
  L_n(A_1,\ldots,A_n;\theta) \defeq
  \frac{1}{n} \sum_{i=1}^n \frac{ \dPtheta(A_i)^2}{
    P_\theta(A_i) (1 - P_\theta(A_i))}.
\end{equation}
We then make the following assumption.
\begin{assumption}
  \label{assumption:detection-regions}
  The density and detection regions satisfy
  \begin{enumerate}[(i)]
  \item \label{item:lipschitz-density}
    The density function $f$ of $X_n - \theta$ is Lipschitz continuous.
  \item \label{item:finite-intervals}
    Each set $A_i$ is the finite union of $k_i$ disjoint intervals
    (which may include $\pm \infty$), where
    \begin{equation*}
      \frac{1}{n} \cdot \max_{i \le n} \frac{k_i^3}{P_\theta(A_i)^4
        (1 - P_\theta(A_i))^4} \to 0.
    \end{equation*}
  \item \label{item:limit-variance}
    The limit
    \begin{equation}
      \label{eqn:LAN-limit}
      \kappa(\theta) \defeq \lim_{n\to \infty} L_n(A_1,\ldots,A_n; \theta)
    \end{equation}
    exists and is finite.
  \end{enumerate}
\end{assumption}
Roughly speaking, \eqref{item:finite-intervals} above holds whenever the intervals consisting each $A_i$ are appropriately seperated. For example, it applies when each set $A_i$ is a half-bounded interval $(t_i,\infty)$ with $\min\{P_\theta((t_i,\infty)), P_\theta((-\infty,t_i])\} = \omega(1/n)$ as we dicscuss in more detail below. More generally, suppose that $A_i = \cup_{j=1}^{k_i}[t_{i,j}^-, t_{i,j}^+]$ and let 
\begin{align*}
c_n^+ & =  \min_{i \le n} \min_{j \leq k_i}  F(t^+_{i,j} - \theta) - F(t^-_{i,j}- \theta) \\
c_n^- & = \min_{i \le n} \min_{j \leq k_i}  F(t^-_{i,j+1}- \theta)  - F(t^+_{i,j}- \theta),
\end{align*}
with $t^-_{i,k_i+1} = \infty$. Then \ref{assumption:detection-regions}\eqref{item:finite-intervals} holds whenever $ (c_n^- c_n^+)^{-4} \max_{i \le n} k_i^3 = o(n)$. 

Under Assumption~\ref{assumption:detection-regions}, we have the following theorem, which provides
a local asymptotic minimax lower bound on the efficiency of
\emph{any} non-adaptive estimator.
\begin{theorem}
  \label{theorem:non-adaptive-minimax}
  Let Assumption~\ref{assumption:detection-regions} hold, and
  let ${\theta}_n$ be an estimator of $\theta \in \Theta$ from
  observations $B_i = \indic{X_i \in A_i}$.
  Then for $Z \sim \normal(0, 1)$ and any
  symmetric and quasi-convex function $L$,
  \begin{align*}
    & \liminf_{c \to \infty} \liminf_{n \to \infty}
    \sup_{\tau\,:\,|\theta-\tau| \leq \frac{c}{\sqrt{n} }}
    \E \left[ L\left( \sqrt{n}({\theta}_{n} - \tau) \right) \right] \\
    & \qquad \qquad \qquad \qquad \geq
    \E\left[ L (Z/\sqrt{\kappa(\theta)}) \right].
  \end{align*}
\end{theorem}
\noindent
See Appendix~\ref{sec:proof-non-adaptive-minimax} for a proof.

Theorem~\ref{theorem:non-adaptive-minimax} shows that the limiting variance
term $\kappa(\theta)$ provides a strong lower bound on the efficiency of any
non-adaptive estimator, and moreover, that this bound necessarily depends
on $\theta$. As a
particular consequence, for the squared error $L(x) = x^2$, for any $\delta
> 0$ and $\theta$, there exists a $c < \infty$ such that $\sup_{|\tau -
  \theta| \le c / \sqrt{n}} \E_\tau[(\theta_n - \tau)^2] \ge \frac{(1 -
  \delta)}{n \kappa(\theta)} + o(1/n)$. Consequently, attaining any type of good (uniform)
efficiency with non-adaptive estimators will be challenging. 

Yet, Theorem~\ref{theorem:non-adaptive-minimax}
limits non-adaptive strategies in stronger ways.
%%
%% \subsection{Non-existence of a Uniformly Optimal Strategy}
%%
Under the density models we have considered, with the additional
Assumption~\ref{assump:failure_rate}, we can show stronger
optimality results that adaptivity is essential for achieving
optimal convergence guarantees.
Recall the transformation~\eqref{eq:eta_def} of the
hazard rate function, $\eta(x) = \frac{f^2(x)}{F(x)(1 - F(x))}$, which
has unique maximum at $x = 0$ under Assumption~\ref{assump:failure_rate}.
When each detection region $A_n$ consists of a bounded number
of intervals, the next theorem shows that
the minimal risk $1/\eta(0)$ can
only be attained at finitely many points within $\Theta$.
In particular, distinct from the adaptive setting, no distributed
estimation scheme can achieve asymptotic variance $\eta(0)$
uniformly in $\theta \in \Theta$.

\begin{thm} \label{thm:non_existence}
  Let Assumptions~\ref{assump:failure_rate}
  and~\ref{assumption:detection-regions} hold.
  Additionally, assume that $A_i$ is the union of at most $K$
  intervals. The number of points $\theta \in \Theta$ satisfying
  $\kappa(\theta) = \eta(0)$ is at most $2K$.
\end{thm}
\noindent
See Appendix~\ref{proof:thm:non_existence} for a proof.


%In the adaptive setting, it follows from Theorem~\ref{thm:sgd} that messages of this kind with appropriately chosen thresholds lead to an estimator with the optimal ARE of $\eta(0)sigma^2$. However, Theorem~\ref{thm:non_existence} implies that such ARE can only be attained for a negligible subset of the parameter space. As we shall see next, messages obtained via comparison against a sequence of thresholds cannot attain the optimal efficiency one more than a single point of $\Theta$. 

\subsection{Threshold Detection}
\label{subsec:threshold}

We now consider a restricted case where each detection region is a
half-open interval, i.e., the $i$th message is obtained by comparing $X_i$
against a single threshold. Under the adaptive signal acquisition setting,
this is sufficient for asymptotic optimality;
in non-adaptive settings, it is not sufficient, though we may characterize
a few additional optimality results.
%% As we explain next, the existence of a density
%% for the sequence of thresholds is enough to establish local asymptotic
%% normality and leads to a closed-form expression for the precision parameter
%% and the ARE.
Assume now that each $B_i$ is of the form
\begin{equation}
  \label{eq:threshold_message}
  B_i = \sgn(t_i - X_i) = \begin{cases} 1 & X_i<  t_i, \\
    -1 & X_i \geq t_i,
  \end{cases}  
\end{equation}
where $t_i\in\mathbb R$ is the \emph{threshold} of the $i$th encoder. In
other words, the detection region of $B_i$ is $A_i = (t_i,\infty)$ and
$\mathbb P(X_i \in A_i) = F \left( B_i(t_i-\theta) \right)$. It follows that
\begin{align}
  L_n(A_1,\ldots,A_n;\theta)
  & = \frac{1}{n} \sum_{i=1}^n \frac{ \left(f(t_i-\theta) \right)^2 }{F\left(t_i-\theta \right) F\left(\theta - t_i \right) } \\
  & = \frac{1}{n} \sum_{i=1}^n \eta(t_i - \theta).
  \label{eq:Ln_threshold}
\end{align}
A natural condition for the existence of the limit \eqref{eq:Ln_threshold}
as $n\to \infty$ is that the empirical distribution of the threshold values
converges to a probability measure. Specifically, for an interval $I \subset
\mathbb R$, define
\begin{equation*}
  \lambda_n(I) = \frac{ \card \left( I \cap \{t_1,t_2,\ldots \} \right)}{n}. 
\end{equation*}
Then an investigation of the proof of
Theorem~\ref{theorem:non-adaptive-minimax} in
Section~\ref{sec:proof-non-adaptive-minimax}, specifically
Sec.~\ref{sec:proof-lan-bits} and the bounds~\eqref{eqn:h-fourth}, show that
as $\eta(t) \le \eta(0)$ for all $t \in \R$ under
Assumption~\ref{assump:failure_rate}, the following corollary follows. (The
corollary relies on local asymptotic normality~\cite[Ch.~7]{VanDerVaart98};
see Appendix~\ref{sec:proof-sgd-regular} for some brief discussion of such
conditions.)
\begin{cor} \label{cor:LAN_thresh}
  Let $\{t_n\}_{n=1}^\infty$ be a sequence of threshold values such that
  $\lambda_n$ converges (weakly) to a probability measure $\lambda$ on
  $\mathbb R$. Then the conclusions of
  Theorem~\ref{theorem:non-adaptive-minimax} apply with
  %% $\left\{ B_i = \sgn(X_i - t_i) \right\}_{i=1}^n$ is a
  %% LAN family with precision parameter
  \begin{equation*}
    \kappa(\theta) = \int_{\mathbb R} \eta(t-\theta) \lambda(dt). 
  \end{equation*}
  Moreover, the family of laws of $\{B_i = \sgn(X_i - t_i)\}_{i = 1}^n$
  under $\{P_\theta\}_{\theta \in \Theta}$ is locally asymptotically normal
  with information $\kappa(\theta)$.
\end{cor}

The condition that $\lambda_n$ converges to a probability measure is
satisfied, for example, whenever $t_1,\ldots,t_n$ are drawn
independently from a probability distribution $\lambda(dt)$ on $\mathbb
R$.

When the conclusions of Corollary~\ref{cor:LAN_thresh} hold, local asymptotic normality of $\{B_n\}_{n=1}^\infty$ implies that the maximum
likelihood estimator (ML) of $\theta$ from $B_1,\ldots,B_n$, denoted here by
${\theta}^{ML}_n$, is local asymptotic minimax in the sense that
\begin{equation*}
  \sqrt{n} \left( {\theta}^{ML}_n - \theta \right)
  \cd \normal\left(0, 1/\kappa(\theta) \right). 
\end{equation*}
We note that ${\theta}^{ML}_n$ solves
\begin{equation}
  \label{eq:ML}
  0 = \sum_{i=1}^n B_i \frac{f \left( t_i-\theta\right) }{F \left(B_i  (t_i-\theta)\right) }.
\end{equation}
If the collection $\{t_1,t_2\ldots\}$ is bounded
(for example $\{t_1,t_2\ldots\} \subset \Theta$), then
\begin{equation*}
\lim_{n\to \infty} n \cdot \ex{\left({\theta}^{ML}_n - \theta \right)^2}  = 1/\kappa(\theta), 
\end{equation*} 
so that the ML estimator attains the local asymptotic MSE of Theorem~\ref{theorem:non-adaptive-minimax}.

By Assumption~\ref{assump:failure_rate},
$\eta(x)$ attains its maximum at the origin, so we conclude that
\begin{equation*}
  \kappa(\theta) \leq \sup_{t\in \mathbb R} \eta \left( t-\theta\right) = \eta(0).
\end{equation*}
Moreover, this upper bound on $\kappa(\theta)$ is attained only when
$\lambda$ is the point mass at $\theta$. Since $\theta$ is \emph{a priori}
unknown, estimation in the distributed setting using
threshold detection is strictly suboptimal compared to the adaptive
setting; the ability to choose the thresholds $t_i$
adaptively conditional on previous messages is necessary for optimal
efficiency.

%We conclude that when the density of the threshold values converges to a probability measure, the ARE of the ML is $\kappa(\theta)$, and this ARE is maximal with respect to all local alternative estimators for $\theta$. 


\subsection{Minimax Threshold Density}

We conclude this section by considering the \newtext{distribution} of the threshold values
that maximizes the worst-case information $\inf_\theta \kappa(\theta)
= \kappa_\lambda(\theta)$ where $\kappa_\lambda(\theta)
= \int \eta(t - \theta) \lambda(dt)$.
The \newtext{optimal distribution $\lambda^\star$} solves the
optimization problem
\begin{align}
  \label{eq:var_cvx_minimax}
  \begin{split}
    \mathrm{maximize} \quad &  \inf_{\theta \in \Theta} \int \eta(t-\theta) \lambda(dt)
    \\ 
    \mathrm{subject~to} 
    \quad & \lambda(dt)\geq 0,\quad \int \lambda(dt) \leq 1. 
  \end{split}
\end{align}
The objective function~\eqref{eq:var_cvx_minimax} is concave in
$\lambda(dt)$ and continuous in the weak topology over measures on $\Theta$,
so that by discretizing, we can approximately solve this problem using
convex optimization. We let $\kappa^\star$ denote the maximal value of
problem~\eqref{eq:var_cvx_minimax} and $\lambda^\star(dt)$ be the density
achieving the maximum. By drawing thresholds
$t_i \simiid \lambda^\star$,
Corollary~\ref{cor:LAN_thresh} guarantees that for any $\theta \in \Theta$, the
maximum likelihood estimator
using $\{B_i = \sgn(X_i - t_i)\}_{i \in \N}$ is at least $\kappa^\star$. \par
%
Figure~\ref{fig:minimax_support} illustrates an approximation to
$\lambda^\star(dt)$ obtained by solving a discretized version of
\eqref{eq:var_cvx_minimax} for the case when $f(x)$ is the normal density
with variance $\sigma^2$ and $\Theta = [-1/2,1/2]$. The minimax asymptotic
precision parameter $\kappa^\star$ obtained this way is illustrated in
Fig.~\ref{fig:minimax_ARE} as a function of $\sigma$. Also
illustrated in these figures is $\kappa_{\unif}$, the precision
parameter corresponding to threshold values uniformly distribution over
$\Theta$,
\begin{align}
& \kappa_{\unif} \triangleq \min_{\theta \in [-T,T]} \frac{1}{2T}\int_{-T}^T \eta\left(t-\theta\right) dt \nonumber
 \\
& = 
\frac{1}{2T}\int_{-T}^{T} \eta\left(t\pm T\right) dt
= \frac{1}{2T}\int_{0}^{2T} \eta(t) dt  \label{eq:uniform_risk}. 
\end{align}


\begin{figure}
  %%%%% COMMENTED TIKZ PICTURE %%%%%
  \begin{center}
\input{Figs/opt_thr_den.tex}
\caption{\label{fig:minimax_support}
Optimal threshold density under distributed encoding. The threshold density $\lambda^\star(dt)$ (blue) that maximizes the asymptotic relative efficiency for $f(x)$ the normal density with variance $\sigma^2$ and $\Theta= [-1/2,1/2]$. 
%
The continuous curve (red) is the ARE for each $\theta \in [-1/2,1/2]$ under the optimal density, hence the minimax ARE is the minimal value of this curve. The dashed curve (green) is the ARE when the threshold values are uniformly distributed over $[-1/2,1/2]$; its minimal value is $\kappa_{\unif}$ \eqref{eq:uniform_risk}. }
\end{center}
\end{figure}


\begin{figure}
  %%%%% COMMENTED TIKZ PICTURE %%%%%
  \begin{center}
\begin{tikzpicture}[scale = 1]
\begin{axis}[
width=8cm, height=6cm,
xmin=.03,
xmax=2.5, 
xmode = log,
restrict y to domain = 0:100,
ymin = 0,
ymax = 1,
samples=1, 
xlabel= {$\sigma$},
%xtick={-0.5,0,0.5},
%xticklabels={-$$, 0, $b$},
ytick={0,0.637,1},
yticklabels={0,$\frac{2}{\pi}$,1},
ylabel = {\scriptsize Relative Efficiency},
line width=1.0pt,
mark size=1.5pt,
ymajorgrids,
xmajorgrids,
legend style= {at={(1,1)},anchor=north east,draw=black,fill=white,align=left}
]
\addplot[color = red, smooth,
line width=1.0pt,
%mark size=1.5pt,mark=x,
] plot table [x = x, y = y, col sep=comma] {./Figs/minmax_ARE_b0.5.csv};
\addlegendentry{\scriptsize optimal threshold density};

\addplot[color = black!35!green, dashed, line width=1.0pt,
        %mark size=1.5pt, mark=^
        ]
 plot table [x = x, y = z, col sep=comma] {./Figs/minmax_ARE_b0.5.csv};
\addlegendentry{\scriptsize uniform threshold density};


\addplot[color = black, smooth, dotted, line width = 1pt] 
coordinates {
            (0.03, .637) (10, .637)
            };
\addlegendentry{\scriptsize attained in the adaptive case};
%\draw[->] (axis cs:0.1,.5) -- node[below, align=center] {\scriptsize ARE attained in \\ \scriptsize the adaptive case} (axis cs:0.1, .63);
\end{axis}
\end{tikzpicture}
\caption{\label{fig:minimax_ARE} 
Minimax relative efficiency under distributed encoding. ARE versus $\sigma$ for $f(x)$ the standard normal density with variance $\sigma^2$ and parameter space $\Theta = [-1/2,1/2]$. The dashed curve (green) is the ARE under a uniform threshold density over $\Theta$ given by $K_{\unif}\sigma^2$ of \eqref{eq:uniform_risk}. \newtext{
The line $\pi/2$ is attained under adaptive encoding uniformly over the parameter space for any $\sigma$.}}
\end{center}
\end{figure}


%%% Optimal density under a prior %%%%
\iffalse %%% This part have been removed $$$
\begin{figure}
\begin{center}
%
\begin{tikzpicture}[scale = 0.55]
\begin{axis}[
width=8cm, height=6cm,
xmin = -0.5, xmax=0.5, 
restrict y to domain = 0:100,
ymin = 0,
ymax = 1,
samples=10, 
xlabel= {$dt$},
title = {$\sigma/\sigma_{\theta} = 4$},
xtick={-0.5,0,0.5},
xticklabels={-$T$, 0, $T$},
ytick={0,1},
ylabel = {$\lambda(dt)$},
yticklabels={0,1},
line width=1.0pt,
mark size=1.5pt,
ymajorgrids,
xmajorgrids,
legend style= {at={(1,1)},anchor=north east,draw=black,fill=white,align=left}
]
\addplot[color = blue, smooth, mark = o ] plot table [col sep=comma] {./Figs/unif_Bayes_lmd_b0.5_sig4.csv};
%\addlegendentry{$\frac{\sigma^2}{\sigma_\theta^2} = 1$};
\end{axis}
\end{tikzpicture}
%
\begin{tikzpicture}[scale = 0.55]
\begin{axis}[
width=8cm, height=6cm,
xmin = -0.5, xmax=0.5, 
restrict y to domain = 0:100,
ymin = 0,
ymax = 1,
samples=10, 
xlabel= {$dt$},
xtick={-0.5,0,0.5},
title = {$\sigma/\sigma_{\theta} = 3$},
xticklabels={-$T$, 0, $T$},
ytick={0,1},
%ylabel = {$\lambda(dt)$},
yticklabels={0,1},
line width=1.0pt,
mark size=1.5pt,
ymajorgrids,
xmajorgrids,
legend style= {at={(1,1)},anchor=north east,draw=black,fill=white,align=left}
]
\addplot[color = blue, smooth, mark = o ] plot table [col sep=comma] {./Figs/unif_Bayes_lmd_b0.5_sig3.csv};
%\addlegendentry{$\frac{\sigma^2}{\sigma_\theta^2} = 1$};
\end{axis}
\end{tikzpicture}
%
\begin{tikzpicture}[scale = 0.55]
\begin{axis}[
width=8cm, height=6cm,
xmin = -0.5, xmax=0.5, 
restrict y to domain = 0:100,
ymin = 0,
ymax = 1,
samples=10, 
xlabel= {$dt$},
title = {$\sigma/\sigma_{\theta} = 2$},
xtick={-0.5,0,0.5},
xticklabels={-$T$, 0, $T$},
ytick={0,1},
ylabel = {$\lambda(dt)$},
yticklabels={0,1},
line width=1.0pt,
mark size=1.5pt,
ymajorgrids,
xmajorgrids,
legend style= {at={(1,1)},anchor=north east,draw=black,fill=white,align=left}
]
\addplot[color = blue, smooth, mark = o ] plot table [col sep=comma] {./Figs/unif_Bayes_lmd_b0.5_sig2.csv};
%\addlegendentry{$\frac{\sigma^2}{\sigma_\theta^2} = 1$};
\end{axis}
\end{tikzpicture}
%
\begin{tikzpicture}[scale = 0.55]
\begin{axis}[
width=8cm, height=6cm,
xmin = -0.5, xmax=0.5, 
restrict y to domain = 0:100,
ymin = 0,
ymax = 1,
samples=10, 
xlabel= {$dt$},
xtick={-0.5,0,0.5},
xticklabels={-$T$, 0, $T$},
title = {$\sigma/\sigma_{\theta} = 1$},
ytick={0,1},
%ylabel = {$\lambda(dt)$},
yticklabels={0,1},
line width=1.0pt,
mark size=1.5pt,
ymajorgrids,
xmajorgrids,
legend style= {at={(1,1)},anchor=north east,draw=black,fill=white,align=left}
]
\addplot[color = blue, smooth, mark = o ] plot table [col sep=comma] {./Figs/unif_Bayes_lmd_b0.5_sig1.csv};
%\addlegendentry{$\frac{\sigma^2}{\sigma_\theta^2} = 1$};
\end{axis}
\end{tikzpicture}
\caption{\label{fig:opt_density}
Optimal threshold density $\lambda^\star(dt)$ that minimizes the asymptotic Bayes risk \eqref{eq:cvx_average} for a uniform prior with $\sigma/\sigma_\theta=1,2,3,4$, where $\sigma_\theta^2=T^2/3$ is the variance of the prior. 
}
\end{center}
\end{figure}

When a prior $\pi$ on $\Theta$ is provided, one may be interested in the threshold density $\lambda$ minimizing the Bayes risk $R_n(\pi)$. The resulting minimization problem is 
\begin{align}
\label{eq:cvx_average}
\begin{split}
\mathrm{minimize} \quad & R_{\pi} =  \int \frac{\pi(d\theta)}{ \int \eta \left( t-\theta\right) \lambda(dt)}. \\ 
\mathrm{subject~to} \quad & \lambda(dt)\geq 0,\quad \int \lambda(dt) =1, 
\end{split}
\end{align}
which is convex in $\lambda$ since $x \rightarrow 1/x$, $x>0$, is convex. Figure~\ref{fig:opt_density}  illustrates the solution to \eqref{eq:cvx_average} for the case of where $f(x)$ is the normal distribution and $\pi$ is the uniform distribution over $\Theta = [-T,T]$. %Figure~\ref{fig:dist_bound_uniform} illustrates the corresponding Bayes risk. 
\fi
