\section{Conclusions \label{sec:conclusions}}
We considered the risk and efficiency in estimating the mean of a symmetric and log-concave distribution from a sequence of bits, where each bit is obtained by encoding a single sample from this distribution. 
%
In an adaptive encoding setting, we showed that, asymptotically, no estimator can be more efficient than the median of the samples. We also showed that this bound is tight by presenting two adaptive encoding and estimation procedures that are as efficient as the median. Furthermore, we showed that only one round of adaptivity is required to attain optimal efficiency. In the distributed setting we provided conditions for local asymptotic normality of the encoded samples, which implies asymptotic minimax bound on both the risk and efficiency relative to the mean. 
%
Under local asymptotic normality, the optimal estimation performance derived for the adaptive case can only be attained over a finite number of points, i.e., no scheme is uniformly optimal in this setting. 
%
We further considered the special case where the sequence of bits is obtained in a distributed manner by comparing against a prescribed sequence of thresholds. We characterized the performance of the optimal estimator from such bit-sequence using the density of the thresholds and considered the density that minimizes the minimax risk. 
%
%\par

\newtext{
Natural extensions of the setting of this work are situations when the communication bit-budget $b$ is larger than one and when each sample is a $d$-dimensional vector. Rough bounds in this general case follow from several recent works (e.g. \cite{zhang2013information,shamir2014fundamental,braverman2016communication,han2018geometric,barnes2020lower}), which imply that in the adaptive and distributed cases the MSE scales in the regular parametric rate of $1/n$ when $b$ and $d$ are held fixed in the sample size $n$. Nevertheless, the coefficients of the $1/n$ term corresponding to the ARE are in general unknown; deriving the ARE for general $b$ and $d$ is left as a future challenge.}
