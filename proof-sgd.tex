\subsection{Proof of Theorem~\ref{thm:sgd}}
\label{proof:sgd}

The estimation algorithm~\eqref{eq:sgd_alg} is a special
case of the stochastic gradient procedures in the papers
\cite{PolyakJu92, polyak1990new}.
We rely on several of their results. Throughout this proof,
we assume without loss of generality that the median
$\theta = \mbox{med}(P) = 0$.

\subsubsection*{Proof of (i)}

Consider the following simplified version of
\cite[Thm. 4]{polyak1992acceleration}:
\begin{corollary}{\cite[Thms. 3 \& 4]{PolyakJu92}}
  \label{corollary:polyak-juditsky}
  Let $\varphi : \R \to \R$ and $Z_i$ be i.i.d.\ random
  variables, and
  \[
  X_i = \theta + Z_i.
  \]
  Define
  \begin{align}
    \begin{split}
      \theta_i & = \theta_{i-1} + \gamma_i \varphi(X_i - \theta_{i-1}), \\
      \bar{\theta}_n & = \frac{1}{n} \sum_{i=0}^{n-1} \theta_i, 
    \end{split}
    \label{eq:Polyak_Juditsky_alg}
  \end{align}
  where in addition,
  \begin{enumerate}[(i)]
  \item There exists $K_1$ such that $\left| \varphi(x) \right| \leq
    K_1(1+|x|)$ for all $x\in \R$.
  \item The sequence $\left\{ \gamma_i \right\}_{i=1}^\infty$ satisfies
    condition~\eqref{eqn:lazy-gamma}.
  \item The function $\psi(x) \triangleq \ex{ \varphi(x+Z_1)}$
    satisfies $\psi(0) = 0$ and
    $x\psi(x) > 0$ for $x\neq 0$.  Moreover, $\psi$ is differentiable
    at 0 with $\psi'(0) > 0$ and there exists
    $K_2$ and $0 < \lambda \leq 1$ such that
    \begin{equation}
      \label{eq:Polyak_Juditsky_cond3}
      \left| \psi(x) - \psi'(0)x \right|\leq K_2 |x|^{1+\lambda}.
    \end{equation}
  \item The function 
    $\chi(x) \triangleq \ex{\varphi^2(x+Z_1)}$ is continuous at zero. 
  \end{enumerate}
  Then $\bar{\theta}_n \cas \theta$ and $ \sqrt{n}({\theta}_n - \theta)
  \cd \normal(0,V)$ for
  $V = \frac{ \chi(0)} {\psi'(0)^2}$.
\end{corollary}

Using the notation in Corollary~\ref{corollary:polyak-juditsky}, we set
$\varphi(x) = \sgn(x)$ and $Z_i = X_i - \theta$, where $\theta =
\mbox{med}(P)$. Without loss of generality and for notational convenience,
we assume for the remainder of this derivation that $\theta = 0$.
As a consequence, we have $\mbox{med}(Z) = 0$,
and $\chi(x) = \ex{ \sgn^2(x+Z_1) }= 1$, so
$\chi(0) = 1$. In addition,
\begin{align*}
  \psi(x) & = \ex{ \sgn(x+ Z_1) } =
  P(Z \ge -x) - P(Z < -x) = 1 - 2 P(Z \le -x).
  %%   \int_{-\infty}^\infty \sgn(x+z) f(z) dz \\
  %% & = \int_{-x}^\infty f(z) dz -\int_{-\infty}^{-x} f(z) dz. 
\end{align*}
Using that $P$ has a density $f$ near its median, it follows that $\psi'(x)
= 2f(-x)$ and thus $\psi'(0) = 2f(0) > 0$.  We may now verify that the
conditions in Corollary~\ref{corollary:polyak-juditsky} hold for $\lambda =
1$. Condition~(i) is obvious, and the convexity of $|\cdot|$ gives most of
condition~(iii) excepting inequality~\eqref{eq:Polyak_Juditsky_cond3}. For
that, note that as $f$ is Lipschitz near 0 with constant $\lip_0(f)$, we
have for small $x$ that
\begin{equation*}
  \psi(x) = 2 \int_0^x f(-t) dt
  = 2 \int_0^x \left[f(0) \pm \lip_0(f) t \right] dt
  = 2 f(0) x \pm \frac{\lip_0(f)}{2} x^2
  = \psi'(0) x \pm \frac{\lip_0(f)}{2} x^2,
\end{equation*}
so that condition~(iii) holds.
As evidently $\chi(0) / \psi'(0)^2 = \frac{1}{4 f(0)^2}$,
Corollary~\ref{corollary:polyak-juditsky} gives
Theorem~\ref{thm:sgd}\eqref{item:normal-sgd}.

\subsubsection*{Proof of (ii)}
We first show that the estimator $\bar{\theta}_n$ is regular in the following sense: For $\theta \in \Theta$, $h\in \R$ and $n$ large enough such that $\theta+h/\sqrt{n} \in \Theta$, let $\Prob_{\theta,n}$ be a product probability measure on $\R^n$ with density $f(x-\theta - h/\sqrt{n})$ in each of its $n$ coordinates. Then 
\begin{align}
\label{eq:sgd_part2}
\sqrt{n}\left( \bar{\theta}_n - \theta\right) \overset{d}{\to} \Ncal\left( h,\frac{1}{\eta(0)}\right),
\end{align}
under $\Prob_{\theta,n}$. In order to show \eqref{eq:sgd_part2} we use the following refinement of Theorem~\ref{corollary:polyak-juditsky}, proof of which is given in Subsection~\ref{proof:thm:normal_expansion} below.
%
\begin{thm} \label{thm:normal_expansion}
Set $\Delta_i = \theta_i - \theta$ and
$\bar{\Delta_i} = \frac{1}{n} \sum_{i=1}^n \Delta_i$. Assume that, in addition to Assumptions (i)-(iv) of Theorem~\ref{corollary:polyak-juditsky}, there exists $K_1$ and $\lambda>0$ such that
\begin{equation}
\ex{\left| \varphi(Z_1) - \varphi(x+Z_1)  \right|} \leq K_1 |x|^{1+\lambda}
\label{eq:PJ_additional_cond}. 
\end{equation}
Then:
\begin{itemize}
\item[(i)] \begin{equation}
\sqrt{n} \bar{\Delta}_n = -\frac{1}{\sqrt{n}} \frac{1}{\psi'(0)} \sum_{i=1}^{n-1} \varphi(Z_i)+ o_{p.n}(1). \label{eq:normal_expansion_lem}
\end{equation}
where $o_{p,n}(1)$ converges in probability to $0$ as $n\to \infty$.
\item[(ii)] If $Z_1$ has continuously differentiable density $f(x)$ with finite Fisher information for location $\sigma_f^2$, then for any converging sequence $h_n \to h$,
\[
\sqrt{n} \left( \bar{\Delta}_n \right) \overset{d}{\to} \Ncal\left( \frac{-h}{\psi'(0)} \int_{\R} \varphi(x) f'(x) dx  , \frac{\chi(0)}{\psi'^2(0)} \right)
\]
under the local alternative $Z_1,\ldots,Z_n \sim \Prob^n_{h_n/\sqrt{n}}$ with density $\prod_{i=1}^n f(x_i-h_n/\sqrt{n})$.
\end{itemize}
\end{thm}
In our setting, we have
\begin{align*}
& %\varphi(Z_1) - \varphi(x+Z_1) = 
\sgn(Z_1) - \sgn(x + Z_1) \\
& = \begin{cases}
2 & Z_1 > 0,\, x+Z_1<0, \\
-2 & Z_1 <0, \, x+Z_1>0, \\
0 & \text{otherwise}.
\end{cases}
\end{align*}
It follows that
\begin{align*}
\ex{ \left| \varphi(Z_1) - \varphi(x+Z_1) \right| }  \leq \Prob\left( |Z_1| < x  \right) \leq f(0) |x|, 
\end{align*}
and hence condition \eqref{eq:PJ_additional_cond} is fulfilled. In addition, by anti symmetry of $f'(x)$ around $x=0$, 
\[
\int_{\R} \varphi(x) f'(x) dx = \int_{\R} \sgn(x) f'(x) dx = 2\int_0^\infty f'(x) dx = -2f(0) = -\psi'(0). 
\]
Theorem~\ref{thm:normal_expansion}, applied to the setting of Theorem~\ref{thm:sgd}, implies \eqref{eq:sgd_part2}. \par

Under the assumption that $f(x)$ is continuously differentiable with a finite Fisher information for location, the model $\{Z_n+ \theta\}_{n \in \N}$ is differentiable in quadratic mean \cite[Exm. 7.8]{van2000asymptotic} and hence local asymptotically normal (LAN) in the sense that
\[
\log \left(\frac{\Prob_{\theta,n}(X^n)}{\Prob_{\theta} (X^n) }\right) = h \eta^{-1/2}(0) Z - \frac{1}{2} h^2 \eta^{-1}(0)  + o_{p,n}(1),
\]
where $Z\sim \Ncal(0,1)$ and $o_{p,n}(1) \to 0$ as $n\to \infty$ under $\Prob_{\theta}$. The proof is concluded since any regular estimator in LAN model satisfies \eqref{eq:attaining_LAM} \cite{beran1995role}. 

\subsubsection*{Proof of (iii)}
%In order to prove part (ii) of Theorem~\ref{thm:sgd} 
Consider the following result from \cite{polyak1990new}:
\begin{thm}{\cite[Thm. 2]{polyak1990new}} \label{thm:polyak_new}
Let
\begin{align} \label{eq:polyak_new_measurements}
\begin{cases}
U_n = U_{n-1} - \gamma_n \varphi(Y_n), & Y_n = g'(U_{n-1})+Z_n \\
\bar{U}_n= \frac{1}{n} \sum_{i=1}^n U_n, & n=1,2,\ldots.
\end{cases}
\end{align}
Assume that the function $g(x)$ is twice differentiable with a strictly positive and uniformly bounded second derivative. In particular, $g(x)$ is convex with a unique minimizer $x^\star \in \R$. Moreover, assume that the noises $Z_n$ are uncorrelated and identically distributed with a distribution with a density for which the Fisher information exits. Let $\psi(x)$ and $\chi(x)$ be defined as in Theorem~\ref{corollary:polyak-juditsky}-(iii) and satisfy the conditions there. Assume in addition that $\chi(0)>0$, condition \eqref{eq:Polyak_Juditsky_cond3} with $\lambda = 1$, 
and there exits $K_3$ such that 
\[
\ex{  | \varphi(x+Z_1) |^4 } \leq K_3(1+|x|^4). 
\]
Finally, assume that the sequence $\{\gamma_n \}$ satisfies conditions \eqref{eq:conditions1} and \eqref{eq:conditions2}. Then
\[
V_n \triangleq \ex{ \left(\bar{U}_n-x^\star \right)^2 } = n^{-1}\frac{\chi(0)} { (\psi'(0))^2 (g''(x^\star))^2 } + o(n^{-1}).
\]
\end{thm}

We now use Theorem~\ref{thm:polyak_new} with $g(x) = 0.5(x-\theta)^2$, $\varphi(x) = \sgn(x)$, $Z_n = \theta-X_n$. From \eqref{eq:polyak_new_measurements} we have
\begin{align*} 
U_n & = U_{n-1} + \gamma_n \sgn(\theta-U_{n-1} - Z_n )  \\
& = U_{n-1} + \gamma_n \sgn(X_n-U_{n-1} ),
\end{align*}
so the estimator $\bar{U}_n$ equals to the one defined by \eqref{eq:sgd_est} and \eqref{eq:sgd_alg}. Note that
\[
\ex{ | \varphi(x+Z_1) |^4 } = 1 \leq K_3(1+|x|^4)
\]
for any $K_3\geq 1$, the Fisher information of $Z_1$ is $\sigma^2$, $\chi(x) = 1 > 0$, and that 
the conditions in Theorem~\ref{thm:polyak_new} on $\psi(x)$ and $\chi(x)$ were verified to hold in the first part of the proof. In particular, $\psi'(0) = (2f(0))^{-2}$. Since $f(x)$ satisfies the conditions above with $x^\star = \theta$ and $g''(x) = 1$. Theorem~\ref{thm:polyak_new} implies that for any $\theta \in \R$, 
\[
V_n = \ex{ \left({\theta}_n-\theta \right)^2 } = \frac{1}{4n f^2(0)} + o(n^{-1}).
\]
